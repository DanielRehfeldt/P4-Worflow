[["index.html", "1 DISCLAIMER", " Statistische Analysen mit R in den MINT-Didaktiken Eine Tutorial-Sammlung Tina Grottke, Philipp Möhrke, Marvin Rost, David Buschhüter (Hrsg.) 2022-08-02 1 DISCLAIMER DIE HERAUSGEBER:INNEN ÜBERNEHMEN KEINE VERANTWORTUNG FÜR DIE RICHTIGKEIT DER INFORMATIONEN. DARÜBER HINAUS IST DER GRUNDGEDANKE DES BUCHS, DIE QUALITÄT DURCH REVISION STÄNDIG ZU ERHÖHEN. DIE HERAUSGEBER:INNEN ÜBERNEHMEN ZUDEM KEINE VERANTWORTUNG FÜR INHALTE AUF VERLINKTEN WEBSEITEN. "],["ziel-dieses-buches.html", "2 Ziel dieses Buches", " 2 Ziel dieses Buches Statistische Analysen in den MINT-Didaktiken sollten noch transparenter kommuniziert werden. Durch eine größere Transparenz würde eine verbesserte Diskussionsgrundlage für Common-Practices in der Datenanalyse geschaffen. In der Folge können auch Best-Practice-Beispiele entstehen, die vor allem Anfänger*innen Sicherheit geben. Damit ist das Ziel dieses Buches in einem Buttom-Up-Ansatz Beiträge zu sammeln, die erprobte Kenntnisse und Beispiele zu statistischen Datenanalysen als Orientierungshilfe für Early-Career-Researchers bereitstellen. "],["wie-kann-ich-zu-diesem-sammelband-einen-beitrag-leisten.html", "3 Wie kann ich zu diesem Sammelband einen Beitrag leisten?", " 3 Wie kann ich zu diesem Sammelband einen Beitrag leisten? Wer kann einen Beitrag einreichen? Theoretisch kann jeder einen Beitrag einreichen, der mit dem Thema seines*ihres Beitrags hinreichend vertraut ist. In erster Linie ist dieses Buch aber an Autor*innen und Leser*innen aus dem Bereich (Science/Math) Education und Psychologie gerichtet. Welche Arten von Beiträgen können eingereicht werden? Ein einreichbarer Beitrag… … ist interessant für die oben genannte Community . … hat eine „Tutorialfunktion“ (Ziel sollte es sein, dass die Person im Anschluss eine entsprechende Analyse umsetzen kann und Parameter sowie den Output interpretieren kann). … betrifft die Nutzung von R-Paketen, die für die oben genannte Community von Interesse sind (insbesondere zur Psychometrie). Es gibt dabei zwei Arten von Beiträgen: Essentiell für das Buch ist der Gedanke der Kollaboration. Sie können also insbesondere beitragen, indem Sie die Beiträge anderer Autor*innen kommentieren. (Öffnen Sie Sie dazu einfach eine GitHub-Issue: https://github.com/dbuschhue/P4-Worflow/issues) Kapitelbeitrag Beiträge zum Kapitel Fehlvorstellungen Aufgrund ihrer Kürze setzen wir voraus, dass diese Beiträge entsprechend in etwaigen Listen zur Darstellung der Publikationsleistung als Kurzbeitrag kenntlich gemacht werden. Inwiefern wird der Beitrag begutachtet? Der Beitrag wird hinsichtlich seiner grundsätzlichen Eignung begutachtet. Es erfolgt dabei kein ausgiebiges Peer-Review-Verfahren (die grundsätzliche Idee ist, dass sich dieses Buch weiterentwickelt online, indem andere Autor*innen Verbesserungsvorschläge machen und kommentieren). Inwiefern bin ich im Anschluss noch involviert? Es wird erwartet, dass Sie im Fall von wichtigen Verbesserungsvorschlägen, den Artikel anpassen (dies betrifft insbesondere notwendige Korrekturen). Sollte es zu einem Korrekturvorschlag kommen, werden Sie von den Herausgeber*innen benachrichtigt. Hinweis: Ein Wesensmerkmal des Buchs ist die permanente Revisionsfähigkeit. Wir schreiben kein Lehrbuch für die Umsetzung als universitäre Lehrveranstaltung, sondern lernen gemeinsam in der Auseinandersetzung mit dem Thema und mit den Beitragenden. Das ist neu und ungewohnt. Wir glauben aber, dass das ein bedeutsamer Beitrag für eine zukunftsfähige Wissenschaftscommunity ist. Wie kann ich den Beitrag einreichen? Die technische Umsetzung erfolgt über die Plattform GitHub. Stellen Sie deswegen bevorzugt einen Pull-Request an das Repository mit Ihrem Beitrag und einer entsprechenden Nachricht. Wenn Ihnen das nicht möglich ist, treten Sie mit den Herausgeber*innen über die Issues-Funktion auf GitHub in Kontakt. Wichtiger als die Nutzung von GitHub ist die inhaltliche Ebene der Beiträge. Bitte halten Sie aber unbedingt die Formatierungsrichtlinien ein (s. u.), um den Revisionsprozess zu beschleunigen und den Fokus auf den Inhalten zu belassen. Welche Formatierungsrichtlinien gelten für den Text? Die Beiträge sind im RMarkdown Format als Rmd-Datei einzureichen und genauer als Buchkapitel zu einem bookdown-Buch. Sollten Sie noch nie mit RMarkdown gearbeitet haben, empfehlen wir eine einsteigerfreundliche, deutschsprachige Videoreihe, die mit dem folgenden Video beginnt: https://www.youtube.com/watch?v=F-6iU1EHoc0 . Weitere Informationen finden Sie hier: Bookdown: https://bookdown.org/; https://bookdown.org/yihui/bookdown/ R Markdown: https://rmarkdown.rstudio.com/ Der Beitrag ist in genderneutraler Sprache geschrieben. Achten Sie auf eine angemessene Länge und Lesbarkeit. Orientieren Sie sich an bisherigen Einreichungen. Wir haben ein Vorlage-Kapitel erstellt, das Sie hier herunterladen können. Welche Regeln gelten für den Code? Keine übermäßig langen Zeilen. Keine unnötigen Redundanzen zwischen Text und R-Kommentaren. Achten Sie auch auf gute Lesbarkeit des Codes. Orientieren Sie sich an bisherigen Einreichungen in diesem Buch. Wie lange dauert die Begutachtung eines Beitrags? Wir möchten ein schnelles Verfahren umsetzen und gehen von einer Richtzeit von 1-2 Wochen je nach Zustand des Beitrags aus. Kann man meinen Beitrag zitieren? Natürlich, einfach als Beitrag eines Sammelbands mit dem Erscheinungsdatum ihres Artikels. "],["basiswissen-für-r-praxisguide.html", "4 Basiswissen für R: Praxisguide 4.1 Motivation 4.2 Inhalte 4.3 RStudio 4.4 Pakete 4.5 Objekte in R 4.6 Variablen: Typen 4.7 Coden: Tipps &amp; Tricks 4.8 Übersichtlich Coden 4.9 Datensätze: Teil-Datensätze bilden", " 4 Basiswissen für R: Praxisguide Autoren: Daniel Rehfeldt, Martin Brämer 4.1 Motivation Herzlich Willkommen zum Beitrag Basiswissen für R. Viele Einführungswerke oder -kurse für das open-source-Statistikprogramm R sind auf ein heterogenes Publikum ausgerichtet und beginnen daher mit Operationen wie Addition von Zahlen und Vektoren oder dem Konstruieren eigener, fiktiver Datensätze. Wir möchten mit diesem Beitrag einen anderen Weg gehen und direkt zielgruppenorientiert eine Einführung für Datenhandling in R aus Perspektiver der quantitativen, empirischen Bildungsforschung liefern. 4.2 Inhalte Der Praxisguide startet mit einer knappen Einführung in die Objekttypen in R mit Fokus auf Funktionen und Pakete (Pakete), die den Werkzeugkasten”* R ausmachen. Datensätzen (wie sie aus quantitativen Erhebungen resultieren), Variablen (z. B. resultierend aus Fragebogen-Fragen resultierend), sowie Nach Klärung dieses Grundwissens werden handlungsnahe, praktische Tipps zum Coden in R gegeben, damit bereits von Beginn an eine gewisse Stringenz und Sauberkeit erlernt wird, die z. B. ein späteres Referenzieren und Nachschlagen, sowie die Weitergabe von z. B. Code-Chunks erleichtert. Abgeschlossen wird diese kleine Einführung mit einer Anleitung zur Bildung von Teil-Datensätzen, die in sehr vielen Funktionen von R benötigt werden bzw. den Umgang mit den Daten wesentlich erleichtern. Für fortgeschrittene Techniken sei auf unseren zweiten Beitrag zum Thema Datenaufbereitung (auch in dieser Reihe) verwiesen. Deutlich ausführlichere Einführungen in R finden sich z. B. hier oder grundsätzlich über die Web-Recherche einzelner Fragestellungen. Außerdem empfehlen wir, regen Gebrauch von der Kommentarfunktion in R zu machen (auskommentieren). Jede Zeile mit einer vorangestellten Raute (#) interpretiert R nicht als Code. Sollen mehrere bereits geschriebene Zeilen nachträglich als Kommentar definiert werden, kann so vorgegangen werden: Text markieren, STRG + UMSCH + C drücken. Genauso kann auch ent-kommentiert werden. Den Lesenden sollte zudem bewusst sein, dass R mittlerweile eine derart umfangreiche Paketauswahl unterstützt, dass für jedes (Datenbereinigungs-)Problem mehrere Lösungen existieren, von denen wir hier nur wenige vorstellen. 4.3 RStudio Für ein deutlich übersichtlicheres Coden und Arbeiten empfehlen wir die Benutzung der Oberfläche RStudio:: Figure 4.1: RStudio im Vergleich zu Basis-R (oben: von xmodulo, lizensiert unter CC BY 2.0. https://creativecommons.org/licenses/by/2.0/?ref=openverse, unten: von Daniel Rehfeldt Der Installationsprozess gestaltet sich folgendermaßen: Installieren der Software R selbst: Link Installieren von RStudio: Rstudio bietet gegenüber basalem R so viele Vorteile, dass seine Nutzung obligatorisch geworden ist: Zusammengehöriger Code wird bereits beim Schreiben durch Highlighting und Farbgebung ausgezeichnet RStudio bietet mehrere Fenster, die unterschiedliche Funktionen erfüllen (Coden, Objekte, Output, Grafiken usw.) Autovervollständigung von Funktions- oder Objektnamen (ähnlich wie z. B. Excel) erleichterte Paketverwaltung Erleichterte Bedienung über z. B. View-Tab (vgl. Kapitel “Datenaufbereitung”), direktes Betrachten von Datensätzen usw. im Environment-Fenster Speichern des R-Scripts, run und re-run des Codes direkt Integrierte Hilfefunktion (u. a. Cheatsheets zum Spicken unter help – cheatsheets) 4.4 Pakete R bietet bereits in der Standard-installierten Version sehr viele Funktionen, um Daten manipulieren zu können. Um einige zusätzliche Funktionen nutzen zu können, werden zunächst Pakete in R installiert und geladen. Pakete sind Container für Funktionen und Datensätze, die meist eine gewisse Analyserichtung teilen, etwa das psych()-Paket für psychometrische Analysen. Informationen zu einzelnen Paketen finden sich über eine einfache Web-Suche. Das Nutzen von Paketen wird über zwei Schritte realisiert. Den ersten und nur einmalig durchzuführenden Schritt bildet das Installieren das Pakets (install.package()) – ähnlich, wie ein Programm am PC nur einmalig installiert werden muss. Zweiter Schritt ist dass das Aktivieren bzw. Laden des Pakets (library()) – vergleichbar mit dem Starten eines Programms am PC. Ebenso analog dazu muss nach jedem Neustart von RStudio auch das Paket neu geladen werden. # Installiere und lade die benötigten Pakete # install.Pakete(&quot;xlsx&quot;) # da dieses Paket bereits zuvor vom Autor installiert wurde, muss es hier nicht erneut installiert werden und ist daher auskommentiert # install.Pakete(&quot;psych&quot;) # da dieses Paket bereits zuvor vom Autor installiert wurde, muss es hier nicht erneut installiert werden und ist daher auskommentiert library(psych) # liefert psychometrisch nützliche Funktionen library(xlsx) # hilft beim Import von Excel-Daten 4.5 Objekte in R 4.5.1 Datensätze: Typen 4.5.1.1 Dataframe Die meisten Datensätze in R sind Dataframes. Sie bestehen im Wesentlichen, ähnlich wie eine Excel-Tabelle, aus Zeilen und Spalten. Die Zeilen sind in der Regel nummeriert und können z. B. Personen entsprechen, die Spalten tragen je einen Namen und stellen je meist eine Variable dar. Im folgenden Beispiel hat Person 2 in der Variablen Motivation eine Ausprägung von 2 und ist männlich. demodata ## pcode motivation geschlecht ## 1 Person 1 3 weiblich ## 2 Person 2 2 männlich ## 3 Person 3 2 divers ## 4 Person 4 4 weiblich ## 5 Person 5 6 männlich ## 6 Person 6 1 divers ## 7 Person 7 4 männlich Der Datensatz-Name kann frei gewählt und auch verändert werden. Es hat sich bewährt, aussagekräftige Namen statt Kürzel wie df o. Ä. zu verwenden. Dies erleichtert die Orientierung bei multiplen Datensätzen. # Ändere den Datensatznamen (genauer: Erzeuge neuen Datensatz, der ein inhaltliches Duplikat ist): demodata_name_anders &lt;- demodata Für große Datensätze kann mit der head()-Funktion der Output auf die ersten 6 Zeilen beschränkt werden: head(demodata_name_anders) ## pcode motivation geschlecht ## 1 Person 1 3 weiblich ## 2 Person 2 2 männlich ## 3 Person 3 2 divers ## 4 Person 4 4 weiblich ## 5 Person 5 6 männlich ## 6 Person 6 1 divers 4.5.2 Matrix Eine Matrix ist wie eine abgespeckte Version eines Dataframes. Sie kann nur Zahlen enthalten und hat keine spezielle Benennung der Zeilen oder Spalten: print(A) ## [,1] [,2] [,3] ## [1,] 2 3 2 ## [2,] 1 5 6 4.5.3 Listen Listen sind sehr variable Container. Sie können nahezu alles beinhalten, wie z.B. Dataframes, Matrizen, einzelne Variablen usw., auch verschiedene Objekttypen in derselben Liste. Mit mehreren Dataframes in einer Liste können z. B. Routineaufgaben der Datenbereinigung auf multiple Datensätze gleichzeitig angewandt werden.1. # Erstelle zwei Datensätze und forme daraus eine Liste mit zwei enthaltenen Datensätzen demodata &lt;- data.frame( pcode = c(&quot;Person 1&quot;, &quot;Person 2&quot;, &quot;Person 3&quot;), motivation = c(3, 2, 2), geschlecht = c(&quot;weiblich&quot;, &quot;männlich&quot;, &quot;divers&quot;) ) demodata2 &lt;- data.frame( pcode = c(&quot;Person 4&quot;, &quot;Person 5&quot;, &quot;Person 6&quot;), motivation = c(4, 1, 2), geschlecht = c(&quot;weiblich&quot;, &quot;weiblich&quot;, &quot;männlich&quot;) ) demodata ## pcode motivation geschlecht ## 1 Person 1 3 weiblich ## 2 Person 2 2 männlich ## 3 Person 3 2 divers demodata2 ## pcode motivation geschlecht ## 1 Person 4 4 weiblich ## 2 Person 5 1 weiblich ## 3 Person 6 2 männlich liste &lt;- list(demodata, demodata2) liste ## [[1]] ## pcode motivation geschlecht ## 1 Person 1 3 weiblich ## 2 Person 2 2 männlich ## 3 Person 3 2 divers ## ## [[2]] ## pcode motivation geschlecht ## 1 Person 4 4 weiblich ## 2 Person 5 1 weiblich ## 3 Person 6 2 männlich 4.6 Variablen: Typen Um Variablen – egal welchen Typs – in einem Dataframe auszuwählen, kann der $-Operator nach der Formel DATASETNAME$VARIABLENNAME verwendet werden. 4.6.1 Numeric Eine Variable vom Typ Numeric besteht innerhalb eines Dataframes aus einer Spalte von Zahlen: # Variable in einem Dataframe demodata &lt;- data.frame( motivation = c(3, 2, 2), geschlecht = c(&quot;weiblich&quot;, &quot;weiblich&quot;, &quot;divers&quot;) ) demodata$motivation ## [1] 3 2 2 4.6.2 Character Eine Character-Variable haben wir bereits im oberen Beispiel kennengelernt, sie besteht aus einer Spalte von Textelementen: demodata$geschlecht ## [1] &quot;weiblich&quot; &quot;weiblich&quot; &quot;divers&quot; 4.6.3 Factor Einen Kompromiss zwischen Numeric und Character stellt der Variablentyp Factor dar. Hier werden bei einer eigentlich numerischen Variablen den Zahlen Text zugeordnet. Zudem besitzt eine Factor-Variable eine definierte Auswahl an Levels, d. h. es gibt eine endliche Anzahl an definierten Kategorien einer Factor-Variablen, andere Kategorien sind unzulässig und müssten erst erstellt werden. In diesem Beispiel erzeugen wir zunächst eine Factor-Variable2: demodata &lt;- data.frame(motivation = c(3, 2, 2), geschlecht = c(1, 2, 3)) demodata$geschlecht &lt;- factor( demodata$geschlecht, levels = c(1, 2, 3), # Dies sind die Zahlenwerte labels = c(&quot;weiblich&quot;, &quot;männlich&quot;, &quot;divers&quot;) # Dies sind die zugeordneten Texte. #Die Reihenfolge ist entscheidend für die Zuordnung ) demodata$geschlecht ## [1] weiblich männlich divers ## Levels: weiblich männlich divers 4.7 Coden: Tipps &amp; Tricks Vorweg sei auf eine für Anfänger*innen, aber auch Fortgeschrittene unerlässliche Hilfe hingewiesen: Eine einfache Websuche des jeweiligen Problems. Durch die gewaltige R-User-Community liefern Websites wie Stack Overflow in fast allen Fällen konkrete und sofortige Hilfe – und dies meist deutlich verständlicher als die Standard R-Hilfe. 4.7.1 Umgang mit Klammern Wie viele Programmiersprachen arbeitet auch R mit Klammern. Betrachtet man den Typus der Klammer, kann man hieraus häufig deren Funktion herleiten und gewinnt so an Orientierung. Dafür ein paar Beispiele: Runde Klammern deuten meist auf Funktionen hin, das Argument (das, worauf die Funktion angewandt wird) steht in der Klammer: summary(demodata) # liefert einen Überblick (Funktion summary() ) über die Daten des Dataframes demodata (das Argument) Eckige Klammern kommen z. B. bei der Auswahl von Teil-Daten(sätzen) zum Einsatz (vgl. Abschnitt 4.9): demodata[1,2] #liefert den Eintrag in der ersten Zeile und zweiten Spalte des Dataframes ## [1] weiblich ## Levels: weiblich männlich divers Geschweifte Klammern werden z. B. für Wenn-Dann-Beziehungen, Schleifen und selbst kreierte Funktionen gebraucht. Findet man diese vor (z. B. bei der Online-Recherche), handelt es sich also meist um einen solchen Fall. Ein kleines Beispiel für eine Schleife folgt nach einer kurzen Einführung: 4.7.2 Schleifen Für Einsteiger*innen in R können Schleifen herausfordernd sein, daher fassen wir uns hier kurz. Viele gewünschte Operationen sind auch ohne Schleifen möglich, Schleifen können allerdings gerade bei großen Datensätzen eine enorme Zeitersparnis bedeuten. Der Datensatz für das Beispiel ist im GitHub-Verzeichnis dieser Publikation verfügbar. Ein Beispiel zu Schleifen: Für die Fach im Abitur gehabt ja/nein-Variablen erhalten wir einen Datensatz, der 1 für ja und NA für nein enthält. Wir möchten gern eine 0 statt NA: # Betrachte beispielhaft die ersten beiden Abitur-Variablen # Finde die Spaltennummer der Abitur-Variablen über die Funktion names(): names(demodata) ## [1] &quot;pcode&quot; &quot;VW.theo&quot; &quot;VW.prax&quot; ## [4] &quot;VW.plan&quot; &quot;VW.durch&quot; &quot;VW.reflex&quot; ## [7] &quot;Abitur_Fach_Biologie&quot; &quot;Abitur_Fach_Chemie&quot; &quot;Abitur_Fach_Geographie&quot; ## [10] &quot;Abitur_Fach_Geschichte&quot; &quot;Abitur_Fach_Informatik&quot; &quot;Abitur_Fach_Physik&quot; ## [13] &quot;Abitur_Fach_PW&quot; &quot;geschlecht&quot; &quot;alter&quot; ## [16] &quot;semester.pre&quot; &quot;abi&quot; &quot;dozent&quot; # Die Abitur-Variablen beginnen also ab Spalte 7. # Betrachte beispielhaft die ersten beiden solchen Variablen: head(demodata[c(1:25), c(7, 8)]) ## Abitur_Fach_Biologie Abitur_Fach_Chemie ## 1 NA NA ## 2 NA NA ## 3 NA NA ## 4 NA NA ## 5 NA NA ## 6 NA NA #Variante ohne Schleife: Jede Variable einzeln: demodata[is.na(demodata[, 7]) == T, 7] &lt;- 0 #Diejenigen Personen, # die in Variable 7 ein NA aufweisen, # erhalten als Wert nun eine 0 demodata[is.na(demodata[, 8]) == T, 8] &lt;- 0 # usw. bis Variable 13. #Überprüfe: head(demodata[c(1:25), c(7, 8)]) ## Abitur_Fach_Biologie Abitur_Fach_Chemie ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 0 0 ## 6 0 0 Als Schleife sähe es – wohlgemerkt für alle Abitur-Variablen gleichzeitig (!) – wie folgt aus: for (i in 7:13) { # Abitur-Variablen stehen in den Spalten 7 bis 13 demodata[is.na(demodata[, i]) == T, i] &lt;- 0 # Wo NAs stehen, soll 0 gesetzt werden } head(demodata[c(1:25), c(7:13)]) # Überprüfe ## Abitur_Fach_Biologie Abitur_Fach_Chemie Abitur_Fach_Geographie ## 1 0 0 0 ## 2 0 0 0 ## 3 0 0 0 ## 4 0 0 0 ## 5 0 0 0 ## 6 0 0 0 ## Abitur_Fach_Geschichte Abitur_Fach_Informatik Abitur_Fach_Physik ## 1 0 0 0 ## 2 0 0 0 ## 3 0 0 0 ## 4 0 0 0 ## 5 0 0 0 ## 6 0 0 0 ## Abitur_Fach_PW ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0 Die for()-Schleife enthält hierbei immer zwei Klammer-Typen. Die runde Klammer umschließt die Laufzahl i und welche Werte sie durchläuft, die geschweifte Klammer umschließt die Operationen, die je Laufzahldurchgang erfolgen sollen. 4.7.3 Eigene Funktionen Da eigene Funktionen auch für Einsteiger*innen eine hohe Komplexität aufweisen können, wird hier nur auf diese Quelle verwiesen. 4.7.4 Klammern in RStudio Um die Übersicht über die Klammern zu erhalten, empfiehlt sich wie bereits erwähnt die Nutzung von RStudio:, da hier: Zusammengehörende Klammern hervorgehoben erscheinen Das Öffnen einer Klammer stets von einer sofortig erscheinenden Schließ-Klammer begleitet wird. Klammern auch gut nachträglich gesetzt werden können: Code z. B. mit der Maus markieren, Klammer eintippen, der Code erscheint automatisch in Klammern eingeschlossen Dies veranschaulicht Abb. 4.2. Figure 4.2: Klammern und visuelle Hilfen in RStudio 4.8 Übersichtlich Coden Informatiker*innen lernen bereits im Studium Grundsätze kennen, mit denen sich so übersichtlich Coden lässt, dass auch andere sich schnell in einem fremden Code zurechtfinden können. Glücklicherweise gibt es für die Anordnung von Code ein Tastenkürzel in R, das auch dabei hilft, selbst den Überblick zu behalten: Code markieren STRG + UMSCH + A` # vorher: demodata$geschlecht&lt;-factor(demodata$geschlecht,levels=c(1,2,3),labels=c(&quot;weiblich&quot;,&quot;männlich&quot;,&quot;divers&quot;)) # nachher: demodata$geschlecht &lt;- factor( demodata$geschlecht, levels = c(1, 2, 3), labels = c(&quot;weiblich&quot;, &quot;männlich&quot;, &quot;divers&quot;) ) 4.8.1 Schneller Coden: Shortcuts RStudio beherrscht diverse Tastenkombinationen, die einem den Coding-Alltag erleichtern, hier unsere Favoriten: [ALT] + [-] erzeugt den Zuweisepfeil &lt;- [STRG] + [R] oder [STRG] + [ENTER] führen den markierten Code aus (run). [STRG] + [ALT] + [B] führen den Code bis zur aktuellen Cursor-Position aus 4.9 Datensätze: Teil-Datensätze bilden Nun kommen wir zu einer für die Datenaufbereitung besonders wichtigen Fähigkeit in R – dem Bilden von Teil-Datensätzen. Diese werden zum Beispiel gebraucht, wenn eine Aufbereitung oder Analyse nur bestimmte Zeilen (z. B. Personen) oder Spalten (z. B. Variablen) berücksichtigen soll. Eine universelle Formel für das Bilden von Teildatensätzen lautet: datensatz[Zeilenauswahl, Spaltenauswahl] Es gibt hierbei mehrere Wege, Zeilen und/oder Spalten auszuwählen, hier einige Beispiele: Weg 1: Direkte Auswahl der Proband*innen (Zeilen) und Variablen (Spalten) über ihre Nummer: demodata #zeigt gesamten Datensatz (zu groß, daher hier nicht dargestellt) demodata[1, 3] #wählt Person 1 und Variable 3 aus ## [1] 4 # Person 1 hat also in der Variablen 3 die Ausprägung 4 demodata[c(1:3),] #wählt Personen 1, 2 und 3 in allen Variablen aus ## pcode VW.theo VW.prax VW.plan VW.durch VW.reflex Abitur_Fach_Biologie ## 1 01.01.2000 4 4 2 2 5 0 ## 2 01.04.1987 3 4 5 4 4 0 ## 3 06 3 3 4 4 4 0 ## Abitur_Fach_Chemie Abitur_Fach_Geographie Abitur_Fach_Geschichte ## 1 0 0 0 ## 2 0 0 0 ## 3 0 0 0 ## Abitur_Fach_Informatik Abitur_Fach_Physik Abitur_Fach_PW geschlecht alter ## 1 0 0 0 4 NA ## 2 0 0 0 4 NA ## 3 0 0 0 4 NA ## semester.pre abi dozent ## 1 NA NA Brämer ## 2 NA NA Brämer ## 3 NA NA Brämer # Folgendes wählt alle Personen, aber nur die Variablen 1, 2 und 3, aus # (entspricht ersten drei Spalten des Dataframes): demodata[, c(1:3)] # Output wäre zu groß, daher mit head(): head(demodata[, c(1:3)]) ## pcode VW.theo VW.prax ## 1 01.01.2000 4 4 ## 2 01.04.1987 3 4 ## 3 06 3 3 ## 4 1.NA2.NA3.284.10 2 1 ## 5 141651 5 5 ## 6 141651BIEN105411138 3 4 # Folgendes wählt alle Personen AUßER Person 1, 2 und 3 in allen Variablen aus. demodata[-c(1:3),] # Output wäre zu groß, daher mit head(): head(demodata[, -c(1:3)]) ## VW.plan VW.durch VW.reflex Abitur_Fach_Biologie Abitur_Fach_Chemie ## 1 2 2 5 0 0 ## 2 5 4 4 0 0 ## 3 4 4 4 0 0 ## 4 1 1 2 0 0 ## 5 4 4 5 0 0 ## 6 4 4 4 0 0 ## Abitur_Fach_Geographie Abitur_Fach_Geschichte Abitur_Fach_Informatik ## 1 0 0 0 ## 2 0 0 0 ## 3 0 0 0 ## 4 0 0 0 ## 5 0 0 0 ## 6 0 0 0 ## Abitur_Fach_Physik Abitur_Fach_PW geschlecht alter semester.pre abi dozent ## 1 0 0 4 NA NA NA Brämer ## 2 0 0 4 NA NA NA Brämer ## 3 0 0 4 NA NA NA Brämer ## 4 0 0 4 NA NA NA Brämer ## 5 0 0 4 NA NA NA Brämer ## 6 0 0 4 NA NA NA Brämer Weg 2: Auswahl der Variablen über den Variablennamen: # Folgendes wählt die Variable &quot;geschlecht&quot; für alle Personen aus. # Die Anführungszeichen kommen immer dann zum Einsatz, # wenn Text als Argument genutzt wird. head() liefert nur die ersten 7 Zeilen. head(demodata[, c(&quot;geschlecht&quot;)]) ## [1] keine Angabe keine Angabe keine Angabe keine Angabe keine Angabe ## [6] keine Angabe ## Levels: weiblich männlich divers keine Angabe Weg 3: Auswahl von Personen über Bedingungen: # Wählt alle weiblichen Personen und alle Variablen aus: demodata[demodata$geschlecht == &quot;weiblich&quot;, ] # Output wäre zu groß, daher mit head(): head(demodata[demodata$geschlecht == &quot;weiblich&quot;, ]) ## pcode VW.theo VW.prax VW.plan VW.durch VW.reflex Abitur_Fach_Biologie ## 13 ALLY2570 4 99 3 3 3 1 ## 14 ALNA2397 2 2 4 3 3 1 ## 21 Anan1913 1 2 1 1 1 1 ## 23 ANDA2157 2 2 3 3 3 1 ## 27 ANER0110 2 3 2 2 2 0 ## 31 ANIA1696 2 3 2 5 3 1 ## Abitur_Fach_Chemie Abitur_Fach_Geographie Abitur_Fach_Geschichte ## 13 1 1 1 ## 14 0 0 1 ## 21 1 1 1 ## 23 1 0 1 ## 27 0 0 1 ## 31 0 0 1 ## Abitur_Fach_Informatik Abitur_Fach_Physik Abitur_Fach_PW geschlecht alter ## 13 0 0 1 weiblich 2 ## 14 0 0 0 weiblich 6 ## 21 0 0 0 weiblich 5 ## 23 0 1 1 weiblich 7 ## 27 0 0 1 weiblich 17 ## 31 0 0 1 weiblich 5 ## semester.pre abi dozent ## 13 1 3.1 Brämer ## 14 1 2.7 Brämer ## 21 1 2.7 Brämer ## 23 1 3.1 Brämer ## 27 1 3.1 Brämer ## 31 1 2.7 Brämer #wählt nur Personen mit vollständigen Daten aus = Zeilen ohne &quot;NA&quot;: demodata[complete.cases(demodata),] # Output wäre zu groß, daher mit head(): head(demodata[complete.cases(demodata),]) ## pcode VW.theo VW.prax VW.plan VW.durch VW.reflex Abitur_Fach_Biologie ## 13 ALLY2570 4 99 3 3 3 1 ## 14 ALNA2397 2 2 4 3 3 1 ## 15 ALNA2754 1 2 1 1 1 1 ## 20 ANAN0770 4 5 6 6 4 1 ## 21 Anan1913 1 2 1 1 1 1 ## 23 ANDA2157 2 2 3 3 3 1 ## Abitur_Fach_Chemie Abitur_Fach_Geographie Abitur_Fach_Geschichte ## 13 1 1 1 ## 14 0 0 1 ## 15 0 0 1 ## 20 0 1 0 ## 21 1 1 1 ## 23 1 0 1 ## Abitur_Fach_Informatik Abitur_Fach_Physik Abitur_Fach_PW geschlecht alter ## 13 0 0 1 weiblich 2 ## 14 0 0 0 weiblich 6 ## 15 0 0 1 divers 16 ## 20 1 0 0 männlich 13 ## 21 0 0 0 weiblich 5 ## 23 0 1 1 weiblich 7 ## semester.pre abi dozent ## 13 1 3.1 Brämer ## 14 1 2.7 Brämer ## 15 1 2.4 Brämer ## 20 1 3.5 Brämer ## 21 1 2.7 Brämer ## 23 1 3.1 Brämer # Folgendes wählt nur Personen aus, die in der Variablen &quot;dozent&quot; den Text &quot;Brämer&quot; # enthalten. Dies geht allerdings nur, wenn die Variable vom Typ &quot;Character&quot; ist: demodata[grepl(&quot;Brämer&quot;, demodata$dozent),] # Output wäre zu groß, daher mit head(): head(demodata[grepl(&quot;Brämer&quot;, demodata$dozent),] ) ## pcode VW.theo VW.prax VW.plan VW.durch VW.reflex ## 1 01.01.2000 4 4 2 2 5 ## 2 01.04.1987 3 4 5 4 4 ## 3 06 3 3 4 4 4 ## 4 1.NA2.NA3.284.10 2 1 1 1 2 ## 5 141651 5 5 4 4 5 ## 6 141651BIEN105411138 3 4 4 4 4 ## Abitur_Fach_Biologie Abitur_Fach_Chemie Abitur_Fach_Geographie ## 1 0 0 0 ## 2 0 0 0 ## 3 0 0 0 ## 4 0 0 0 ## 5 0 0 0 ## 6 0 0 0 ## Abitur_Fach_Geschichte Abitur_Fach_Informatik Abitur_Fach_Physik ## 1 0 0 0 ## 2 0 0 0 ## 3 0 0 0 ## 4 0 0 0 ## 5 0 0 0 ## 6 0 0 0 ## Abitur_Fach_PW geschlecht alter semester.pre abi dozent ## 1 0 keine Angabe NA NA NA Brämer ## 2 0 keine Angabe NA NA NA Brämer ## 3 0 keine Angabe NA NA NA Brämer ## 4 0 keine Angabe NA NA NA Brämer ## 5 0 keine Angabe NA NA NA Brämer ## 6 0 keine Angabe NA NA NA Brämer #wählt Personen aus, die zwischen 17 und 19 Jahren alt sind: demodata[demodata$alter %in% c(17:19), ] ## pcode VW.theo VW.prax VW.plan VW.durch VW.reflex Abitur_Fach_Biologie ## 27 ANER0110 2 3 2 2 2 0 ## 251 JANA2801 2 2 2 1 2 1 ## 271 KAER0824 1 1 1 3 1 1 ## 476 SONA1510 4 5 2 4 3 0 ## Abitur_Fach_Chemie Abitur_Fach_Geographie Abitur_Fach_Geschichte ## 27 0 0 1 ## 251 0 1 1 ## 271 0 0 0 ## 476 1 0 1 ## Abitur_Fach_Informatik Abitur_Fach_Physik Abitur_Fach_PW geschlecht alter ## 27 0 0 1 weiblich 17 ## 251 0 0 0 weiblich 17 ## 271 0 0 0 weiblich 17 ## 476 0 0 1 weiblich 18 ## semester.pre abi dozent ## 27 1 3.1 Brämer ## 251 1 3.1 Brämer ## 271 1 2.7 Brämer ## 476 1 3.9 Brämer Auch Funktionen können Teil-Datensätze erzeugen, wie beispielsweise subset(). subset(demodata,demodata$alter %in% c(17:19)) #wählt Personen aus, die zwischen 17 und 19 Jahren alt sind ## pcode VW.theo VW.prax VW.plan VW.durch VW.reflex Abitur_Fach_Biologie ## 27 ANER0110 2 3 2 2 2 0 ## 251 JANA2801 2 2 2 1 2 1 ## 271 KAER0824 1 1 1 3 1 1 ## 476 SONA1510 4 5 2 4 3 0 ## Abitur_Fach_Chemie Abitur_Fach_Geographie Abitur_Fach_Geschichte ## 27 0 0 1 ## 251 0 1 1 ## 271 0 0 0 ## 476 1 0 1 ## Abitur_Fach_Informatik Abitur_Fach_Physik Abitur_Fach_PW geschlecht alter ## 27 0 0 1 weiblich 17 ## 251 0 0 0 weiblich 17 ## 271 0 0 0 weiblich 17 ## 476 0 0 1 weiblich 18 ## semester.pre abi dozent ## 27 1 3.1 Brämer ## 251 1 3.1 Brämer ## 271 1 2.7 Brämer ## 476 1 3.9 Brämer Um einen mit den Wegen 1 bis 3 ausgewählten Datensatz als Objekt in R zu speichern, nutzt man den Zuweisepfeil: # Links steht das neu kreierte Objekt, rechts ein bereits bekanntes demodata_vollstaendige_daten &lt;- demodata[complete.cases(demodata), ] Im obigen Beispiel wird ein Datensatz mit dem Namen “demodata_vollstaendige_daten” erzeugt, der dann für spätere Operationen genutzt werden kann. Mit apply(datensatz, 2, help_percentmissing) können auch missing-Quoten je Variable bestimmt werden.↩︎ Die hohen missing-Quoten stammen aus nicht-erfolgreich verlaufenden matchings und technischen Importartefakten wie ganzen NA-Zeilen: Es sind also verwaiste Fälle und technisch bedingte Leerfälle.↩︎ "],["datenaufbereitung-in-r-praxisguide.html", "5 Datenaufbereitung in R: Praxisguide 5.1 Motivation 5.2 Inhalte 5.3 Daten importieren 5.4 Überblick über die Daten verschaffen 5.5 Umbenennen von Variablen 5.6 Erste deskriptive Analyse 5.7 Einen Aufbereitungsplan machen 5.8 Datenaufbereitung durchführen 5.9 Aufbereitung nominaler Variablen (z. B. Ja/Nein) 5.10 Fortgeschrittene Datenaufbereitung", " 5 Datenaufbereitung in R: Praxisguide Autoren: Daniel Rehfeldt, Martin Brämer Herzlich Willkommen zum Beitrag Datenaufbereitung in R. Um einige Funktionen nutzen zu können, werden zunächst Pakete in R installiert und geladen: # Installiere und lade die benötigten Pakete # install.packages(&quot;xlsx&quot;) # install.packages(&quot;foreign&quot;) # install.packages(&quot;plyr&quot;) # install.packages(&quot;psych&quot;) library(xlsx) # hilft beim Import von Excel-Daten library(foreign) # hilft beim Import von SPSS-Daten library(plyr) # enthält die nützliche rename()-Funktion zum Umbenennen von Variablennamen library(psych) # liefert psychometrische Funktionen 5.1 Motivation Vor jeder statistischen Analyse steht ein nur selten ehrlich beschriebener Prozess, die Datenaufbereitung. Diese ist essenziell, um einen Überblick über die Güte der Rohdaten zu erhalten und überhaupt weitere Analysen adäquat durchführen zu können. Beim manuellen Aufbereiten von Daten, sei es händisch in Excel oder SPSS, kann es sehr schnell zu Unaufmerksamkeiten und demnach zu Fehlern kommen, die aufgrund ihrer Nicht-Nachvollziehbarkeit schwere Konsequenzen für die Qualität des Datenmaterials und somit die spätere Analyse haben können. Wir verdeutlichen das einmal an einem Beispiel: Eine versehentliche Verschiebung der Spaltennamen um 1 nach rechts kann dazu führen, dass der komplette Datensatz unnachvollziehbar beschädigt wird. Denn im Nachhinein erscheint es gerade bei großen Datensätzen fast unmöglich zu entscheiden, zu welcher Variable die jeweilige Spalte von Werten gehört. Hier zeigt sich eine Stärke Code-basierter Statistiksoftware: Jede Manipulation am Datensatz ist schriftlich fixiert und jederzeit retrospektiv nachvollzieh- oder veränderbar. Das praktische Rüstzeug für eine basale Datenaufbereitung in R soll dieser Praxisguide liefern. Das Wissen zu vielen Basis-Funktionen, Datentypen und vielem mehr sind in unserem anderen Beitrag zum Basiswissen für R enthalten, deren Kenntnis in der nachfolgenden statistischen Analyse von Vorteil sein kann. 5.2 Inhalte Es handelt sich bei allen genutzten Beispielen um Datensätze, die bei Fragebogen-Erhebungen mit Likert-Skalen sowie Angaben zur Person allgemein oder Ähnlichem entstehen. Eine Übertragbarkeit auf andere quantitative Erhebungsformen ist aber oft gegeben. Der Praxisguide umfasst diverse übliche Schritte der Datenaufbereitung: Importieren von Daten Überblick über die Daten gewinnen Anfertigen eines Plans zur Datenaufbereitung Typische Verfahren der Datenaufbereitung (z. B. Manipulieren von Variablen) Verschmelzen von Daten (Merging, z. B. bei Längsschnitten). Dabei wird bis zum Abschnitt 5.7.2 eher funktionsorientiert und unter Nutzung diverser, teils vereinfachter Datensätze operiert, um wesentliche Funktionen für die Datenaufbereitung in R kennenzulernen. Das Herzstück des Beitrags bildet dann die systematische Aufbereitung eines realen, stark verschmutzten Datensatzes ab Abschnitt 5.7.2. Begonnen wird dieser Beitrag aber zunächst mit einer der besten Möglichkeiten der Datenaufbereitung – schmutzige Daten gar nicht erst entstehen zu lassen. 5.2.1 Hinweise für quantitative Befragungen – Präventive Datenaufbereitung Die Datenaufbereitung beginnt nicht erst bei der ersten Betrachtung des Rohdatensatzes, sondern bereits vorher – bei einem datensensiblen Befragungsdesign. Gemeint sind hiermit alle Maßnahmen innerhalb der Befragungssoftware oder des Papierfragebogens, die den Aufwand einer nachträglichen Aufbereitung verringern. Ein Beispiel wäre etwa die Nutzung von Personencodes mit Einführung von Restriktionen. Besteht ein solcher Code etwa aus vier Buchstaben und vier Ziffern, so kann ein und dieselbe Person z. B. “ERNW1665” oder “ernw1665” oder “eRnW1665” schreiben. Wird hier – sofern die Erhebungssoftware dies erlaubt – die Restriktion auf Großbuchstaben eingeführt, so ist ein Aufbereitungsschritt gespart. Wenn zudem (beim Online-Fragebogen) die Restriktion eingeführt wird, dass der Fragebogen nur fortgesetzt werden kann, wenn tatsächlich vier Buchstaben, gefolgt von vier Ziffern niedergeschrieben wurden, vermeidet man Codes wie “ERNW16”, “1665” usw. – und ja, dies sind reale Beispiele. Weiters sollten Antwortoptionen erschöpfend sein (Porst, 2014), um die Datenaufbereitung und -interpretation zu gewährleisten. Die Frage Welche der folgenden Fächer belegten Sie im Abitur?, gefolgt von den Ankreuzmöglichkeiten Physik und Chemie führt nämlich zu der Frage, welche Interpretation erfolgt, wenn keine der beiden Möglichkeiten angekreuzt wurde. Hat die Person die Frage übersprungen? Hat sie beide Fächer nicht belegt? Es müsste hier die Antwortoption keine der beiden unbedingt ergänzt werden. Ein letztes exemplarisches Beispiel ist die Abitur- oder Hochschulzugangsberechtigungsnote. Die Wahl eines Freitextfeldes zur Beantwortung der Frage ist hier unbedingt zu vermeiden, da sonst Antworten wie “sehr gut”, “weiß ich nicht”, “zweikommazwei”, “1.3” oder “1,7” erfolgen können, ein Albtraum für die Datenaufbereitung. Ich empfehle hier je nach Genauigkeitswunsch entweder a) das Nutzen von Ankreuzmöglichkeiten in Ranges, also “1,0–1,3”, “1,4–1,7”, …, und weiß ich nicht oder bei guter Befragungssoftware das Nutzen von Feldern, die nur die Schreibweise [Ziffer],[Ziffer] zulassen. 5.3 Daten importieren Nun beginnt der tatsächliche Anwendungsfall einer Datenbereinigung mit einem großen und unübersichtlichen Datensatz, der mit dem Import der Daten in R beginnt. Je nach Ausgangsformat der Daten gibt es hierzu eine Anleitung. R bietet den großen Vorteil, dass es mit allen regulären und auch exotischen Datentypen umgehen kann. An dieser Stelle erscheint es zudem wichtig, eine gängige Praxis beim Erstkontakt mit Rohdaten anzusprechen: Rohdaten sollten sofort nach dem Erhalt kopiert und separat gespeichert werden. Diese Rohdaten sollten in keiner Weise verändert oder manipuliert werden, sie dienen als wertvolle Rückversicherung, falls bei der Datenaufbereitung und/oder dem Abspeichern manipulierter Daten ein gravierender Fehler unterlaufen sollte. 5.3.1 Import von Excel Daten, die auf .xlsx oder .xls enden, können folgendermaßen importiert werden: demodata_xlsx &lt;- read.xlsx(&quot;Dateiname.xlsx&quot;, 1) # &quot;Dateiname.xls&quot; bei xls-Dateien Dies importiert einen Standard-Excel-Datensatz korrekt, der Daten im Excel-Arbeitsblatt 1 enthält und in der ersten Zeile die Variablennamen enthält. Weitere Optionen siehe hier. 5.3.2 Import von CSV Daten die auf .csv enden, können folgendermaßen importiert werden: demodata_csv &lt;- read.csv2(file = &quot;Dateiname.csv&quot;) 5.3.3 Import von SPSS Daten die auf .sav enden und aus SPSS stammen, können folgendermaßen importiert werden (mehr Erläuterungen finden sich hier): demodata_spss &lt;- read.spss( &quot;Dateiname.sav&quot;, use.value.labels = F, to.data.frame = T, use.missings = T, trim.factor.names = T ) Die angegebenen Optionen stellen in vielen Fällen eine gute Ausgangsbasis dar. Sie werden hier einmal erläutert: use.value.labels=F Gerade Ratingskalen-Items (z. B. stimme nicht zu bis stimme voll zu) werden beim automatischen Import als Factor-Variablen deklariert (mit z. B. den Levels stimme nicht zu, stimme ein wenig zu, stimme zu, stimme voll zu). Wenn dies gewünscht ist, kann die Option auf TRUE gesetzt werden. Setzt man auf FALSE, so werden die Antwort-Optionen als Zahlen angegeben, also z.B. 1 bis 4. Dies gilt dann aber auch für echte Factor-Variablen, wie das Geschlecht, hier sollte nachträglich im SPSS-Datensatz eine Rückversicherung stattfinden, ob etwa bei der Variablen Geschlecht die Zahl 1 der Ausprägung männlich, weiblich oder divers entsprach. to.data.frame = T Damit werden SPSS-Daten standardmäßig als Dataframe importiert. use.missings = T Dies ist relevant, falls in SPSS verschiedene Arten von missing values definiert wurden (z. B. missing at random, missing by design usw.). Diese Informationen bleiben so erhalten. trim.factor.names = T Factor-Variablen werden beim Import automatisch erkannt, diese Option sorgt dafür, dass am Anfang und Ende eines Factor-Level-Namens keine Leerzeichen stehen, also z.B. \"Gruppe 1\" statt \" Gruppe 1 \". 5.4 Überblick über die Daten verschaffen Nach dem Daten-Import sollte zunächst ein Überblick über die neuen Dataframes in R gewonnen werden. 5.4.1 Erster Überblick mit den Funktionen nrow(), ncol() und View() 5.4.1.1 nrow() und ncol() Diese beiden basalen und oft nützlichen Funktionen liefern die Anzahl der Zeilen (nrow(), i. d. R. Proband:innenanzahl) respektive die Anzahl der Spalten (ncol(), i. d. R. Variablenanzahl). Nach der Datenbereinigung (also bspw. dem Entfernen von Missings) kann über nrow() z. B. die Stichprobengröße ermittelt werden. ncol() dient z. B. zur Überprüfung, ob nach komplexen Operationen die verbleibende Variablenzahl plausibel ist. Wir benötigen zunächst einen Datensatz. Er ist im GitHub-Verzeichnis dieser Publikation verfügbar. # Importiert den Datensatz. Dieser liegt im R-eigenen rds-Format vor: datensatz_roh &lt;- readRDS(file = &quot;./data_for_chapters/baseknowledge_R/df_unrein_1_neu.rds&quot;) #speichere ein Duplikat des datensatz_roh für spätere Reaktivierung: datensatz_roh_duplikat_1 &lt;- datensatz_roh # View(datensatz_roh) # Dies wird ausgeführt, um einen visuellen Gesamteindruck zu erhalten. # Aus View() wird deutlich: # 1) Erst ab Person 11 sind sinnvolle Personencodes vorhanden # (die ersten 10 waren Testdurchläufe der Testleitenden) # 2) Die Variable &quot;dozent&quot; enthält IMMER den Text &quot;brämer&quot; # Es folgt ein Beispiel zur Nutzung von nrow() und ncol(): nrow(datensatz_roh) #Zeilen (Personen) Zahl vorher ## [1] 546 datensatz_bearbeitet &lt;- datensatz_roh[-c(1:10),] #Entferne die ersten 10 Zeilen (= Personen) nrow(datensatz_bearbeitet) #Zeilen (Personen) Zahl hinterher ## [1] 536 ncol(datensatz_bearbeitet) #Spalten (Variablen) Zahl vorher ## [1] 18 names(datensatz_bearbeitet) # Prüfe, an welcher Stelle des Datensatzes die Variable &quot;dozent&quot; steht ## [1] &quot;pcode&quot; &quot;VW.theo&quot; &quot;VW.prax&quot; ## [4] &quot;VW.plan&quot; &quot;VW.durch&quot; &quot;VW.reflex&quot; ## [7] &quot;Abitur_Fach_Biologie&quot; &quot;Abitur_Fach_Chemie&quot; &quot;Abitur_Fach_Geographie&quot; ## [10] &quot;Abitur_Fach_Geschichte&quot; &quot;Abitur_Fach_Informatik&quot; &quot;Abitur_Fach_Physik&quot; ## [13] &quot;Abitur_Fach_PW&quot; &quot;geschlecht&quot; &quot;alter&quot; ## [16] &quot;semester.pre&quot; &quot;abi&quot; &quot;dozent&quot; # Es ist also die Stelle 18 datensatz_bearbeitet &lt;- datensatz_bearbeitet[,-c(18)] #entferne Variable 18, dozent. ncol(datensatz_bearbeitet) #Spalten (Variablen) Zahl hinterher ## [1] 17 5.4.2 Vertiefung der View()-Funktion Einen, wie oben bereits angedeutet, eher gewohnten, grafischen Überblick gibt die View()-Funktion (vgl. Abb. 5.1 und 5.2), wir stellen vorher noch den Rohdatensatz wieder her: #wiederherstellen des Rohdatensatzes datensatz_roh &lt;- datensatz_roh_duplikat_1 View(datensatz_roh) Figure 5.1: View-Tab in RStudio (von Daniel Rehfeldt) Figure 5.2: Filter-Funktion und Show in new Window-Funktion (von Daniel Rehfeldt) In den Abbildungen 5.1 und 5.2 sind die verschiedenen Funktionen innerhalb der View-Ansicht hervorgehoben. Damit lässt sich der Datensatz in einem eigenen Fenster anzeigen. ein Filter hinzufügen (z. B. Ansicht so, dass nur diejenigen Personen zu sehen sind, die Physik im Abitur hatten). im Datensatz eine Stichwortsuche vollführen. der Datensatz nach einer Variablen alphabetisch/numerisch sortieren. 5.5 Umbenennen von Variablen Es gibt unzählige Wege, Variablen in R umzubenennen. Den für uns anschaulichsten Weg präsentieren wir hier. Der Datensatz für das Beispiel ist im GitHub-Verzeichnis dieser Publikation verfügbar. Im Rahmen der rename()-Funktion des plyr-Pakets wird immer wieder die Schreibweise neuer Variablenname = alter Variablenname benutzt. So bleibt stets ersichtlich, welcher Variablenname durch welchen ersetzt wurde: # Lade zunächst als Beispiel einen Rohdatensatz mit ungünstigen Variablennamen: datensatz_unbenannt &lt;- readRDS(file = &quot;./data_for_chapters/data_cleaning/df_POST_1_neu.rds&quot;) # Prüfe die Variablennamen: names(datensatz_unbenannt) ## [1] &quot;fr_277654&quot; &quot;fr_277655&quot; &quot;fr_277656&quot; &quot;fr_277657&quot; &quot;fr_277658&quot; &quot;fr_277659&quot; ## [7] &quot;fr_277660&quot; &quot;fr_277661&quot; &quot;fr_277662&quot; &quot;fr_277663&quot; &quot;fr_277664&quot; &quot;fr_277665&quot; ## [13] &quot;fr_277666&quot; &quot;fr_277667&quot; &quot;fr_277668&quot; &quot;fr_277669&quot; &quot;fr_277670&quot; &quot;fr_277671&quot; #Benenne um (hierfür wurde vorher eine Zuordnungstabelle o. Ä. aus dem Erhebungsmanual gebraucht): datensatz_renamed &lt;- plyr::rename( datensatz_unbenannt, replace = c( &quot;fr_277654&quot; = &quot;pcode&quot;, &quot;fr_277655&quot; = &quot;VW.theo&quot;, &quot;fr_277656&quot; = &quot;VW.prax&quot;, &quot;fr_277657&quot; = &quot;VW.plan&quot;, &quot;fr_277658&quot; = &quot;VW.durch&quot;, &quot;fr_277659&quot; = &quot;VW.reflex&quot;, &quot;fr_277660&quot; = &quot;Abitur_Fach_Biologie&quot;, &quot;fr_277661&quot; = &quot;Abitur_Fach_Chemie&quot;, &quot;fr_277662&quot; = &quot;Abitur_Fach_Geographie&quot;, &quot;fr_277663&quot; = &quot;Abitur_Fach_Geschichte&quot;, &quot;fr_277664&quot; = &quot;Abitur_Fach_Informatik&quot;, &quot;fr_277665&quot; = &quot;Abitur_Fach_Physik&quot;, &quot;fr_277666&quot; = &quot;Abitur_Fach_PW&quot;, &quot;fr_277667&quot; = &quot;geschlecht&quot;, &quot;fr_277668&quot; = &quot;alter&quot;, &quot;fr_277669&quot; = &quot;semester.pre&quot;, &quot;fr_277670&quot; = &quot;abi&quot;, &quot;fr_277671&quot; = &quot;dozent&quot; ) ) # Variablennamen hinterher: names(datensatz_renamed) ## [1] &quot;pcode&quot; &quot;VW.theo&quot; &quot;VW.prax&quot; ## [4] &quot;VW.plan&quot; &quot;VW.durch&quot; &quot;VW.reflex&quot; ## [7] &quot;Abitur_Fach_Biologie&quot; &quot;Abitur_Fach_Chemie&quot; &quot;Abitur_Fach_Geographie&quot; ## [10] &quot;Abitur_Fach_Geschichte&quot; &quot;Abitur_Fach_Informatik&quot; &quot;Abitur_Fach_Physik&quot; ## [13] &quot;Abitur_Fach_PW&quot; &quot;geschlecht&quot; &quot;alter&quot; ## [16] &quot;semester.pre&quot; &quot;abi&quot; &quot;dozent&quot; 5.6 Erste deskriptive Analyse Will man einen ersten Überblick über deskriptive, quantitative Statistiken gewinnen (z. B. für Items aus Skalen oder numerische Kovariaten wie das Alter), so stehen diverse Funktionen zur Verfügung. 5.6.1 Deskriptive Statistiken Die summary()-Funktion liefert in Blöcken einen Überblick über jede Variable, inklusive Mittelwert, Median und fehlenden Werten. Nützlich sind zudem die Angaben des Minimal- und Maximalwertes, z. B., um Skalenwerte außerhalb des definierten Bereichs zu erkennen. summary(datensatz_renamed) ## pcode VW.theo VW.prax VW.plan ## ANAN0770 : 2 Min. :1.000 Min. : 1.000 Min. :1.000 ## Anan1913 : 1 1st Qu.:2.000 1st Qu.: 2.000 1st Qu.:1.000 ## 01.01.2000 : 1 Median :3.000 Median : 3.000 Median :2.000 ## 01.04.1987 : 1 Mean :2.705 Mean : 3.061 Mean :2.566 ## 06 : 1 3rd Qu.:4.000 3rd Qu.: 4.000 3rd Qu.:4.000 ## 1.NA2.NA3.284.10: 1 Max. :5.000 Max. :99.000 Max. :6.000 ## (Other) :539 NA&#39;s :221 NA&#39;s :220 NA&#39;s :221 ## VW.durch VW.reflex Abitur_Fach_Biologie Abitur_Fach_Chemie ## Min. :1.00 Min. :1.000 Min. :1 Min. :1 ## 1st Qu.:2.00 1st Qu.:1.000 1st Qu.:1 1st Qu.:1 ## Median :3.00 Median :2.000 Median :1 Median :1 ## Mean :2.62 Mean :2.495 Mean :1 Mean :1 ## 3rd Qu.:3.00 3rd Qu.:3.000 3rd Qu.:1 3rd Qu.:1 ## Max. :6.00 Max. :6.000 Max. :1 Max. :1 ## NA&#39;s :220 NA&#39;s :221 NA&#39;s :391 NA&#39;s :466 ## Abitur_Fach_Geographie Abitur_Fach_Geschichte Abitur_Fach_Informatik ## Min. :1 Min. :1 Min. :1 ## 1st Qu.:1 1st Qu.:1 1st Qu.:1 ## Median :1 Median :1 Median :1 ## Mean :1 Mean :1 Mean :1 ## 3rd Qu.:1 3rd Qu.:1 3rd Qu.:1 ## Max. :1 Max. :1 Max. :1 ## NA&#39;s :459 NA&#39;s :391 NA&#39;s :531 ## Abitur_Fach_Physik Abitur_Fach_PW geschlecht alter semester.pre ## Min. :1 Min. :1 1:154 Min. : 1.000 1 :175 ## 1st Qu.:1 1st Qu.:1 2: 25 1st Qu.: 4.000 3 : 3 ## Median :1 Median :1 3: 2 Median : 5.000 5 : 1 ## Mean :1 Mean :1 4:365 Mean : 7.236 NA:367 ## 3rd Qu.:1 3rd Qu.:1 3rd Qu.: 9.000 ## Max. :1 Max. :1 Max. :33.000 ## NA&#39;s :476 NA&#39;s :462 NA&#39;s :364 ## abi dozent ## Min. :1.100 Length:546 ## 1st Qu.:2.475 Class :character ## Median :2.700 Mode :character ## Mean :2.721 ## 3rd Qu.:3.100 ## Max. :3.900 ## NA&#39;s :364 Die describe()-Funktion aus dem psych-Package liefert je Variable eine Zeile mit deutlich mehr Statistiken. Es sind gültige Fälle (und damit auch fehlende Werte), Mittelwert, Standardabweichung, Median, Min, Max, Schiefe, Kurtosis und Standardfehler angegeben. Für character-Variablen erhält man allerdings keine sinnvollen Angaben. describe(datensatz_renamed) ## vars n mean sd median trimmed mad min max ## pcode* 1 546 272.54 157.70 272.5 272.50 202.37 1.0 545.0 ## VW.theo 2 325 2.70 1.20 3.0 2.69 1.48 1.0 5.0 ## VW.prax 3 326 3.06 5.49 3.0 2.70 1.48 1.0 99.0 ## VW.plan 4 325 2.57 1.34 2.0 2.44 1.48 1.0 6.0 ## VW.durch 5 326 2.62 1.33 3.0 2.50 1.48 1.0 6.0 ## VW.reflex 6 325 2.50 1.28 2.0 2.39 1.48 1.0 6.0 ## Abitur_Fach_Biologie 7 155 1.00 0.00 1.0 1.00 0.00 1.0 1.0 ## Abitur_Fach_Chemie 8 80 1.00 0.00 1.0 1.00 0.00 1.0 1.0 ## Abitur_Fach_Geographie 9 87 1.00 0.00 1.0 1.00 0.00 1.0 1.0 ## Abitur_Fach_Geschichte 10 155 1.00 0.00 1.0 1.00 0.00 1.0 1.0 ## Abitur_Fach_Informatik 11 15 1.00 0.00 1.0 1.00 0.00 1.0 1.0 ## Abitur_Fach_Physik 12 70 1.00 0.00 1.0 1.00 0.00 1.0 1.0 ## Abitur_Fach_PW 13 84 1.00 0.00 1.0 1.00 0.00 1.0 1.0 ## geschlecht* 14 546 3.06 1.36 4.0 3.20 0.00 1.0 4.0 ## alter 15 182 7.24 5.58 5.0 6.10 2.97 1.0 33.0 ## semester.pre* 16 546 3.03 1.40 4.0 3.16 0.00 1.0 4.0 ## abi 17 182 2.72 0.48 2.7 2.74 0.44 1.1 3.9 ## dozent* 18 546 1.00 0.00 1.0 1.00 0.00 1.0 1.0 ## range skew kurtosis se ## pcode* 544.0 0.00 -1.21 6.75 ## VW.theo 4.0 0.10 -1.15 0.07 ## VW.prax 98.0 16.34 282.54 0.30 ## VW.plan 5.0 0.48 -0.78 0.07 ## VW.durch 5.0 0.51 -0.53 0.07 ## VW.reflex 5.0 0.49 -0.75 0.07 ## Abitur_Fach_Biologie 0.0 NaN NaN 0.00 ## Abitur_Fach_Chemie 0.0 NaN NaN 0.00 ## Abitur_Fach_Geographie 0.0 NaN NaN 0.00 ## Abitur_Fach_Geschichte 0.0 NaN NaN 0.00 ## Abitur_Fach_Informatik 0.0 NaN NaN 0.00 ## Abitur_Fach_Physik 0.0 NaN NaN 0.00 ## Abitur_Fach_PW 0.0 NaN NaN 0.00 ## geschlecht* 3.0 -0.78 -1.34 0.06 ## alter 32.0 2.15 5.14 0.41 ## semester.pre* 3.0 -0.74 -1.44 0.06 ## abi 2.8 -0.33 1.07 0.04 ## dozent* 0.0 NaN NaN 0.00 Für deskriptive Statistiken bei Gruppenvergleichen lohnt sich ein Blick auf describeBy() aus dem psych-Package. Hier wird über ein zweites Argument group = Gruppierungsvariable eine Variable festgelegt, die die Auswertung je Gruppe separat ermöglicht. Der Output zeigt die identischen Statistiken wie describe(), allerdings je Gruppe. Empfehlenswert ist diese Funktion aber meist erst nach einer groben Datenbereinigung, in unserem Fall müsste beispielweise bei der Variablen Abitur_Fach_Physik der Wert NA teils durch 0 ersetzt werden, mehr dazu in Abschnitt 5.9. Soll ein Überblick über Kreuztabellen gewonnen werden, bietet sich die table()-Funktion an. Diese liefert theoretisch in unserem Beispiel die Anzahlen der Proband:innen, die sowohl Physik als auch Biologie im Abitur belegten, die Anzahl der Proband:innen, die weder Physik noch Biologie belegten und die Anzahlen derjenigen, die je eines, aber nicht das andere belegten. Die erste Variable in table(Variable 1, Variable 2) steht im Output immer links und die zweite oben. table(datensatz_renamed$Abitur_Fach_Biologie, datensatz_renamed$Abitur_Fach_Physik) ## ## 1 ## 1 56 Interpretation: Es wird nur der Fall (1, 1) angezeigt, also diejenigen Personen, die sowohl Physik, als auch Biologie belegten. Weshalb dies so angezeigt wird, kann über eine Inspektion mittels View() nachvollzogen werden: View(datensatz_renamed[,c(&quot;Abitur_Fach_Physik&quot;,&quot;Abitur_Fach_Biologie&quot;)]) Es ist hier ersichtlich, dass im Datensatz (durch die Erhebungssoftware begründet) je NA, also ein fehlender Wert vergeben wurde für nicht im Abitur belegt. R schließt diese fehlenden Werte bei vielen Funktionen aus Analysen aus. Es gibt aber auch oft Möglichkeiten, diese NA in der Analyse anzuzeigen, bei der table()-Funktion gelingt dies mit useNA: table(datensatz_renamed$Abitur_Fach_Biologie, datensatz_renamed$Abitur_Fach_Physik, useNA = &quot;always&quot;) ## ## 1 &lt;NA&gt; ## 1 56 99 ## &lt;NA&gt; 14 377 Interpretation: 377 Studierende belegten weder Physik noch Biologie, 56 beides, 14 nur Physik und 99 nur Biologie. Notiz für die spätere umfassende Datenaufbereitung: Die NAs in den Abitur-Variablen sollten in typische Werte wie z. B. 0 = nein überführt werden. 5.6.2 Graphische Übersicht hist() und plot() Für nummerische Variablen kann mittels hist(Datensatz$Variable) ein Histogramm erzeugt werden: hist(datensatz_renamed$alter) Figure 5.3: Histogramm Interpretation: Hier ist zukünftig wohl eine Datenaufbereitung nötig, es handelte sich nämlich ausschließlich um erwachsene Studierende und das Alter umfasst im Histogramm scheinbar auch Kinder und Jugendliche. Die Funktion plot(Datensatz$Variable1, Datensatz$Variable2) kann einen Scatterplot erzeugen: plot(datensatz_renamed$alter, datensatz_renamed$VW.prax) Figure 5.4: Scatterplot Interpretation: In der Variablen VW.prax scheint es einen Ausreißer in der Größenordnung 100 zu geben. Eigentlich verläuft die Skala im Bereich 1 bis 6. Dies sind zwei basale Visualisierungsoptionen, weitere, professionellere Optionen liefert z. B. das Paket ggplot. Damit gelingt auch eine professionell gestaltete Präsentation von Analyseergebnissen, nachfolgend finden sich Beispiele: Beispiel-Visualisierungen ggplot (von plosone-phylo, lizensiert unter CC BY 2.0. https://creativecommons.org/licenses/by/2.0/?ref=openverse.) 5.7 Einen Aufbereitungsplan machen 5.7.1 Beispiele für “schmutzige” Daten Nachdem ein Überblick über die Daten gewonnen wurde, sind bereits einige Bereinigungsschritte deutlich geworden, hier ein paar Beispiele: Es gibt leere oder spaltenweit identisch ausgeprägte Variablen, z. B. Dozent, stets mit der Ausprägung Brämer. Variablennamen sind nicht adäquat, das Programm Unizensus der Freien Universität Berlin etwa liefert standardmäßig durchnummerierte Variablennamen wie fr_122350, die nichts über den Inhalt der Variablen aussagen und aussagekräftig umbenannt werden sollten. Es existieren leere Zeilen, also Proband:innen, die keine Frage beantwortet haben. Dies können Testversuche der Testentwickelnden vor dem eigentlichen Befragungsstart gewesen sein oder schlicht Proband:innen, die die Umfrage nur scheinbar absolviert haben. Es gibt identische Fälle (Zeilen) im Datensatz. Dies kann Erhebungssoftware-bedingt sein oder durch Verschmelzen von Datensätzen passieren. Variablen weisen inadäquate Ausprägungen auf, dies konnte bereits im Scatterplot in Abb. 5.4 vermutet werden. Ein anderes Beispiel sind Variablen wie Seminar mit den Ausprägungen 1, 2 und 3. Diese könnte bspw. eine Frage an die Proband:innen danach sein, welches Seminar sie gerade besuchen, das bei Herrn Müller, bei Frau Genf oder bei Frau Darn. Je nach Anordnung dieser Antwortoptionen kann nun 1 für Herrn Müller, Frau Genf oder Frau Darn stehen, dies sollte deutlicher aus der Variablen herauszulesen sein, beispielweise über die Bennenung als factor-Variablen (vgl. z. B. Abschnitt 5.9). Likert-skalierte Variablen sind manchmal invertiert, z. B. wenn bei Fragebögen einmal positiv und einmal negativ gepolt befragt wird (Physik macht mir Spaß. vs. Physik reizt mich nicht.). Zur Anonymisierung verwendete Personencodes wurden von den Proband:innen uneinheitlich eingegeben, etwa “ERHU0502”, “ebji8706”, “ER33” usw. Eine Variable enthält versetzte Werte: Beispiel: Alter in Jahren wurde über ein Dropdown-Menü mit den Zahlen 17 bis 70 (für Studierende) erfragt. Nun setzt das Erhebungsprogramm z. B. eine “1” für 17 Jahre, eine “2” für 18 Jahre usw. Es gibt also einen Versatz von 16 Jahren. Textlastige Variablen weisen Leerzeichen o. Ä. am Anfang und/oder Ende der Antwort auf. Ein Beispiel: Die Variable Kommentar wird zur Erfassung von Kommentaren zur Erhebung genutzt und darf mit bis zu 160 Zeichen beschrieben werden. Darin finden sich Antworten wie “     Ich fand die Befragung zu lang     ”. Dies kann zu Darstellungsproblemen in R führen. Beim Import von Variablen kann die automatische Erkennung und Faktorisierung von Variablen nützlich sein, die numerischen Ausprägungen der Antwortoptionen sind allerdings manchmal unbrauchbar. Ein Beispiel: Die Variable geschlecht mit Antwortoptionen wie männlich, weiblich, divers , keine Angabe wurde beim Import automatisch faktorisiert. Die zu den Labels männlich, weiblich, divers, keine Angabe gehörenden numerischen Werte wurden als 2, 3, 4 und 5 festgelegt. Fehlende Werte enthalten eigentlich Informationen: Eine Frage wie Welche dieser Fächer belegten Sie im Abitur (Mehrfachantworten möglich)? Physik Chemie Biologie Dies wird von mancher Erhebungs-Software in drei dichotome Ja/NA-Variablen aufgeteilt, in denen sie das setzen eines Kreuzes als Ja, das weglassen als NA registrieren. Das Weglassen des Kreuzes entspricht in diesem Beispiel jedoch einem Nein. Steht jedoch bei allen Fächern ein NA, so ist nicht klar, ob die Person die Frage einfach nicht beantwortet hat, oder ob sie tatsächlich alle drei Fächer nicht im Abitur belegt hatte. Eine weitere Antwortoption keine dieser drei oder eine freie Antwortmöglichkeit Sonstiges, und zwar: wäre hier sinnvoll (vgl. Abschnitt 5.2.1). 5.7.2 Detaillierte Erstellung eines Aufbereitungsplans an einem echten Beispiel Wir starten mit dem Import der Rohdaten und mit der View()-Funktion, um Informationen für unseren Aufbereitungsplan zu erhalten: # Importiere Rohdaten. Diese liegen hier im R-eigenen rds-Format vor: datensatz_roh &lt;- readRDS(&quot;./data_for_chapters/data_cleaning/df_POST_1_neu.rds&quot;) View(datensatz_roh) Zunächst sollte ein Umbennenen der Variablen erfolgen. Hilfreich kann dazu das Original-Befragungsdokument sein oder eine Tabelle, die vor der Befragung die Reihenfolge der Items festlegte (Noch besser wäre eine Befragungssoftware, in der die Namen selbst festgelegt werden können, z. B. SosciSurvey). datensatz_renamed &lt;- plyr::rename(datensatz_roh, replace=c(&quot;fr_277654&quot;=&quot;pcode&quot;, &quot;fr_277655&quot;=&quot;VW.theo&quot;, &quot;fr_277656&quot;=&quot;VW.prax&quot;, &quot;fr_277657&quot;=&quot;VW.plan&quot;, &quot;fr_277658&quot;=&quot;VW.durch&quot;, &quot;fr_277659&quot;=&quot;VW.reflex&quot;, &quot;fr_277660&quot;=&quot;Abitur_Fach_Biologie&quot;, &quot;fr_277661&quot;=&quot;Abitur_Fach_Chemie&quot;, &quot;fr_277662&quot;=&quot;Abitur_Fach_Geographie&quot;, &quot;fr_277663&quot;=&quot;Abitur_Fach_Geschichte&quot;, &quot;fr_277664&quot;=&quot;Abitur_Fach_Informatik&quot;, &quot;fr_277665&quot;=&quot;Abitur_Fach_Physik&quot;, &quot;fr_277666&quot;=&quot;Abitur_Fach_PW&quot;, &quot;fr_277667&quot;=&quot;geschlecht&quot;, &quot;fr_277668&quot;=&quot;alter&quot;, &quot;fr_277669&quot;=&quot;semester.pre&quot;, &quot;fr_277670&quot;=&quot;abi&quot;, &quot;fr_277671&quot;=&quot;dozent&quot; )) #Überprüfe: names(datensatz_renamed) ## [1] &quot;pcode&quot; &quot;VW.theo&quot; &quot;VW.prax&quot; ## [4] &quot;VW.plan&quot; &quot;VW.durch&quot; &quot;VW.reflex&quot; ## [7] &quot;Abitur_Fach_Biologie&quot; &quot;Abitur_Fach_Chemie&quot; &quot;Abitur_Fach_Geographie&quot; ## [10] &quot;Abitur_Fach_Geschichte&quot; &quot;Abitur_Fach_Informatik&quot; &quot;Abitur_Fach_Physik&quot; ## [13] &quot;Abitur_Fach_PW&quot; &quot;geschlecht&quot; &quot;alter&quot; ## [16] &quot;semester.pre&quot; &quot;abi&quot; &quot;dozent&quot; # Überblicke den Datensatz: View(datensatz_renamed) Die erste Variable stellt augenscheinlich den Personencode (pcode) dar. Für unsere Studie hatten wir einen Personencode genutzt, der aus vier Großbuchstaben und vier Ziffern bestand. Wir entdecken hier folgende Unzulänglichkeiten (z. B. über View(df)): Manche pcodes enthalten ein Datum. Manche pcodes sind kürzer als acht Stellen. Manche pcodes sind länger als acht Stellen. Manche pcodes enthalten Kleinbuchstaben. Es gibt Kombinationen aus den obigen vier Fällen. Zudem sollte auf doppelt vorkommende Personencodes geprüft werden, die ggf. ausgeschlossen werden müssen, da es sich um doppelte Zeilen handeln könnte. Wir können für den Aufbereitungsplan also schreiben: # Aufbereitungsplan # Personencode (1. Spalte, pcode) # Suche nach pcodes kleiner/größer 8 Stellen und ersetze durch NA # Ersetze Kleinbuchstaben durch Großbuchstaben # Suche nach pcodes mit Datum (z. B. Punkt &quot;.&quot; suchen) und ersetze durch NA # Suche nach doppelten pcodes und lösche Zeile, falls Duplikat einer anderen Zeile # Prüfe mit View(), ob danach noch Aufbereitung nötig Zu beachten ist hier, dass mit denjenigen Bereinigungen gestartet wird, die möglichst viele Fälle erfassen. So ist etwa das Datum “01.01.2001” auch länger als 8 Zeichen, wird also durch den ersten Punkt in unserer Liste gleich miterfasst. Weiter kann Ausschau gehalten werden nach Bereinigungen, die global für alle Personencodes ausgeführt werden können, wie hier das Ersetzen von Klein- durch Großbuchstaben. So wird in weiten Teilen vermieden, jeden unregelmäßigen pcode einzeln verändern zu müssen. Entscheidend ist am Ende das Prüfen, ob weitere Aufbereitungen nötig sind, hier verbleiben dann manchmal Fälle, die tatsächlich einzeln zu ändern sind. Anmerkung: Zum Erhalt von Daten kann statt unregelmäßige pcodes NA zu setzen auch milder verfahren werden. Denkbar wäre z. B. das speichern der Roh-Personencodes in einer neuen Variablen pcode_roh, um bei späterem matching von Personencodes (z. B. Längsschnitt) bei hinreichend ähnlichen, unkonformen pcodes doch noch ein matching zu erhalten. Die Variablen in den Spalten 2 bis 6 stellen Likert-skalierte Items zur Abfrage von Vorwissen von 1 (niedrig) bis 6 (hoch) dar. Hier fällt auf, dass sehr viele Personen fehlende Werte (NA) aufweisen. Während der Umgang mit fehlenden Werten nicht Teil dieses Beitrags ist, kann zumindest für unseren Aufbereitungsplan festgehalten werden, dass Personen mit sehr hohen missing-Anteilen (z. B. 60%) als nicht valide Datenquellen gelten und aus der Analyse ausgeschlossen werden können. Zusätzlich kann gerade bei Likert-skalierten, also rein numerischen Variablen noch einmal geschaut werden, ob alle Ausprägungen auch im intendierten Bereich (hier 1 bis 6) liegen. Dies können wir schnell mit der summary()-Funktion prüfen: summary(datensatz_renamed[,2:6]) ## VW.theo VW.prax VW.plan VW.durch ## Min. :1.000 Min. : 1.000 Min. :1.000 Min. :1.00 ## 1st Qu.:2.000 1st Qu.: 2.000 1st Qu.:1.000 1st Qu.:2.00 ## Median :3.000 Median : 3.000 Median :2.000 Median :3.00 ## Mean :2.705 Mean : 3.061 Mean :2.566 Mean :2.62 ## 3rd Qu.:4.000 3rd Qu.: 4.000 3rd Qu.:4.000 3rd Qu.:3.00 ## Max. :5.000 Max. :99.000 Max. :6.000 Max. :6.00 ## NA&#39;s :221 NA&#39;s :220 NA&#39;s :221 NA&#39;s :220 ## VW.reflex ## Min. :1.000 ## 1st Qu.:1.000 ## Median :2.000 ## Mean :2.495 ## 3rd Qu.:3.000 ## Max. :6.000 ## NA&#39;s :221 Die Variable VW.prax weist einen maximalen Wert 99 auf, der wahrscheinlich einem fehlenden Wert im importierten Datenformat entsprach. Eine Nachprüfung ergab, dass es sich hierbei um einen regulären fehlenden Wert handelte. Zudem ist, das wissen wir aus der Erhebung, das Skalen-Item VW.reflex invertiert gepolt (Ich hatte kein besonderes Vorwissen zum Thema Reflexion.), so dass eine Umpolung erfolgen muss, wenn etwa später Skalenscores berechnet werden sollen. Wir fügen also zum Aufbereitungsplan hinzu: #Likert-skalierte Variablen allgemein #Ersetze in allen Likert-skalierten Variablen &quot;99&quot; durch NA #Variablen Vorwissen (2 bis 6) #Schließe Personen aus, die in ALLEN Variablen missing-Quoten von über 60% aufweisen. # Variable VW.reflex: Umpolen Hierbei ist die Reihenfolge der beiden Operationen wichtig, da durch die erste Bereinigung neue NAs entstehen könnten. Weiter geht es mit den Variablen zur Wahl der Abiturfächer in den Spalten 7 bis 13. Hier ist ersichtlich, dass es anscheinend nur die Ausprägungen NA und 1 gibt, wir prüfen kurz mit der summary()-Funktion und es bestätigt sich. Die Items waren so gestellt, dass die Personen je Abiturfach entweder ein Häkchen setzen konnten für im Abitur belegt oder das Ankreuzfeld leer ließen für nicht im Abitur belegt. Problematisch ist nun, dass kein Wert in den Variablen existiert, der auf ein klares Nein hindeutet. Es finden sich nur NAs, die entweder bedeuten, dass ein Fach nicht im Abitur gewählt wurde oder dass es sich um einen fehlenden Wert handelt, die Person etwa die Frage übersprungen hat. Dieser Fall stellt also eine Herausforderung für die Datenbereinigung dar. Nach einiger Überlegung kann folgendes festgehalten werden: Personen, die mindestens bei einem Fach eine 1 für Ja aufweisen, können wohl guten Gewissens als Personen angesehen werden, die alle Fragen zum Abitur beantwortet haben. Die NAs in den je anderen Abitur-Variablen dieser Personen können also 0 = Nein gesetzt werden. Schwieriger ist es bei Personen, die überall in den Abitur-Variablen NA aufweisen. Aufgrund der Fülle an ankreuzbaren Abiturfächern kann allerdings mit recht gutem Gewissen darauf geschlussfolgert werden, dass die Personen tatsächlich diese Items nicht bearbeitet haben, die NAs bleiben hier also bestehen. Wenn diese Änderungen vorgenommen sind, könnten diese Variablen in Factor-Variablen mit 0 = nein und 1 = ja umgewandelt werden. Wir ergänzen für den Aufbereitungsplan: # Abitur-Fachwahl-Variablen (7 bis 13) # Ersetze nur für Personen mit mind. einer 1 = &quot;ja&quot; die NAs in den Abitur-Fachwahl- # Variablen durch 0 = Nein # Wandle die Variablen in Factor-Variablen (0=nein, 1=ja) um Die nächste Variable ist geschlecht mit den Ausprägungen 1, 2, 3 und 4. Zur Identifikation, welche der Zahlen zu welchem der Geschlechter (weiblich, männlich, divers, keine Angabe) gehören, kann wieder ein Blick in das Befragungstool oder das Forschungstagebuch helfen. In diesem Fall stellte sich heraus: 1=weiblich, 2=männlich, 3=divers, 4=keine Angabe. Das Geschlecht ist eine prädestinierte Factor-Variable, da sie oft zur Gruppenbildung herangezogen wird. Wir halten also im Aufbereitungsplan fest: # Geschlecht (14) # Wandle in Factor-Variable um, 1=weiblich, 2=männlich, 3=divers, 4=keine Angabe Bei der folgenden Variablen alter wird es schwieriger. Ein Blick über die table()-Funktion und die Abbildung 5.3 zeigt, dass die Ausprägungen zwischen 1 und 33 liegen, mit einer Häufung um die Ausprägung 4. Auch hier kann ein Blick in das Befragungsinstrument helfen. Es wurde hier mit Ankreuzoptionen gearbeitet, die erste war das Alter 17, die zweite 18 usw. Die Forscher:innen wollten damit vermeiden, dass bei einem z. B. offenen Angabefeld Angaben wie “dreiunddreißig” o. Ä. gemacht werden. Da 17 nun dem ersten Ankreuzfeld entsprach, scheint die Befragungssoftware dafür eine 1 gesetzt zu haben. Es gilt also 1 = 17, 2 = 18 usw. Die Daten sind also systematisch zu verändern mit der Operation +16. Wir ergänzen zum Aufbereitungsplan: # Alter (15) # Addiere 16 auf alle Altersangaben Die drittletzte Variable semester.pre stellt das Hochschulsemester zum Befragungszeitpunkt (hier PRE) dar. Über table() wird ersichtlich, dass trotz Befragung im 1. Semester nicht alle Ausprägungen 1 sind, es sind drei Personen im 3. und eine Person im 5. Semester vorhanden. Die Variable muss augenscheinlich nicht bereinigt werden. Die vorletzte Variable abi enthält die numerische Hochschulzugangsberechtigungsnote. Ein Blick in die summary()-Funktion zeigt ein Minimum von 1.1 und ein Maximum von 3.9. Die Variable scheint ebenso keine Datenbereinigung zu benötigen. Die letzte Variable dozent enthält buchstäblich immer denselben Wert, nämlich den Text (String) Brämer. Ist geplant, weitere Erhebungen bei anderen Dozierenden zu unternehmen, kann diese Variable nützlich werden, um etwa Dozierendeneffekte einer Intervention zu untersuchen. In unserem Fall ist allerdings lediglich eine PRE-POST-Untersuchung mit stets demselben Dozenten Brämer geplant gewesen, daher kann diese – vom Befragungstool automatisch kreierte – Variable einfach gelöscht werden: # Dozent (18) #lösche die Variable Zu guter Letzt sollte noch ein Blick auf die Variablentypen geworfen werden, vor allem factor-Variablen verhalten sich teils wenig intuitiv und können zunächst einmal in Character (Text) oder Numeric (Zahl) umgewandelt werden. str(datensatz_renamed) #zeigt detaillierte Informationen zu u.a. den Variablentypen im Datensatz ## &#39;data.frame&#39;: 546 obs. of 18 variables: ## $ pcode : Factor w/ 545 levels &quot; Anan1913 &quot;,..: 2 3 4 5 6 7 8 9 10 11 ... ## $ VW.theo : num 4 3 3 2 5 3 2 1 3 4 ... ## $ VW.prax : num 4 4 3 1 5 4 2 1 3 3 ... ## $ VW.plan : num 2 5 4 1 4 4 5 1 3 2 ... ## $ VW.durch : num 2 4 4 1 4 4 5 1 3 2 ... ## $ VW.reflex : num 5 4 4 2 5 4 2 1 4 3 ... ## $ Abitur_Fach_Biologie : num NA NA NA NA NA NA NA NA NA NA ... ## $ Abitur_Fach_Chemie : num NA NA NA NA NA NA NA NA NA NA ... ## $ Abitur_Fach_Geographie: num NA NA NA NA NA NA NA NA NA NA ... ## $ Abitur_Fach_Geschichte: num NA NA NA NA NA NA NA NA NA NA ... ## $ Abitur_Fach_Informatik: num NA NA NA NA NA NA NA NA NA NA ... ## $ Abitur_Fach_Physik : num NA NA NA NA NA NA NA NA NA NA ... ## $ Abitur_Fach_PW : num NA NA NA NA NA NA NA NA NA NA ... ## $ geschlecht : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 4 4 4 4 4 4 4 4 4 4 ... ## $ alter : num NA NA NA NA NA NA NA NA NA NA ... ## $ semester.pre : Factor w/ 4 levels &quot;1&quot;,&quot;3&quot;,&quot;5&quot;,&quot;NA&quot;: 4 4 4 4 4 4 4 4 4 4 ... ## $ abi : num NA NA NA NA NA NA NA NA NA NA ... ## $ dozent : chr &quot;Brämer&quot; &quot;Brämer&quot; &quot;Brämer&quot; &quot;Brämer&quot; ... Es zeigt sich, dass folgende Variablen als Factor vorliegen: pcode \\(\\Rightarrow\\) ungünstig, sollte in Character umgewandelt werden, um leichter Umwandlungen vornehmen zu können geschlecht \\(\\Rightarrow\\) ok, sofern die Ziffern ein label erhalten (weiblich, männlich usw.) semester.pre \\(\\Rightarrow\\) ok Den ersten Punkt fügen wir also noch zu unserem Aufbereitungsplan hinzu, und zwar bevor andere Operationen am pcode vorgenommen werden. 5.7.3 Aufbereitungsplan Es ist geschafft, der finale Aufbereitungsplan ist erstellt: # Aufbereitungsplan # Personencode (1. Spalte, &quot;pcode&quot;) #Wandle pcode in Character-Variable um # Suche nach pcodes kleiner/größer 8 Stellen und ersetze durch NA # Ersetze Kleinbuchstaben durch Großbuchstaben # Suche nach pcodes mit Datum (Punkt &quot;.&quot; suchen) und ersetze durch NA # Suche nach doppelten pcodes # Prüfe mit View(), ob danach noch Aufbereitung nötig #Likert-skalierte Variablen allgemein #Ersetze in allen Likert-Skalierten Variablen &quot;99&quot; durch NA #Variablen Vorwissen (2 bis 6) #Schließe Personen aus, die in ALLEN Variablen missing-Quoten von über 60% aufweisen. # Variable VW.reflex: Umpolen # Abitur-Fachwahl-Variablen (7 bis 13, &quot;Abitur_Fach_...) # Ersetze nur für Personen mit mind. einer 1 = &quot;ja&quot; die NAs in den Abitur- # Fachwahl-Variablen durch 0 = Nein # Wandle die Variablen in Factor-Variablen (0=nein, 1=ja) um # Geschlecht (14, &quot;geschlecht&quot;) # Wandle in Factor-Variable um, 1=weiblich, 2=männlich, 3=divers, 4=keine Angabe # Alter (15, &quot;alter&quot;) # Addiere 16 auf alle Altersangaben # Dozent (18, &quot;dozent&quot;) #lösche die Variable 5.8 Datenaufbereitung durchführen 5.8.1 Aufbereitung eines Personencodes Gemäß Aufbereitungsplan sind folgende Operationen zu realisieren: # Personencode (1. Spalte, &quot;pcode&quot;) #Wandle pcode in Character-Variable um # Suche nach pcodes kleiner/größer 8 Stellen und ersetze durch NA # Ersetze Kleinbuchstaben durch Großbuchstaben # Suche nach pcodes mit Datum (Punkt &quot;.&quot; suchen) und ersetze durch NA # Suche nach doppelten pcodes # Prüfe mit View(), ob danach noch Aufbereitung nötig Wir verfahren Schritt für Schritt: #Wandle pcode in Character-Variable um Der Personencode hat sich in der Aufbereitungsplanung als Factor-Variable mit über 400 Levels gezeigt. Da dies gerade für das Recodieren einiger pcodes ungünstige Hürden erzeugt, wandeln wir nun in das Format Character (Text) um: datensatz_renamed$pcode &lt;- as.character(datensatz_renamed$pcode) Der Befehl as.character() wandelt hierbei jeden anzeigbaren Text in tatsächlichen Text um. Von nun an können wir einfache Veränderungen an einzelnen Personencodes vornehmen. Wir prüfen den Erfolg der Operation: str(datensatz_renamed$pcode) ## chr [1:546] &quot;01.01.2000&quot; &quot;01.04.1987&quot; &quot;06&quot; &quot;1.NA2.NA3.284.10&quot; &quot;141651&quot; ... Im nächsten Bereinigungsschritt wird geprüft, ob Personencodes zu wenige oder zu viele Zeichen aufweisen. Im strengsten Fall können diese direkt NA gesetzt werden: # Suche nach pcodes kleiner/größer 8 Stellen und ersetze durch NA datensatz_renamed$pcode[nchar(datensatz_renamed$pcode) != 8] &lt;- NA #man beachte das UNGLEICH != Zudem sollen noch Klein- in Großbuchstaben umgewandelt werden: # Ersetze Kleinbuchstaben durch Großbuchstaben---- datensatz_renamed$pcode &lt;- toupper(datensatz_renamed$pcode) Bei der ersten Durchsicht der Daten zeigten sich auch Personencodes mit einem “.” (Punkt) darin: Um Text bzw. Textteile (wie den Punkt) in R zu suchen, gibt es u. a. die Funktion grepl(). Der Aufbau dieser Funktion sieht im einfachsten Fall folgendermaßen aus: grepl(&quot;TEXT&quot;, Variable oder (Teil-)Datensatz) Der Output zeigt dann je Variable und Person mit einer 1 oder 0 an, ob der Text gefunden wurde oder nicht. So lassen sich zum Beispiel Personencodes finden und ggf. ändern, die einen Punkt (“.”) enthalten: datensatz_renamed[grepl(&quot;.&quot;, datensatz_renamed$pcode, fixed = T), &quot;pcode&quot;] ## character(0) # fixed = T sorgt dafür, dass Sonderzeichen wie ein Punkt oder ein &quot;$&quot; auch als solche # behandelt werden und nicht als Operatoren. # Output character(0) zeigt, dass bereits alle pcodes mit einem &quot;.&quot; entfernt sind. # Dies war bereits durch den vorherigen Schritt mit Zeichenzahl ungleich 8 geschehen. Auch mögliche doppelte Zeilen oder pcodes gilt es zu identifizieren und zu bereinigen: # Suche nach doppelten pcodes ---- head(duplicated(datensatz_renamed$pcode)) # TRUE zeigt an, dass ein pcode mind. 2mal vorkommt ## [1] FALSE TRUE TRUE TRUE TRUE TRUE # Einen Gesamtüberblick liefert table(): table(duplicated(datensatz_renamed$pcode)) ## ## FALSE TRUE ## 515 31 # dies ist 31-mal der Fall datensatz_renamed[duplicated(datensatz_renamed$pcode) == T, &quot;pcode&quot;] ## [1] NA NA NA NA NA NA ## [7] NA NA NA NA NA NA ## [13] NA NA NA NA NA NA ## [19] NA NA NA NA NA NA ## [25] NA NA NA NA NA NA ## [31] &quot;ANAN0770&quot; # Die meisten Fälle sind einfach doppelte NA, aber # Person 31 scheint ein reales Duplikat zu sein # mit der Bedingung is.na(datensatz_renamed$pcode) == F werden hier alle pcode = NA ausgeschlossen: datensatz_renamed[duplicated(datensatz_renamed$pcode) == T &amp; is.na(datensatz_renamed$pcode) == F, &quot;pcode&quot;] ## [1] &quot;ANAN0770&quot; # speichere extrahierten pcode in neuem (Character-)Objekt &quot;duplicated_pcodes&quot;: duplicated_pcodes &lt;- datensatz_renamed[duplicated(datensatz_renamed$pcode) == T &amp; is.na(datensatz_renamed$pcode) == F, &quot;pcode&quot;] # Einfacher: Bei nur wenigen Fällen kann auch der betreffende pcode einfach kurz kopiert oder notiert werden # Zur Veranschaulichung: So sieht das erstellte Objekt aus: duplicated_pcodes ## [1] &quot;ANAN0770&quot; # prüfe, ob Zeilen identisch sind: datensatz_renamed[datensatz_renamed$pcode %in% duplicated_pcodes, ] ## pcode VW.theo VW.prax VW.plan VW.durch VW.reflex Abitur_Fach_Biologie ## 20 ANAN0770 4 5 6 6 4 1 ## 546 ANAN0770 4 5 6 6 4 1 ## Abitur_Fach_Chemie Abitur_Fach_Geographie Abitur_Fach_Geschichte ## 20 NA 1 NA ## 546 NA 1 NA ## Abitur_Fach_Informatik Abitur_Fach_Physik Abitur_Fach_PW geschlecht alter ## 20 1 NA NA 2 13 ## 546 1 NA NA 2 13 ## semester.pre abi dozent ## 20 1 3.5 Brämer ## 546 1 3.5 Brämer # Die beiden Fälle sind identisch (für das erste Auftreten # des pcodes gilt duplicated=F, für das zweite duplicated = T), # das zweite Auftreten des Falls wird also entfernt: nrow(datensatz_renamed) # Fälle vorher ## [1] 546 datensatz_renamed[duplicated(datensatz_renamed$pcode) == T &amp; is.na(datensatz_renamed$pcode) == F, &quot;pcode&quot;] ## [1] &quot;ANAN0770&quot; datensatz_renamed &lt;- datensatz_renamed[duplicated(datensatz_renamed$pcode) == F &amp; is.na(datensatz_renamed$pcode) == F, ] # Achtung: Nur duplicated = FALSE werden # ausgewählt. Zugleich werden hier alle pcode = NA ausgeschlossen nrow(datensatz_renamed) ## [1] 514 # Fälle hinterher: Es sind 32 weniger: # 31 doppelte pcodes: # 1-mal ANAN0770 # 30mal NA # 1 erster NA-pcode, der bewirkte, dass alle # nachfolgenden duplicated = TRUE gesetzt wurden datensatz_renamed[duplicated(datensatz_renamed$pcode) == T &amp; is.na(datensatz_renamed$pcode) == F, &quot;pcode&quot;] ## character(0) # keine doppelted pcodes mehr Nun führen wir noch eine abschließende Prüfung durch: # Prüfe &quot;per Auge&quot;, ob nun noch eine Aufbereitung nötig ist View(datensatz_renamed) # Prüfe die Anzahlen gültiger Personencodes: #zeigt Anzahl fehlender Werte (NA): sum(is.na(datensatz_renamed$pcode)) ## [1] 0 #zeigt Anzahl gültiger pcodes: sum(!is.na(datensatz_renamed$pcode)) ## [1] 514 Die Bereinigung des Personencodes ist abgeschlossen. Wir indizieren dies über einen neuen Namen für den Datensatz: datensatz_cleaned_pcode &lt;- datensatz_renamed 5.8.2 Aufbereitung Likert-skalierter Variablen Für die Likert-skalierten Variablen soll gemäß Aufbereitungsplan folgendes erfolgen: #Likert-skalierte Variablen allgemein #Ersetze in allen Likert-Skalierten Variablen &quot;99&quot; durch NA #Variablen Vorwissen (2 bis 6, &quot;VW...&quot;) #Schließe Personen aus, die in ALLEN Variablen missing-Quoten von über 60% aufweisen. # Variable VW.reflex: Umpolen Ein Problem betraf das Auftreten des Wertes “99” in den Likert-skalierten Vorwissens-Variablen (2 bis 6). Es gilt zunächst, diese Anomalie zu detektieren: summary(datensatz_cleaned_pcode[,c(2:6)]) ## VW.theo VW.prax VW.plan VW.durch ## Min. :1.00 Min. : 1.000 Min. :1.000 Min. :1.000 ## 1st Qu.:2.00 1st Qu.: 2.000 1st Qu.:1.000 1st Qu.:2.000 ## Median :3.00 Median : 3.000 Median :2.000 Median :3.000 ## Mean :2.71 Mean : 3.109 Mean :2.546 Mean :2.616 ## 3rd Qu.:4.00 3rd Qu.: 4.000 3rd Qu.:4.000 3rd Qu.:3.000 ## Max. :5.00 Max. :99.000 Max. :6.000 Max. :6.000 ## NA&#39;s :221 NA&#39;s :220 NA&#39;s :221 NA&#39;s :220 ## VW.reflex ## Min. :1.000 ## 1st Qu.:1.000 ## Median :2.000 ## Mean :2.474 ## 3rd Qu.:3.000 ## Max. :6.000 ## NA&#39;s :221 # bei VW.prax existiert also eine einzige 99 # Prüfe, welches der zugehörige Personencode ist: head(datensatz_cleaned_pcode[datensatz_cleaned_pcode$VW.prax == 99, &quot;pcode&quot;]) ## [1] NA NA &quot;ALLY2570&quot; NA NA NA #ALLY2570 das ist der zugehörige pcode (neben einigen NA-Artefakten) Die Auffälligkeit wurde nur in der Variablen VW.prax gefunden und auch nur einmalig. Die 99 tritt bei der Proband:in mit dem Personencode ALLY2570 auf. Ein Blick in die Ursprungsdaten mit Excel oder ein Betrachten des Fragenbogenentwurfs in der zugehörigen Software offenbart hier, dass es sich bei der 99 um ein reguläres NA handelt: datensatz_cleaned_pcode[datensatz_cleaned_pcode$pcode == &quot;ALLY2570&quot; &amp; is.na(datensatz_cleaned_pcode$pcode) == F, &quot;VW.prax&quot;] &lt;- NA # setze den Wert auf NA 5.8.2.1 Basale Bereinigung fehlender Werte über Fallausschlüsse Zunächst sei vorangestellt, dass das Verwerfen von Daten mit einigen Verzerrungen der Ergebnisse einhergehen kann und daher oftmals Imputationen (Rekonstruktionen) fehlender Daten zu bevorzugen sind (Lüdtke, Robitzsch, Trautwein &amp; Köller, 2007). Für dieses einfache Beispiel möchten wir uns allerdings mit listenweisen Fallausschlüssen gemäß missing-Quote begnügen (Field, Miles &amp; Field, 2012, S. 216). Wir benötigen nun zunächst eine Information darüber, welche Personen hohe missing-Quoten von z. B. über 60% aufweisen, also 60% des Fragebogens nicht ausgefüllt haben. Diese Personen sehen wir hier als nicht-zuverlässige Datenquellen an und schließen sie aus der Analyse aus. Da das Feststellen von missing-Quoten eine sehr häufig wiederkehrende Notwendigkeit darstellt, schreiben wir eine Funktion (vgl. Kapitel Basiswissen für R), die wir dann stets für diesen Fall nutzen können: Funktion 1: help_percentmissing(): Berechnet je Proband:in (entspricht einer Zeile) den prozentualen Anteil fehlender Werte3: help_percentmissing = function(x) { sum(is.na(x)) / length(x) * 100 # Hier wird die Summe aller Missings durch die Anzahl der # Antworten der Testteilnehmenden geteilt } Nun wollen wir diese Funktion nicht nur auf eine:n Proband:in, sondern auf den ganzen Datensatz anwenden. Wir schreiben daher eine zweite Funktion, die die erste Funktion direkt nutzt: percentmissing = function(data) { (apply(data, 1, help_percentmissing)) # apply() wendet eine Funktion auf multiple Instanzen an, hier, # symbolisiert durch die &quot;1&quot;, auf alle Zeilen des Datensatzes # (&quot;2&quot; wäre für alle Spalten) } Nun wenden wir percentmissing() an: nrow(datensatz_cleaned_pcode) #Proband:innen vorher ## [1] 514 head(percentmissing(datensatz_cleaned_pcode)) ## 11 12 13 14 15 16 ## 77.77778 77.77778 16.66667 27.77778 22.22222 77.77778 Da der Output bei diesem großen Datensatz recht unübersichtlich gerät, bilden wir eine Tabelle. Diese fasst uns Fälle mit gleicher Anzahl prozentualer missings zusammen: # Die round()-Funktion bewirkt hier zusätzlich eine Darstellung mit zwei Nachkommastellen: table(round(percentmissing(datensatz_cleaned_pcode), digits = 2)) ## ## 5.56 11.11 16.67 22.22 27.78 33.33 38.89 50 77.78 ## 7 26 61 57 18 10 2 113 220 Der Output zeigt z. B., dass sieben Proband:innen eine missing-Quote von 5.56% aufweisen. Über den Befehl summary(datensatz) kann gesehen werden, dass vor allem die Abitur-Variablen (7 bis 13) hohe missing-Quoten aufweisen. Da diese (wie bereits in Abschnitt 5.7.2 dargelegt) teils keine echten missings darstellen, sollten sie (temporär) für die Bestimmung der missing-Quoten ausgeschlossen werden: table(percentmissing(datensatz_cleaned_pcode[, -c(7:13)])) ## ## 0 9.09090909090909 18.1818181818182 27.2727272727273 ## 177 3 113 1 ## 63.6363636363636 ## 220 Das Ergebnis zeigt nun 177 Proband:innen mit missing-Quote 0% und 220 Personen mit über 60% missings.4 Schließe also Proband:innen mit missng-Quote größer 60% aus: datensatz_cleaned_pcode &lt;- datensatz_cleaned_pcode[percentmissing(datensatz_cleaned_pcode[, -c(7:13)]) &lt; 60, ] nrow(datensatz_cleaned_pcode) #294 ## [1] 294 Es verbleiben danach 294 Proband:innen. Eine weitere zu bereinigende Likert-skalierte Variable war VW.reflex, hier sollte eine Umpolung erfolgen: # Variable VW.reflex: Umpolen table(datensatz_cleaned_pcode$VW.reflex) ## ## 1 2 3 4 5 6 ## 79 87 59 47 19 2 datensatz_cleaned_pcode$VW.reflex &lt;- 7 - datensatz_cleaned_pcode$VW.reflex # Hier wird die Formel &quot;7 – Wert&quot; verwendet, um 1 in 6, 2 in 5 usw. zu recodieren. table(datensatz_cleaned_pcode$VW.reflex) ## ## 1 2 3 4 5 6 ## 2 19 47 59 87 79 So wird die 1 in die 6 überführt usw. Die erfolgreiche Datenbereinigung wird erneut durch einen neuen Namen des Datensatzes gekennzeichnet: datensatz_cleaned_pcode_likert &lt;- datensatz_cleaned_pcode 5.9 Aufbereitung nominaler Variablen (z. B. Ja/Nein) Im Aufbereitungsplan zeigten sich die Abitur-Variablen auffällig: # Abitur-Fachwahl-Variablen (7 bis 13, &quot;Abitur_Fach_...) # Ersetze nur für Personen mit mind. einer 1 = &quot;ja&quot; die NAs in den Abitur-Fachwahl- # Variablen durch 0 = Nein # Wandle die Variablen in Factor-Variablen (0=nein, 1=ja) um Bei diesen nominalen Variablen (z.B. Abitur_Fach_Physik) steht eine 1 für Ja, im Abitur belegt und eine 0 für Nein, im Abitur nicht belegt. Der Datensatz enthält aber gar keine 0, sondern stets 1 oder NA. Personen mit mind. einer 1 sollen daher in den restlichen NA-codierten Abitur-Variablen ein 0 = nein erhalten. Bei Personen mit durchgängig NAs sollen diese erhalten bleiben, da die Person offensichtlich keines der Fächer angekreuzt und damit die Fragen mutmaßlich übersprungen hat. Nun kann das Recodieren beginnen: # Schritt 1: Setze alle NAs zunächst 0 # Bem.: is.na()-Funktion lässt sich auch auf ganze Dataframes anwenden. # Diejenigen Personen, die in Variable 7 (erste Abitur-Variable) ein NA aufweisen, # erhalten als Wert in ihrer Variablen 7 eine 0 datensatz_cleaned_pcode_likert[is.na(datensatz_cleaned_pcode_likert[, 7]) == T, 7] &lt;- 0 # Für die zweite Abitur-Variable entsprechend: datensatz_cleaned_pcode_likert[is.na(datensatz_cleaned_pcode_likert[, 8]) == T, 8] &lt;- 0 # usw. bis 13 # oder als Schleife: for (i in 7:13) { datensatz_cleaned_pcode_likert[is.na(datensatz_cleaned_pcode_likert[,i])==T,i] &lt;- 0 } # Überprüfe den Erfolg obiger Operationen: # 1) Zeile für Zeile: table(datensatz_cleaned_pcode_likert[,7]) ## ## 0 1 ## 141 153 # usw. bis 13 # Oder 2) per Schleife und table(): for (i in 7:13) { print(colnames(datensatz_cleaned_pcode_likert)[i]) print(table(datensatz_cleaned_pcode_likert[, i])) } ## [1] &quot;Abitur_Fach_Biologie&quot; ## ## 0 1 ## 141 153 ## [1] &quot;Abitur_Fach_Chemie&quot; ## ## 0 1 ## 215 79 ## [1] &quot;Abitur_Fach_Geographie&quot; ## ## 0 1 ## 209 85 ## [1] &quot;Abitur_Fach_Geschichte&quot; ## ## 0 1 ## 140 154 ## [1] &quot;Abitur_Fach_Informatik&quot; ## ## 0 1 ## 280 14 ## [1] &quot;Abitur_Fach_Physik&quot; ## ## 0 1 ## 224 70 ## [1] &quot;Abitur_Fach_PW&quot; ## ## 0 1 ## 210 84 # Oder 3) per describe(): describe(datensatz_cleaned_pcode_likert[,c(7:13)]) ## vars n mean sd median trimmed mad min max range ## Abitur_Fach_Biologie 1 294 0.52 0.50 1 0.53 0 0 1 1 ## Abitur_Fach_Chemie 2 294 0.27 0.44 0 0.21 0 0 1 1 ## Abitur_Fach_Geographie 3 294 0.29 0.45 0 0.24 0 0 1 1 ## Abitur_Fach_Geschichte 4 294 0.52 0.50 1 0.53 0 0 1 1 ## Abitur_Fach_Informatik 5 294 0.05 0.21 0 0.00 0 0 1 1 ## Abitur_Fach_Physik 6 294 0.24 0.43 0 0.17 0 0 1 1 ## Abitur_Fach_PW 7 294 0.29 0.45 0 0.23 0 0 1 1 ## skew kurtosis se ## Abitur_Fach_Biologie -0.08 -2.00 0.03 ## Abitur_Fach_Chemie 1.04 -0.93 0.03 ## Abitur_Fach_Geographie 0.93 -1.15 0.03 ## Abitur_Fach_Geschichte -0.09 -2.00 0.03 ## Abitur_Fach_Informatik 4.23 15.92 0.01 ## Abitur_Fach_Physik 1.22 -0.50 0.02 ## Abitur_Fach_PW 0.94 -1.11 0.03 # Schritt 2: Setze die Personen mit überall 0 gleich überall NA: # Gewinne einen Eindruck über die Summe der Nullen und Einsen: head(rowSums(datensatz_cleaned_pcode_likert[, c(7:13)])) ## 13 14 15 20 23 26 ## 5 2 3 3 5 0 # fragt, ob dies = 0 ist, also keine einzige 1=&quot;Ja&quot; vorhanden ist: head(rowSums(datensatz_cleaned_pcode_likert[, c(7:13)]) == 0) ## 13 14 15 20 23 26 ## FALSE FALSE FALSE FALSE FALSE TRUE # Stellt das Ganze als Tabelle übersichtlich dar: table(rowSums(datensatz_cleaned_pcode_likert[, c(7:13)]) == 0) ## ## FALSE TRUE ## 179 115 # 115 Personen betrifft dies -&gt; Setze für Personen, die obige Bedingung erfüllen, die # Variablenwerte der Variablen 7 bis 13 gleich NA: datensatz_cleaned_pcode_likert[rowSums(datensatz_cleaned_pcode_likert[, c(7:13)]) == 0, c(7:13)] &lt;- NA # prüfe Erfolg: table(rowSums(datensatz_cleaned_pcode_likert[, c(7:13)]) == 0) ## ## FALSE ## 179 Nun enthalten die Abitur-Variablen die Werte 0, 1 und NA. Wir wandeln sie hier noch in Factor-Variablen um (vgl. Kapitel Basiswissen für R): # Wandle die Variablen in Factor-Variablen (0=nein, 1=ja) um unter # Nutzung der Variablen-Namen # (Auch die Spaltennummern wären hier möglich.) datensatz_cleaned_pcode_likert$Abitur_Fach_Biologie &lt;- factor(datensatz_cleaned_pcode_likert$Abitur_Fach_Biologie, levels = c(0, 1), labels = c(&quot;nein&quot;, &quot;ja&quot;)) # Levels sind die &quot;Zahlenwerte&quot;, also 0 und 1. Labels sind die schriftlichen Entsprechungen, also &quot;nein&quot; und &quot;ja&quot;, die Reihenfolge ist relevant. # Oder als Schleife unter Nutzung der Spaltennummern der Variablen: for (i in 7:13) { datensatz_cleaned_pcode_likert[, i] &lt;- factor(datensatz_cleaned_pcode_likert[, i], levels = c(0, 1), labels = c(&quot;nein&quot;, &quot;ja&quot;)) } Im Aufbereitungsplan gab es zudem auch die nominale Variable geschlecht: # Geschlecht (14, &quot;geschlecht&quot;) # Wandle in Factor-Variable um, 1=weiblich, 2=männlich, 3=divers, 4=keine Angabe Prüfe zunächst die Variable: head(datensatz_cleaned_pcode_likert$geschlecht) #ist schon factor mit 1, 2, 3 und 4, muss nur noch umbenennen ## [1] 1 1 3 2 1 4 ## Levels: 1 2 3 4 Es zeigt sich, dass die Variable bereits eine factor-Variable ist, allerdings nur mit Ziffern, ohne Labels: # Auch hier ist die Reihenfolge bei &quot;level&quot; und &quot;label&quot; wieder entscheidend: datensatz_cleaned_pcode_likert$geschlecht &lt;- factor( datensatz_cleaned_pcode_likert$geschlecht, levels = c(1, 2, 3, 4), labels = c(&quot;weiblich&quot;, &quot;männlich&quot;, &quot;divers&quot;, &quot;keine Angabe&quot;) ) head(datensatz_cleaned_pcode_likert$geschlecht) #prüfe ## [1] weiblich weiblich divers männlich weiblich ## [6] keine Angabe ## Levels: weiblich männlich divers keine Angabe Die Zuordnung der numerischen levels zu den textbasierten labels ist also wie folgt: 1 = weiblich, 2 = männlich, 3 = divers, 4 = keine Angabe. Der Abschluss der Aufbereitung nominaler Daten wird im Datensatznamen vermerkt: datensatz_cleaned_pcode_likert_nominal &lt;- datensatz_cleaned_pcode_likert 5.9.1 Recodieren einer numerischen Variablen Im Abschnitt 5.8.2 haben wir bereits eine Likert-skalierte Variable umgepolt. Eine ähnliche Recodierung wenden wir nun auf die Variable alter an: Die Variable alter muss gemäß Aufbereitungsplan neu codiert werden: # Alter (15, &quot;alter&quot;) # Addiere 16 auf alle Altersangaben Dies lässt sich einfach durch Addition realisieren: datensatz_cleaned_pcode_likert_nominal$alter &lt;- 16 + datensatz_cleaned_pcode_likert_nominal$alter 5.9.2 Löschen von Variablen Zu guter Letzt wird noch die Variable dozent gemäß Aufbereitungsplan entfernt. # Dozent (18, &quot;dozent&quot;) #lösche die Variable Das Löschen geschieht über den Wert NULL datensatz_cleaned_pcode_likert_nominal$dozent &lt;- NULL An dieser Stelle sollte davor gewarnt werden, Variablen mit tatsächlichen Daten vorschnell zu löschen. Meist ist es besser, diese zu behalten – beispielsweise umbenannt (z. B. Vorsilbe trash_) oder in einem Papierkorb-Dataframe. Der Datensatz ist nun bereinigt: datensatz_clean &lt;- datensatz_cleaned_pcode_likert_nominal 5.10 Fortgeschrittene Datenaufbereitung 5.10.1 Zwei Datensätze verschmelzen Eine große Stärke von R ist der Umgang mit diversen Datensätzen gleichzeitig, da diese einfach als Objekte datensatz, datensatz_2 usw. zwischengespeichert werden können. Gerade stärkere Forschungsdesigns wie Längsschnitte (multiple Messzeitpunkte) oder Querschnitte mit multiplen Erhebungen (z. B. mehrere Parallelseminare oder Schulklassen, multiple Fächergruppen, multiple Standorte usw.) haben oftmals multiple Datensätze zur Folge. Eine übergreifende oder vergleichende Auswertung dieser erfordert allerdings in der Regel einen einzigen Datensatz, weshalb das data-merging, also das aggregieren verschiedener Datensätze in einen einzigen eine relevante Fähigkeit darstellt. Ziel dieses Abschnitts ist es daher, für zwei häufige Fälle, Querschnitt multipler Gruppen und Längsschnitt einer Gruppe, zu zeigen, wie die Datensätze aggregiert werden können. 5.10.1.1 Querschnittsdaten verschmelzen Um den Verschmelzungsprozess nachvollziehbar zu gestalten, betrachten wir ein minimalistisches Beispiel von zwei Gruppen, einer Physik-Gruppe und einer Chemie-Gruppe. In beiden Gruppen hat bei drei Personen eine Befragung stattgefunden, es wurden die Personencodes, das Vorwissen, das Fach und das Alter abgefragt. Wir konstruieren zunächst die minimalistischen Datensätze: df_physik_pre &lt;- data.frame( pcode = c(&quot;GK1203&quot;, &quot;ZB8900&quot;, &quot;WS4451&quot;), vorwissen_pre = c(2, 4, 5), alter = c(21, 21, 25), fach = c(&quot;Physik&quot;,&quot;Physik&quot;,&quot;Physik&quot;) ) df_chemie_pre &lt;- data.frame( pcode = c(&quot;NN8780&quot;, &quot;FD1212&quot;, &quot;KK0996&quot;), vorwissen_pre = c(2, 4, 5), alter = c(21, 21, 25), fach = c(&quot;Chemie&quot;,&quot;Chemie&quot;,&quot;Chemie&quot;) ) df_physik_pre ## pcode vorwissen_pre alter fach ## 1 GK1203 2 21 Physik ## 2 ZB8900 4 21 Physik ## 3 WS4451 5 25 Physik df_chemie_pre ## pcode vorwissen_pre alter fach ## 1 NN8780 2 21 Chemie ## 2 FD1212 4 21 Chemie ## 3 KK0996 5 25 Chemie An den Personencodes ist zunächst ersichtlich, dass weder innerhalb noch zwischen den Datensätzen doppelte Personen vorkommen (mehr dazu in Abschnitt 5.10.1.4). Zudem sind alle Variablennamen in beiden Datensätzen 100% identisch und auch die Anzahl der Variablen gleich. Solche Fälle treten beispielsweise dann auf, wenn ein und derselbe Fragebogen unterschiedlichen Gruppen in je getrennten Erhebungen ausgehändigt wird. Für diesen Fall genügt für ein Verschmelzen der Daten die Funktion rbind() (row bind), die namensgemäß die Zeilen aneinander bindet: df_pre &lt;- rbind(df_physik_pre,df_chemie_pre) df_pre ## pcode vorwissen_pre alter fach ## 1 GK1203 2 21 Physik ## 2 ZB8900 4 21 Physik ## 3 WS4451 5 25 Physik ## 4 NN8780 2 21 Chemie ## 5 FD1212 4 21 Chemie ## 6 KK0996 5 25 Chemie Nun haben wir einen einzigen Datensatz, mit dem wir weiterführende Analysen ausführen können. So könnte zum Beispiel (bei mehr Personen) ein t-Test für unverbundene Stichproben (Physik vs. Chemie) durchgeführt werden. Komplexer wird das Verschmelzen von Längsschnittdaten, die im kommenden Abschnitt erläutert werden. 5.10.1.2 Längsschnittdaten verschmelzen Bei unserem Längsschnitt-Beispiel gehen wir von einer klassischen PRE-POST-Messung aus, es hat also eine Befragung zu Beginn (z. B. eines Seminars oder einer Unterrichtsreihe) stattgefunden und eine zum Ende hin. Dabei entstehen zwei Datensätze PRE und POST, die idealerweise je dieselben Personen enthalten. Als Indikator, dass zwei Personen identisch sind, dient hier der Personencode: df_physik_pre &lt;- data.frame( pcode = c(&quot;GK1203&quot;, &quot;ZB8900&quot;, &quot;WS4451&quot;), vorwissen_pre = c(2, 4, 5), alter = c(21, 21, 25), fach = c(&quot;Physik&quot;,&quot;Physik&quot;,&quot;Physik&quot;) ) df_physik_post &lt;- data.frame( pcode = c(&quot;ZB8900&quot;, &quot;WS4451&quot;, &quot;GL8621&quot;), vorwissen_post = c(5, 5, 3), geschlecht = c(&quot;w&quot;, &quot;w&quot;, &quot;m&quot;) ) df_physik_pre ## pcode vorwissen_pre alter fach ## 1 GK1203 2 21 Physik ## 2 ZB8900 4 21 Physik ## 3 WS4451 5 25 Physik df_physik_post ## pcode vorwissen_post geschlecht ## 1 ZB8900 5 w ## 2 WS4451 5 w ## 3 GL8621 3 m In unserem Beispiel ist zu erkennen, dass nur zwei Personen in beiden Datensätzen vorkommen (ZB8900, WS4451). Dies kann passieren, wenn Personen zu einem Messzeitpunkt nicht anwesend waren oder ihren Personencode falsch gebildet haben. In jedem Fall wollen wir nun einen Datensatz bilden, der alle Daten dieser zwei Personen enthält und für die sonstigen Personen die vorhandenen Daten erhält. In R lässt sich dies über die merge()-Funktion realisieren: df_physik &lt;- merge(df_physik_pre, df_physik_post, by = &quot;pcode&quot;, all = T) Das Argument by = legt fest, durch welche Variable(n) die Gleichheit zweier Personen erkennbar ist, hier also lediglich der pcode. Das Argument all = T sorgt dafür, dass auch Personen ohne PRE-POST-matching erhalten bleiben (für die dadurch fehlenden Werte wird bei diesen Personen ein NA gesetzt). Da es sich bei einer Längsschnittstudie meist um die gleichen Variablen zu unterschiedlichen Zeitpunkten handelt, sollte man hier insbesondere darauf achten, dass die wiederholt erhobenen Variablen das Präfix/Suffix pre bzw. post bekommen. Die merge()-Funktion in R würde sonst aus den verschiedenen Messzeitpunkten einen einzigen machen, was eine Datenanalyse im Längsschnitt unmöglich macht. Betrachten wir den aggregierten Datensatz: df_physik ## pcode vorwissen_pre alter fach vorwissen_post geschlecht ## 1 GK1203 2 21 Physik NA &lt;NA&gt; ## 2 GL8621 NA NA &lt;NA&gt; 3 m ## 3 WS4451 5 25 Physik 5 w ## 4 ZB8900 4 21 Physik 5 w Der neue Datensatz zeigt für die identifizierten PRE-POST-aggregierbaren Personen (ZB8900, WS4451) vollständige Daten in allen Variablen an. Hier zeigt sich auch, warum die Umbenennung der (inhaltlich identischen) Variablen vorwissen in pre und post wichtig war. Die beiden sonstigen Fälle weisen wie erwartet fehlende Werte (NA) auf. Mit diesem Datensatz (mit mehr Personen) könnte dann etwa ein t-Test für verbundene Stichproben (Field et al., 2012, S. 386) durchgeführt werden. 5.10.1.3 Extra 1: Querschnittsdaten mit unterschiedlichem Variablensatz verschmelzen Manchmal müssen Querschnittsdaten mit unterschiedlichem Variablensatz verschmolzen werden. Diese entstehen z. B., wenn in zwei Fächern im Wesentlichen dieselben Variablen erhoben werden, aber eine davon fachspezifisch ist: df_physik_pre &lt;- data.frame( pcode = c(&quot;GK1203&quot;, &quot;ZB8900&quot;, &quot;WS4451&quot;), vorwissen_pre = c(2, 4, 5), physiknote = c(1, 2, 1), alter = c(21, 21, 25), fach = c(&quot;Physik&quot;,&quot;Physik&quot;,&quot;Physik&quot;) ) df_chemie_pre &lt;- data.frame( pcode = c(&quot;NN8780&quot;, &quot;FD1212&quot;, &quot;KK0996&quot;), vorwissen_pre = c(2, 4, 5), chemienote = c(2, 1, 2), alter = c(21, 21, 25), fach = c(&quot;Chemie&quot;,&quot;Chemie&quot;,&quot;Chemie&quot;) ) df_physik_pre ## pcode vorwissen_pre physiknote alter fach ## 1 GK1203 2 1 21 Physik ## 2 ZB8900 4 2 21 Physik ## 3 WS4451 5 1 25 Physik df_chemie_pre ## pcode vorwissen_pre chemienote alter fach ## 1 NN8780 2 2 21 Chemie ## 2 FD1212 4 1 21 Chemie ## 3 KK0996 5 2 25 Chemie Nun funktioniert das Verschmelzen mit rbind() nicht mehr, da rbind() identische Variablen benötigt: rbind(df_physik_pre, df_chemie_pre) Es entsteht dann die Fehlermeldung: #Fehler in match.names(clabs, names(xi)) : names do not match previous names Im Folgenden werden zwei Lösungsmöglichkeiten vorgestellt. Lösung 1 zeigt einen kleinschrittigen Weg und soll zum Verständnis der Operationen beitragen. Lösung 2 verstetigt diese Einzelschritte in Form einer handlichen eigenen Funktion, die dann für spätere Fälle wieder eingesetzt werden kann und den Aufwand auf eine Zeile Code minimiert. Lösung 1: kleinschrittig #Nehme die Variablen physiknote und chemienote # (sind je an Stelle 3 im Datensatz) zunächst heraus: df_physik_pre_ohnephysiknote &lt;- df_physik_pre[, -c(3)] df_chemie_pre_ohnechemienote &lt;- df_chemie_pre[, -c(3)] df_physik_pre_ohnephysiknote ## pcode vorwissen_pre alter fach ## 1 GK1203 2 21 Physik ## 2 ZB8900 4 21 Physik ## 3 WS4451 5 25 Physik df_chemie_pre_ohnechemienote ## pcode vorwissen_pre alter fach ## 1 NN8780 2 21 Chemie ## 2 FD1212 4 21 Chemie ## 3 KK0996 5 25 Chemie Beide Datensätze enthalten nun exakt dieselben Variablennamen. Nun kann die rbind()-Funktion wieder eingesetzt werden: #Verschmelze diese, nun identische Variablen enthaltenden, Teile: df_pre &lt;- rbind(df_physik_pre_ohnephysiknote, df_chemie_pre_ohnechemienote) df_pre ## pcode vorwissen_pre alter fach ## 1 GK1203 2 21 Physik ## 2 ZB8900 4 21 Physik ## 3 WS4451 5 25 Physik ## 4 NN8780 2 21 Chemie ## 5 FD1212 4 21 Chemie ## 6 KK0996 5 25 Chemie Nun können wir unser Wissen aus dem Abschnitt 5.10.1.2 anwenden. Wir wollen die Physiknote der folgenden Personen wieder in den Datensatz integrieren: df_physik_pre_physiknote &lt;- df_physik_pre[,c(&quot;pcode&quot;,&quot;physiknote&quot;)] df_physik_pre_physiknote ## pcode physiknote ## 1 GK1203 1 ## 2 ZB8900 2 ## 3 WS4451 1 Da dieser Datensatz dieselben Personencodes enthält wie einige Personen im Datensatz df_pre, haben wir datentechnisch denselben Fall wie bei einem Längsschnitt, können also die Funktion merge() benutzen: df_pre_inkl_physiknote &lt;- merge(df_pre, df_physik_pre_physiknote, by = &quot;pcode&quot;, all = T) df_pre_inkl_physiknote ## pcode vorwissen_pre alter fach physiknote ## 1 FD1212 4 21 Chemie NA ## 2 GK1203 2 21 Physik 1 ## 3 KK0996 5 25 Chemie NA ## 4 NN8780 2 21 Chemie NA ## 5 WS4451 5 25 Physik 1 ## 6 ZB8900 4 21 Physik 2 #Dasselbe können wir für die Chemienote wiederholen: df_chemie_pre_chemienote &lt;- df_chemie_pre[,c(&quot;pcode&quot;,&quot;chemienote&quot;)] df_chemie_pre_chemienote ## pcode chemienote ## 1 NN8780 2 ## 2 FD1212 1 ## 3 KK0996 2 # Ein Zusammenfügen dieser beiden Datensätze ergibt dann das gewünschte Ergebnis: df_pre_final &lt;- merge(df_pre_inkl_physiknote, df_chemie_pre_chemienote, by = &quot;pcode&quot;, all = T) df_pre_final ## pcode vorwissen_pre alter fach physiknote chemienote ## 1 FD1212 4 21 Chemie NA 1 ## 2 GK1203 2 21 Physik 1 NA ## 3 KK0996 5 25 Chemie NA 2 ## 4 NN8780 2 21 Chemie NA 2 ## 5 WS4451 5 25 Physik 1 NA ## 6 ZB8900 4 21 Physik 2 NA Lösung 2: Eigene Funktion Warnung: Diese Funktion funktioniert nur, wenn alle Variablen entweder vom Typ numeric oder character sind (keine factor-Variablen unterstützt, Umwandlung über as.numeric() oder as.character() möglich) und die beiden Datensätze x und y komplett unterschiedliche Personencodes enthalten (vorher bitte prüfen). merge2 &lt;- function(x, y, identifier) { i &lt;- intersect(names(x), names(y)) j &lt;- setdiff(names(x), names(y)) k &lt;- setdiff(names(y), names(x)) merged_data_1 &lt;- rbind(x[, i], y[, i]) merged_data_2 &lt;- merge(merged_data_1, x[, c(identifier, j)], by = identifier, all = T) merged_data_3 &lt;- merge(merged_data_2, y[, c(identifier, k)], by = identifier, all = T) return(merged_data_3) } Ohne zu sehr in’s Detail zu gehen, vollführt die Funktion ähnliche Operationen wie in Lösung 1: Mit der intersect()-Funktion werden gemeinsame Variablennamen gewonnen, mit setdiff() werden nicht-gemeinsame Variablennamen gewonnen. Die rbind()-Funktion fügt auch hier die gemeinsamen Variablenspalten aneinander, die merge()-Funktion übernimmt dann das Verschmelzen mit den je Datensatz einzigartigen Variablen. Der Einsatz der Funktion ist (nachdem der Code der Funktion oben einmal ausgeführt wurde) denkbar einfach: # Das dritte Argument entspricht dem &quot;by = &quot;-Argument der regulären merge()-Funktion df_pre &lt;- merge2(df_physik_pre, df_chemie_pre, &quot;pcode&quot;) df_pre ## pcode vorwissen_pre alter fach physiknote chemienote ## 1 FD1212 4 21 Chemie NA 1 ## 2 GK1203 2 21 Physik 1 NA ## 3 KK0996 5 25 Chemie NA 2 ## 4 NN8780 2 21 Chemie NA 2 ## 5 WS4451 5 25 Physik 1 NA ## 6 ZB8900 4 21 Physik 2 NA Das Ergebnis entspricht dem aus Lösung 1. 5.10.1.4 Extra 2: Querschnittsdaten verschmelzen, die identische Personen enthalten Bezogen auf unser Physik- und Chemie-Beispiel ist es denkbar, dass eine Person sowohl Physik, als auch Chemie belegt. Diese Person würde folglich sowohl an der Physik- als auch an der Chemie-Befragung teilnehmen und käme in beiden Datensätzen vor: df_physik_pre &lt;- data.frame( pcode = c(&quot;GK1203&quot;, &quot;ZB8900&quot;, &quot;WS4451&quot;), vorwissen_pre = c(2, 4, 5), alter = c(21, 25, 25), fach = c(&quot;Physik&quot;,&quot;Chemie&quot;,&quot;Physik&quot;) ) df_chemie_pre &lt;- data.frame( pcode = c(&quot;NN8780&quot;, &quot;FD1212&quot;, &quot;ZB8900&quot;), vorwissen_pre = c(2, 4, 5), alter = c(21, 21, 25), fach = c(&quot;Chemie&quot;,&quot;Chemie&quot;,&quot;Chemie&quot;) ) df_physik_pre ## pcode vorwissen_pre alter fach ## 1 GK1203 2 21 Physik ## 2 ZB8900 4 25 Chemie ## 3 WS4451 5 25 Physik df_chemie_pre ## pcode vorwissen_pre alter fach ## 1 NN8780 2 21 Chemie ## 2 FD1212 4 21 Chemie ## 3 ZB8900 5 25 Chemie Das folgende Vorgehen eignet sich auch für deutlich größere Datensätze: #Schritt 1: Prüfe, ob solche Fälle vorliegen: #Vektor mit allen pcodes hintereinander: pcodes &lt;- c(df_physik_pre$pcode,df_chemie_pre$pcode) pcodes ## [1] &quot;GK1203&quot; &quot;ZB8900&quot; &quot;WS4451&quot; &quot;NN8780&quot; &quot;FD1212&quot; &quot;ZB8900&quot; Für die Identifikation doppelter Personencodes gibt es die Funktion duplicated() (vgl. Abschnitt 5.8.1). Diese gibt für alle Personencodes den Wert TRUE oder FALSE aus und analysiert dabei zuerst die Person 1, dann 2 usw. Wird ein pcode als TRUE gekennzeichnet, so kam dieser bereits vorher vor (Beispiel: Wenn Person 2 und 15 denselben pcode aufweisen, so steht dann FALSE bei Person 2 und TRUE bei Person 15). duplicated(pcodes) ## [1] FALSE FALSE FALSE FALSE FALSE TRUE # es gibt also Fälle, die &quot;TRUE&quot; sind table(duplicated(pcodes)) #und zwar genau ein Paar ## ## FALSE TRUE ## 5 1 Identifiziere den Personencode: #Schritt 2: Welcher pcode gehört dazu? pcodes[duplicated(pcodes)] # Es ist der pcode ZB8900 ## [1] &quot;ZB8900&quot; #Betrachte Daten dieser Person je Datensatz: df_physik_pre[df_physik_pre$pcode==&quot;ZB8900&quot;,] ## pcode vorwissen_pre alter fach ## 2 ZB8900 4 25 Chemie df_chemie_pre[df_chemie_pre$pcode==&quot;ZB8900&quot;,] ## pcode vorwissen_pre alter fach ## 3 ZB8900 5 25 Chemie # Das Alter stimmt auch überein =&gt; Das ist ein und dieselbe Person! # Ziel: Doppelte Person im Physik-Datensatz entfernen, aber alle Daten erhalten. =&gt; Nur vorwissen_pre ist unterschiedlich =&gt; Bilde Mittelwert 4.5 df_chemie_pre[df_chemie_pre$pcode == &quot;ZB8900&quot;, &quot;vorwissen_pre&quot;] &lt;- 4.5 Die Person hat, wie es bei Fragebogenfragen üblich ist, das Kreuz beim Vorwissen nicht exakt gleich gesetzt. Eine Möglichkeit ist hier, den Mittelwert zu bilden. Aus dem Physik-Datensatz soll nun die doppelte Person entfernt werden, dies gelingt über ein Überschreiben des Datensatzes mit einem Datensatz ohne diese Person: #man beachte das UNGLEICH != df_physik_pre &lt;- df_physik_pre[df_physik_pre$pcode != &quot;ZB8900&quot;, ] #prüfe Erfolg durch Wiederholung der ersten Code-Zeilen: pcodes &lt;- c(df_physik_pre$pcode, df_chemie_pre$pcode) table(duplicated(pcodes)) ## ## FALSE ## 5 Es sind nun keine doppelten Fälle mehr vorhanden und es kann z. B. mit dem Verschmelzen der Datensätze begonnen werden. 5.10.2 Zeitformate in R In manchen Untersuchungen ist es notwendig mit sog. Timecodes (bspw. Stunde:Minute:Sekunde) zu arbeiten. Dies kann beispielsweise eine Reaktionszeit einer Person oder die durchschnittliche Zeit zum Lösen einer Aufgabe in einem Test sein. R kann mit diesem Format wenig Rechenoperationen durchführen, weshalb es von Vorteil ist, dieses in Sekunden bzw. je nach Format in die kleinste mögliche Einheit umzuwandeln. In unserem Fall handelt es sich um einen Datensatz, welcher die Zeit zum Lösen einer Aufgabe pro Proband:in enthält. Diese wird im Format Stunde:Minute:Sekunde dargestellt. Wir betrachten zunächst die Daten: dfr ## pcode vorwissen_pre alter fach zeit1 zeit2 ## 1 GK1203 2 21 Physik 00:06:01 00:05:49 ## 2 ZB8900 4 25 Chemie 00:05:10 00:05:2 ## 3 WS4451 5 25 Physik 00:07:15 00:04:33 Zunächst sollten wir auch hier darauf achten, mögliche Faktoreigenschafften zu entfernen (über as.numeric(), s.u.). Danach können wir die Daten mit folgendem Code umwandeln (Erläuterung folgt): dfr[, c(5:6)] &lt;- apply(dfr[, c(5:6)], c(1, 2), function(x) as.numeric(as.difftime(x, format = &quot;%H:%M:%S&quot;, units = &quot;sec&quot;))) Die apply()-Funktion wendet zunächst alle weiteren Schritte nur auf die Spalten 5 und 6 an. Dies sind die Variablen mit Zeitinformationen. In diesen Spalten wird nun zunächst der Timecode als solcher erkannt (as.difftime()) und das passende Urspungs-Format (format = \"%H:%M:%S\") sowie Zielformat bzw. die Zieleinheit (units = \"sec\") angegeben. Im Anschluss wird mit der as.numeric()-Funktion dieses neue Format in ein Numeric-Datenformat (Zahl) umgewandelt, mit dem R später rechnen kann. Hat man die Daten entsprechend umgewandelt, lassen sich alle weiteren Berechnungen wie bspw. t-Tests o. Ä. problemlos durchführen. Möchte man eine andere Einheit (bspw. Minuten statt Sekunden) verwenden, kann man den das Argument units entsprechend anpassen (units = \"min\"). Bibliographie Field, A. P., Miles, J. &amp; Field, Z. (2012). Discovering statistics using R. London ; Thousand Oaks, Calif: Sage. Lüdtke, O., Robitzsch, A., Trautwein, U. &amp; Köller, O. (2007). Umgang mit fehlenden Werten in der psychologischen Forschung. Psychologische Rundschau, 58(2), 103–117. Porst, R. (2014). Fragebogen: ein Arbeitsbuch (Studienskripten zur Soziologie) (4., erw. Aufl.). Wiesbaden: Springer VS. Mit apply(datensatz, 2, help_percentmissing) können auch missing-Quoten je Variable bestimmt werden.↩︎ Die hohen missing-Quoten stammen aus nicht-erfolgreich verlaufenden matchings und technischen Importartefakten wie ganzen NA-Zeilen: Es sind also verwaiste Fälle und technisch bedingte Leerfälle.↩︎ "],["praxisorientierte-einführung-in-die-item-response-theorie-mit-dem-fokus-auf-das-rasch-modell.html", "6 Praxisorientierte Einführung in die Item-Response-Theorie mit dem Fokus auf das Rasch-Modell 6.1 Motivation und konzeptuelle Herleitung 6.2 Der Datensatz 6.3 Laden der benötigten Pakete und des Datensatzes 6.4 Modellschätzung 6.5 Erzeugen einer Wright-Map 6.6 ICC (Item Characteristic Curve)", " 6 Praxisorientierte Einführung in die Item-Response-Theorie mit dem Fokus auf das Rasch-Modell Autorinnen: Tina Grottke, Philipp Möhrke, Marvin Rost 6.1 Motivation und konzeptuelle Herleitung Bei vielen Tests geht es um die Frage, ob eine Testperson eine bestimmte Fähigkeit besitzt und damit eine Aufgabe richtig lösen kann. (Um Verwirrung zu vermeiden sei an dieser Stelle darauf hingewiesen, dass der Einfachheit halber nicht zwischen Aufgabe und Item unterschieden und genrell das Wort Aufgabe verwendet wird.) Diese Fähigkeit, die man messen möchte, ist aber meist nicht direkt zugänglich. Man spricht auch von einer latenten Fähigkeit. Der Zugang zu dieser Fähigkeit kann also nur über Aufgaben oder Fragen erfolgen, die die Testperson beantwortet und in deren Beantwortung sich die latente Fähigkeit zeigt. Die Wahrscheinlichkeit, dass eine Person eine Aufgabe richtig lösen kann, hängt sowohl von der Fähigkeit der Person als auch von der Schwierigkeit der Aufgabe ab. Eine Aufgabe, die sehr oft richtig gelöst wird, könnte z.B. sehr einfach sein. Die Probanden könnten aber auch alle eine sehr hohe Fähigkeit haben, die ihnen erlaubt, die Aufgabe erfolgreich zu bearbeiten. Beide Punkte sind also auf den ersten Blick nicht voneinander zu trennen. Die Rasch-Modellierung (benannt nach dem dänischen Statistiker Georg Rasch (1901–1980)) löst dieses Problem. Das Rasch-Modell beschreibt wie wahrscheinlich es ist, dass ein Proband \\(n\\) ein Item \\(i\\) richtig löst in Abhängigkeit eines individuellen Personen- oder Personenfähigkeitsparameters \\(\\theta_n\\) für jede Person, welche das Fähigkeitsausmaß der Person \\(n\\) beschreibt und eines Aufgaben- oder Aufgabenschwierigkeitsparameters \\(\\delta_i\\), der den Schwierigkeitsgrad der Aufgabe \\(i\\) beschreibt. Man erhält also aus einem Datensatz von richtigen oder falschen Antworten (ordinale Daten) der Personen zu verschiedenen Aufgaben ein kontinuierliches intervallskaliertes Maß in Form der Personenfähigkeitsparameter \\(\\theta_n\\) und der Aufgabenschwierigkeitsparameter \\(\\delta_i\\). Prinzipiell ist dieses Modell nicht auf dichotom zu bewertende Aufgaben beschränkt. Auch likert-skalierte Items sind z.B. möglich, sollen aber nicht Gegenstand dieses Beitrages sein. Die Modellannahmen lauten wie folgt: Das Maß der Fähigkeit jedes Probanden ist ausschließlich durch den Personenfähigkeitsparameter charakterisiert. Das heißt, dass es keine anderen Einflussfaktoren auf Seiten der Person gibt und ein sog. eindimensionales Modell berechnet wird. Die Schwierigkeit der einzelnen Aufgaben ist ausschließlich durch den Aufgabenschwierigkeitsparameter charakterisiert. Die Schwierigkeit stellt also ebenfalls ein eindimensionales Merkmal dar. Beide Parameter werden auf derselben Skala gemessen. Die Leistungen einer Person hängt über alle Aufgaben hinweg – abgesehen von Zufall –, einzig von der Fähigkeit des Probanden und der Schwierigkeit der Aufgabe ab, nicht aber davon, welche anderen Aufgaben er oder sie bereits gelöst hat oder noch lösen wird. Wie modelliert das Rasch-Modell nun aber genau das Antwortverhalten? Der Zusammenhang zwischen der Lösungswahrscheinlichkeit einer Aufgabe, deren Schwierigkeit und der Fähigkeit der Person sollte jeden Fall derart sein, dass die Wahrscheinlichkeit einer richtigen Antwort mit wachsender Fähigkeit steigt. Im Idealfall könnte man annehmen, dass die Wahrscheinlichkeit einer richtigen Antwort für Personen mit einer niedrigen Fähigkeit bei null liegt und ab einer bestimmten Fähigkeit sprunghaft auf eins ansteigt. Ganz so ideal wird es aber nicht sein und der Übergangsbereich wird etwas weicher verlaufen. Explizit wird angenommen, dass sich die Wahrscheinlichkeit einer richtigen Antwort einer logistischen Funktion folgend entwickelt. Man setzt für die Wahrscheinlichkeit einer richtigen Antwort (mit 1 codiert) unter der Bedingung einer Personenfähigkeit \\(\\theta_n\\) und Schwierigkeit der Aufgabe \\(\\delta_i\\) an: \\[P(1|\\theta_n, \\delta_i) = \\frac{\\exp(\\theta_n - \\delta_i)}{1+\\exp(\\theta_n - \\delta_i)} \\] Für eine Aufgabe mit dem Schwierigkeitsparameter 0 erhält man so folgenden Verlauf der Lösungswahrscheinlichkeit über den Fähigkeitsparameter. Personen mit einem Fähigkeitsparameter von -5.0 lösen diese Aufgabe also mit einer Wahrscheinlichkeit von etwa 0 (also wahrscheinlich nie), während Personen mit einem Fähigkeitsparameter von +5.0 die Aufgabe mit einer Wahrscheinlichkeit von etwa 1 (also wohl immer) lösen. Personen, deren Fähigkeitsparameter gerade so groß ist wie der Parameter der Aufgabenschwierigkeit, lösen die Aufgabe mit einer Wahrscheinlichkeit von 0.5 (also in etwa 50% der Fälle). Im folgenden Plot ist die Lösungswahrscheinlichkeit einer Aufgabe in Abhängigkeit der Personenfähigkeit aufgetragen. Die Schwierigkeit der Aufgabe kann über den Schieberegler verändert werden. So kann z.B. verfolgt werden, wie sich die Lösungswahrscheinlichkeit einer Person mit Fähigkeitsparameter 0 ändert, wenn die Aufgabenschwierigkeit variiert wird. Personen mit einem Personenfähigkeitsparameter von 0 würden also eine Aufgabe mit \\(\\delta = -0.5\\) mit einer Wahrscheinlichkeit von 0.6 lösen, während sie eine Aufgabe mit \\(\\delta = 1.7\\) nur mit einer Wahrscheinlichkeit von 0.15 lösen würde. Schaut man sich die Formel für die Lösungswahrscheinlichkeit noch einmal an, sieht man, dass die Lösungswahrscheinlichkeit für Aufgabe \\(i\\) einzig von der Differenz der Parameter für Personenfähigkeit und Aufgabengabenschwierigkeit abhängt. Formt man die Gleichung etwas um, kann man auch schreiben \\[\\ln\\left(\\frac{P(1|\\theta_n, \\delta_i)}{1 - P(1|\\theta_n, \\delta_i)}\\right) = \\ln\\left(\\frac{P(1|\\theta_n, \\delta_i)}{ P(0|\\theta_n, \\delta_i)}\\right) = \\theta_n - \\delta_i\\] Der natürliche Logarithmus der Chance (Wahrscheinlichkeit der richtigen Lösung geteilt durch Wahrscheinlichkeit der falschen Lösung) ist gleich der Differenz von Personenfähigkeits- und Aufgabenschwierigkeitsparameter. Plottet man die Chance gegen diese Differenz erhält man eine Gerade mit Steigung 1. Diese Steigung und damit die Steigung des Übergangs von \\(P\\) wird für das Rasch-Modell also als fest angenommen. Die Steigung wird auch als Trennschärfe bezeichnet, weil Sie eine Aussage darüber liefert, wie klar ein Item zwischen den Fähigkeiten zweier Personen trennen kann. Bei höheren Modellen wie dem 2-PL geht die Steigung als zusätzlicher Parameter ein. 6.1.1 Beispiel Stellt man sich für ein Beispiel vor, dass die Kochkompetenz von Probanden gemessen werden soll. Dies stellt eine latente Fähigkeit dar, die nur über Testaufgaben auf diesem Gebiet erforscht werden kann. Ein entsprechender Test sollte aus verschieden schweren Aufgaben bestehen, wie etwa Zubereiten einer Tütensuppe (einfach) Kochen einer Tomatensoße nach Rezept (mittel) Kochen von Milchreis (mittel) Zubereitung eines mehrgängigen Menüs ohne Rezept für 4 Personen (schwer) Welche Aufgabenschwierigkeitsparameter man diesen drei Aufgaben zuweisen würden, wie schwierig die Aufgaben also in Relation zueinander sind, ist nicht bekannt. Ob zum Beispiel Aufgabe 2 oder 3 schwieriger ist, ist schwer zu beurteilen. In realen Studien kann das bei der Erstellung der Aufgaben noch schwieriger sein. Lässt man nun eine Zahl von Probanden alle vier Aufgaben in beliebiger Reihenfolge bearbeiten und vermerkt Erfolg (Ergebnis genießbar) oder Misserfolg (nicht genießbar), so erhält man eine Liste mit den Ergebnissen der Personen als Zeilen und den Aufgaben als Spalten. Diese Form wird auch als tidy Data (aufgeräumte/ordentliche Daten) bezeichnet und ist die bevorzugte Form, in der Daten vorliegen sollten (Wickham, 2014) oder * Tidy data. Mit diesen Daten kann eine Rasch Analyse gemacht werden, mit welcher man ein intervallskaliertes Maß für sowohl die Kochfähigkeit jedes einzelnen Probanden erhält (die Personenfähigkeitsparameter \\(\\theta_n\\)) als auch für die Schwierigkeit der einzelnen Aufgaben (die Aufgabenschwierigkeitsparameter \\(\\delta_i\\)) erhält. 6.1.2 Wie wird der beste Fit gefunden? Der beste Fit zwischen Modell und Daten, also der Satz von \\(\\theta_n\\) und \\(\\delta_i\\), der die beste Passung zwischen Modell und Daten liefert, kann über verschiedene Verfahren wie unter anderem das Least-quares- oder aber das Maximum-Likelihood-Verfahren gefunden werden (Linacre, 1999). Bei letzterem wird durch ein iteratives Vorgehen, die sog. Likelihood-Funktion \\[ L_n= \\prod\\limits_{i}P(1/0|\\theta_n, \\delta_i)\\] für jede Personen \\(n\\) berechnet. Diese wird genau für den Personenfähigkeitsparameter maximal, für den die Wahrscheinlichkeit maximal wird, genau dieses Antwortverhalten auf die einzelnen Items zu erhalten. Es werden also die \\(\\theta_n\\) gesucht, für die die \\(L_n\\) maximal werden. Die Berechnung startet mit einer Schätzung der Itemschwierigkeit über den Anteil der Personen, die eine Aufgabe richtig bearbeitet haben \\[ \\delta_i = \\log\\left(\\frac{p}{1-p}\\right) \\] und sucht den Satz von Personenfähigkeitsparametern \\(\\theta_n\\), für den die Likelihood-Funktion maximal wird. Im nächsten Schritt wird mit diesen Satz von Personenfähigkeitsparametern \\(\\theta_n\\), die Likelihood-Funktion bezüglich der Itemschwierigkeitsparameter \\(\\delta_i\\) maximiert. So wird die Likelihood-Funktion immer weiter abwechseln bezüglich der \\(\\theta_n\\) und der \\(\\delta_i\\) maximiert. Dieser Vorgang sollte nach einer Zahl von Schritten konvergieren - sprich die Änderung der \\(\\theta_n\\) und \\(\\delta_i\\) wird immer kleiner und unterschreitet irgendwann einen festgelegten Wert. Dann wird der Prozess abgebrochen. Für einen Test mit zwei Items mit den Schwierigkeitsparametern (\\(\\delta_1 = -1\\), \\(\\delta_1 = 0.5\\)) wäre die Likelihood-Funktion eines Probanden mit einer richtigen Antwort im ersten Item und einer falschen im zweiten Item: \\[L(\\theta) = P(1|\\theta, \\delta_1 = -1) \\cdot P(0|\\theta, \\delta_2 = 0.5)\\\\ = \\left(\\frac{\\exp(\\theta + 1)}{1+\\exp(\\theta + 1)}\\right)\\cdot\\left(1-\\frac{\\exp(\\theta - 0.5)}{1+\\exp(\\theta - 0.5)}\\right)\\] Für diese muss nun das Maximum bezüglich \\(\\theta\\) gefunden werden. An dieser Stelle gibt es verschiedene Schätzmethoden für die Itemschwierigkeits- und Personenfähigkeitsparameter wie die Joint-Maximum-Likelihood-Schätzung (JML) und die Weighted Likelihood Schätzung (WLE), welche ein Bias in der Schätzung mit JML verringert. Die genauen Details dieser verschiedenen Verfahren sollen an dieser Stelle aber nicht weiter behandelt werden. 6.1.3 zum Weiterlesen Sehr interessante Videos zu diesem Thema, die das Ganze noch einmal wesentlich umfangreicher beschreiben, sind: A Conceptual Introduction to Item Response Theory Logistic Regression 6.2 Der Datensatz Der für diese Erklärung verwendete Datensatz stammt aus dem Paket TAM und hat das Format einer einfachen Matrix. Es handelt sich um simulierte Daten von 2000 Personen (als Zeilen) zu jeweils 40 Items (als Spalten). Die Spalten für die einzelnen Items sind mit I1 bis I40 bezeichnet. Eine Spalte für eine Personen-ID oder ähnliches gibt es nicht. Auch hat der Datensatz keine fehlenden Daten, d.h. zu jedem Probanden gibt es Daten zu jedem Item. Die Items selbst sind alle dichotom (richtig/falsch) und mit 1 und 0 codiert. data(data.sim.rasch) head(data.sim.rasch) ## I1 I2 I3 I4 I5 I6 I7 I8 I9 I10 I11 I12 I13 I14 I15 I16 I17 I18 I19 I20 I21 ## [1,] 1 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 ## [2,] 0 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 0 ## [3,] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 ## [4,] 1 0 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0 0 ## [5,] 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 ## [6,] 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 ## I22 I23 I24 I25 I26 I27 I28 I29 I30 I31 I32 I33 I34 I35 I36 I37 I38 I39 ## [1,] 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [2,] 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 ## [3,] 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 ## [4,] 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 ## [5,] 0 1 1 1 1 1 0 1 0 0 0 1 1 1 0 1 0 0 ## [6,] 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 ## I40 ## [1,] 0 ## [2,] 0 ## [3,] 1 ## [4,] 0 ## [5,] 0 ## [6,] 0 6.3 Laden der benötigten Pakete und des Datensatzes Da jede Anwendung der Software R die Nutzung von Paketen vorsieht, sind diese hier aufgelistet. Der geladene Datensatz wird mit dem “TAM”-Paket mitgeliefert. library(TAM) # TAM-Paket als zentrales Paket library(WrightMap) # zur Erzeugung von Wright-Maps data(data.sim.rasch) # Lerndatensatz mit 2000 Personen &amp; 40 Items, integriert in das TAM-Paket 6.3.1 Paketquellen “TAM” (Robitzsch, Kiefer &amp; Wu, 2020) “WrightMap” (Torres Irribarra &amp; Freund, 2016) 6.4 Modellschätzung Der folgende Code initiiert die Schätzung eines Rasch-Modells für den vorliegenden Datensatz. Das Listen-Objekt mod1PL mit 57 Sublisten wird erzeugt. Außerdem wird ein umfänglicher Konsolen-Output sichtbar. Dieser wird im Folgenden erläutert, es wird sich auf die mit Punkten getrennten Teilabschnitte bezogen. mod1PL &lt;- TAM::tam.mml(resp=data.sim.rasch) ## .................................................... ## Processing Data 2022-08-02 11:43:04 ## * Response Data: 2000 Persons and 40 Items ## * Numerical integration with 21 nodes ## * Created Design Matrices ( 2022-08-02 11:43:04 ) ## * Calculated Sufficient Statistics ( 2022-08-02 11:43:04 ) ## .................................................... ## Iteration 1 2022-08-02 11:43:04 ## E Step ## M Step Intercepts |---- ## Deviance = 84803.9272 ## Maximum item intercept parameter change: 0.33272 ## Maximum item slope parameter change: 0 ## Maximum regression parameter change: 0 ## Maximum variance parameter change: 0.125876 ## .................................................... ## Iteration 2 2022-08-02 11:43:04 ## E Step ## M Step Intercepts |--- ## Deviance = 84201.8406 | Absolute change: 602.0866 | Relative change: 0.00715052 ## Maximum item intercept parameter change: 0.044568 ## Maximum item slope parameter change: 0 ## Maximum regression parameter change: 0 ## Maximum variance parameter change: 0.167316 ## .................................................... ## Iteration 3 2022-08-02 11:43:04 ## E Step ## M Step Intercepts |-- ## Deviance = 84162.1461 | Absolute change: 39.6945 | Relative change: 0.00047164 ## Maximum item intercept parameter change: 0.017468 ## Maximum item slope parameter change: 0 ## Maximum regression parameter change: 0 ## Maximum variance parameter change: 0.069771 ## .................................................... ## Iteration 4 2022-08-02 11:43:04 ## E Step ## M Step Intercepts |-- ## Deviance = 84156.6153 | Absolute change: 5.5308 | Relative change: 6.572e-05 ## Maximum item intercept parameter change: 0.006594 ## Maximum item slope parameter change: 0 ## Maximum regression parameter change: 0 ## Maximum variance parameter change: 0.026369 ## .................................................... ## Iteration 5 2022-08-02 11:43:04 ## E Step ## M Step Intercepts |-- ## Deviance = 84155.879 | Absolute change: 0.7363 | Relative change: 8.75e-06 ## Maximum item intercept parameter change: 0.00247 ## Maximum item slope parameter change: 0 ## Maximum regression parameter change: 0 ## Maximum variance parameter change: 0.00966 ## .................................................... ## Iteration 6 2022-08-02 11:43:04 ## E Step ## M Step Intercepts |-- ## Deviance = 84155.7823 | Absolute change: 0.0966 | Relative change: 1.15e-06 ## Maximum item intercept parameter change: 0.000952 ## Maximum item slope parameter change: 0 ## Maximum regression parameter change: 0 ## Maximum variance parameter change: 0.0035 ## .................................................... ## Iteration 7 2022-08-02 11:43:04 ## E Step ## M Step Intercepts |-- ## Deviance = 84155.7696 | Absolute change: 0.0128 | Relative change: 1.5e-07 ## Maximum item intercept parameter change: 0.000396 ## Maximum item slope parameter change: 0 ## Maximum regression parameter change: 0 ## Maximum variance parameter change: 0.001263 ## .................................................... ## Iteration 8 2022-08-02 11:43:04 ## E Step ## M Step Intercepts |-- ## Deviance = 84155.7678 | Absolute change: 0.0018 | Relative change: 2e-08 ## Maximum item intercept parameter change: 0.00019 ## Maximum item slope parameter change: 0 ## Maximum regression parameter change: 0 ## Maximum variance parameter change: 0.000455 ## .................................................... ## Iteration 9 2022-08-02 11:43:04 ## E Step ## M Step Intercepts |-- ## Deviance = 84155.7674 | Absolute change: 4e-04 | Relative change: 0 ## Maximum item intercept parameter change: 0.000111 ## Maximum item slope parameter change: 0 ## Maximum regression parameter change: 0 ## Maximum variance parameter change: 0.000164 ## .................................................... ## Iteration 10 2022-08-02 11:43:04 ## E Step ## M Step Intercepts |- ## Deviance = 84155.7672 | Absolute change: 2e-04 | Relative change: 0 ## Maximum item intercept parameter change: 7.9e-05 ## Maximum item slope parameter change: 0 ## Maximum regression parameter change: 0 ## Maximum variance parameter change: 5.9e-05 ## .................................................... ## Item Parameters ## xsi.index xsi.label est ## 1 1 I1 -1.9590 ## 2 2 I2 -1.8570 ## 3 3 I3 -1.7444 ## 4 4 I4 -1.6407 ## 5 5 I5 -1.5448 ## 6 6 I6 -1.5344 ## 7 7 I7 -1.3466 ## 8 8 I8 -1.3499 ## 9 9 I9 -1.2603 ## 10 10 I10 -1.0469 ## 11 11 I11 -1.0233 ## 12 12 I12 -0.8563 ## 13 13 I13 -0.8002 ## 14 14 I14 -0.7447 ## 15 15 I15 -0.5202 ## 16 16 I16 -0.3962 ## 17 17 I17 -0.3727 ## 18 18 I18 -0.2298 ## 19 19 I19 -0.1062 ## 20 20 I20 -0.0522 ## 21 21 I21 0.0376 ## 22 22 I22 0.1044 ## 23 23 I23 0.3343 ## 24 24 I24 0.4232 ## 25 25 I25 0.5209 ## 26 26 I26 0.6415 ## 27 27 I27 0.6443 ## 28 28 I28 0.8484 ## 29 29 I29 0.9395 ## 30 30 I30 1.0179 ## 31 31 I31 1.1160 ## 32 32 I32 1.1372 ## 33 33 I33 1.3117 ## 34 34 I34 1.3149 ## 35 35 I35 1.5559 ## 36 36 I36 1.6198 ## 37 37 I37 1.7039 ## 38 38 I38 1.7490 ## 39 39 I39 1.9352 ## 40 40 I40 2.0502 ## ................................... ## Regression Coefficients ## [,1] ## [1,] 0 ## ## Variance: ## [,1] ## [1,] 1.404 ## ## ## EAP Reliability: ## [1] 0.899 ## ## ----------------------------- ## Start: 2022-08-02 11:43:04 ## End: 2022-08-02 11:43:04 ## Time difference of 0.08912086 secs ## Processing Data: Mit Uhrzeit und Datum versehener Diagnoseabschnitt. Calculated Sufficient Statistics bestätigt für das Raschmodell, dass ausreichend viele Personen ausreichend viele Aufgaben bearbeitet haben. Weitere Details werden an dieser Stelle nicht ausgeführt, es sei lediglich darauf hingewiesen, dass die Berechnung bereits an dieser Stelle abgebrochen werden kann. Dies geschieht bspw. wenn ein fehlerhafter Datensatz vorliegt, der von der Funktion nicht lesbar ist. Iteration 1 bis Iteration 10: Hier werden iterativ die Modellparameter geschätzt (vgl. Abschnitt 4.1.2). Weil im Raschmodell nur die Schwierigkeit (item intercept parameter) und keine Steigung (item slope parameter) und kein Einfluss von Hintergrundvariablen (regression parameter) in das Modell eingebaut wurden, sind diese konstant \\(0\\). Gut erkennbar ist das kontinuierliche Absinken der Deviance-Änderung (Absolute change &amp; Relative change): Die Berechnung konvergiert. Die Deviance selbst kann als Transformation der Likelihood (vgl. Abschnitt 3.1.2) verstanden werden. Sie ist also ein Maß für die Passung des Modells zu den Daten, die von Iteration zu Iteration besser wird, bis ein willkürlicher Grenzwert für die Änderung (Default bei tam.mml(): \\(0.0001\\)) erreicht wurde. Item Parameters: Äquivalent zum Output head(mod1PL$item_irt), s. u. bei der Erklärung der Kennwerte aus dem Objekt mod1PL$item. Regression Coefficients: Es wurden keine weiteren Hintergrundvariablen (z. B. Alter oder Gender) in das Modell eingefügt. Daher ist der Wert in diesem Fall \\(0\\). Variance: Varianz des berechneten Parameters. EAP Reliability: Expected-A-Posteriori-Reliabilität. Über ein statistisches Verfahren berechnetes Maß für die Reliabilität des Testinstruments. Kann in erster Näherung analog zu Cronbachs-\\(\\alpha\\) aus der klassischen Testtheorie interpretiert werden. Wichtiger Hinweis: Die EAP-Reliabilität ist ein zuverlässiger Schätzer für Populationen, nicht jedoch für Einzelpersonen! Folgend wird das Dataframe-Objekt mod1PL$item aus dem Listen-Objekt mod1PL ausgewählt und mit der Funktion head() die ersten 6 von insgesamt 40 Einträgen ausgegeben. Es enthält verschiedene Spalten mit deskriptiven Kennwerten: head(mod1PL$item) ## item N M xsi.item AXsi_.Cat1 B.Cat1.Dim1 ## I1 I1 2000 0.8270 -1.959017 -1.959017 1 ## I2 I2 2000 0.8145 -1.857027 -1.857027 1 ## I3 I3 2000 0.8000 -1.744435 -1.744435 1 ## I4 I4 2000 0.7860 -1.640747 -1.640747 1 ## I5 I5 2000 0.7725 -1.544800 -1.544800 1 ## I6 I6 2000 0.7710 -1.534362 -1.534362 1 item: Laufnummer der jeweiligen Items. N: Anzahl an Personen, die das jeweilige Item bearbeitet haben. M: Deskriptive Schwierigkeit des Items (\\(M = \\frac{N_{korrekt}}{N_{gelöst}}\\)). xsi.item: xsi wird ausgesprochen wie geschrieben und bezieht sich auf den griechischen Buchstaben (\\(\\xi\\)). Der \\(\\xi\\)-Parameter ist die, mit dem Modell geschätzte, Aufgabenschwierigkeit auf der Skala von \\(-\\infty\\) bis \\(\\infty\\) mit 0 als Mittelwert. Übliche Werte in der Praxis liegen zwischen -3 bis 3. Eine Aufgabe mit Schwierigkeit \\(\\xi=1\\) ist dabei schwerer als eine Aufgabe mit Schwierigkeit \\(\\xi=0\\). Oft findet man den Schwierigkeitsparameter auch unter anderen griechischen Buchstaben in der Literatur, z. B. \\(\\delta\\) oder \\(\\beta\\). TAM übernimmt die Notation mit \\(\\beta\\) unter der Bezeichnung IRT parametrization. Siehe dazu die ersten 6 von 40 Einträgen im folgenden Dataframe-Objekt mod1PL$item_irt. head(mod1PL$item_irt) ## item alpha beta ## 1 I1 1 -1.959017 ## 2 I2 1 -1.857027 ## 3 I3 1 -1.744435 ## 4 I4 1 -1.640747 ## 5 I5 1 -1.544800 ## 6 I6 1 -1.534362 AXsi_.Cat1: Da im gezeigten Beispiel nur ein eindimensionales Modell ohne weitere Bedingungen berechnet wurde, entspricht der Wert dem \\(\\xi\\)-Parameter, d. h. der Schwierigkeit. Unter anderen Bedingungen (beispielsweise Mehrdimensionalität, oder verschiedenen Testheftpositionen der Aufgabe), können verschiedende, bedingte Schwierigkeiten auftreten. Dazu ein fiktives, vereinfachtes Beispiel: Eine Aufgabe ist in zwei verschiedenen Testheften zu finden. In Testheft Nr. 1 ist sie immer die erste Aufgabe, in Testheft Nr. 2 ist sie immer die letzte Aufgabe. Die Testheftposition kann im statistischen Modell berücksichtigt werden und es würden dann zwei testheftabhängige Schwierigkeitsparameter berechnet, die sich als AXsi_.Cat1 und AXsi_.Cat2 im Output wiederfinden würden. B.Cat1.Dim1: Steigungsparameter. Im eindimensionalen 1PL-Modell wird der B-Parameter nicht berechnet, sondern auf \\(B=1\\) fixiert. Oft findet man den Steigungsparameter auch unter der Bezeichnung \\(\\alpha\\) (siehe Output oben zu xsi.item). Eine Zusammenfassung des Modellobjekts mit allen oben ausgeführten Kennwerten kann über den Befehl summary(mod1PL) erhalten werden. 6.4.1 Deutung der Aufgabenschwierigkeiten Die erhaltenen Schwierigkeitsparameter stehen unter dem Vorbehalt, dass das Rasch-Modell gültig ist (Wu, Tam &amp; Jen, 2016, S. 139ff.). Sie setzen die Aufgaben nun jenseits von subjektiven Einflüssen miteinander in Beziehung. War eine Schwierigkeitseinschätzung a-priori (bspw. durch ein Expertenrating) noch von zahlreichen, personenabhängigen Einflüssen getragen, sind nun die Bearbeitungserfolge durch die Zielgruppe in der quantitativ-empirischen Erhebung integriert: Die Schwierigkeiten wurden gemessen und nicht über den Daumen gepeilt. Noch vor weiterführenden Analysen können die so erhaltenen, quantitativen Befunde wiederum durch qualitative Urteile abgeglichen werden: Gibt es Aufgaben, die erwartungsgemäß schwerer/leichter waren als andere? Gibt es Aufgaben, deren Schwierigkeiten nicht erwartungskonform sind? War eine Aufgabe vielleicht zu einfach, weil die Distraktoren unpassend gewählt waren? … Die Aufgabenentwicklung führt so in einem Zusammenspiel aus objektiven Kriterien und subjektiver Synthese (bspw. Erfahrung von PraktikerInnen, Literaturrecherche, curriculare Anforderungen) zu validen und reliablen Testinstrumenten. 6.5 Erzeugen einer Wright-Map Mit einem Mathematiktest für GrundschülerInnen wird es kaum möglich sein, verschiedene Studierende der Mathematik und ihrer Fähigkeit zu rechnen zu unterscheiden. Analoges gilt für einen zu schweren Test für die GrundschülerInnen. Für Leistungstests ist also eine breite Verteilung von Aufgabenschwierigkeiten über die Personenfähigkeiten wünschenswert. Wright-Maps erlauben eine Prüfung der Abdeckung, indem sie die Aufgabenschwierigkeiten und die Personenfähigkeiten grafisch miteinander in Beziehung setzen. Es wird dabei auf einen Blick deutlich, wenn in einem bestimmten Schwierigkeits-/Fähigkeitsintervall zu wenige oder zuviele Aufgaben vorliegen. Dies ist insbesondere in der Pilotierungsphase eines Tests hilfreich. Das folgende Code-Beispiel schätzt die Personenfähigkeiten und schreibt sie über die Funktion tam.wle in das Objekt thetas_1pl. Außerdem werden die Aufgabenschwierigkeiten separat als Objekt item_xsis_1pl abgespeichert um anschließend mit der Funktion wrightMap() eine automatische Visualisierung zu erhalten (Abb. 6.1). thetas_1pl &lt;- tam.wle(mod1PL) item_xsis_1pl &lt;- mod1PL$xsi$xsi wrightMap(thetas_1pl$theta, item_xsis_1pl, main.title = &quot;Beispiel für Wright-Map&quot;, axis.persons = &quot;Fähigkeitsverteilung&quot;, axis.items = &quot;Aufgaben&quot;) Figure 6.1: Eine Variante für eine Wright-Map. Gut erkennbar ist die Fähigkeitsverteilung in Dimension 1 (schmaleres Panel links) und die numerische Zuordnung der Aufgaben zu dieser Dimension (breiteres Panel rechts). Ebenfalls deutlich wird die Äquivalenz von Personenfähigkeiten und Aufgabenschwierigkeiten, erkennbar an der Logit-Skala auf der rechten Seite. Ungünstig ist die kategorische Aufgaben-Achse. Da der Datensatz 40 Aufgaben enthält, resultiert ein Overplot der Aufgaben-Namen, dem mit einer Alternativdarstellung begegnet werden kann (Abb. 6.2). wrightMap(thetas_1pl$theta, item_xsis_1pl, item.side = itemClassic, main.title = &quot;Beispiel für Wright-Map&quot;, axis.persons = &quot;Fähigkeitsverteilung&quot;, axis.items = &quot;Aufgabenverteilung&quot;) Figure 6.2: Eine Alternative Darstellung der Wright-Map. In diesem Fall ist gut zu sehen, wie die Aufgaben - markiert durch ihren Laufindex aus dem Datensatz - von unten nach oben schwieriger werden. Außerdem erkennbar: Die Aufgaben sind näherungsweise uniform über das Schwierigkeits-/Fähigkeitsintervall verteilt. Außerdem stehen für sehr leistungsstarke und -schwache Personen (Ränder der Fähigkeitsverteilung) empirisch keine äquivalent schwierigen Aufgaben zur Verfügung. 6.5.1 Deutung der Wright-Maps Wäre dieser Datensatz aus einer realen Studie entstanden, wäre die Nachkonstruktion von sehr schweren und sehr leichten Aufgaben zu empfehlen. Darüber hinaus halten sich im mittleren Fähigkeitsniveau die meisten Personen auf. Um zwischen diesen besser differenzieren zu können, wären auch dort mehr Aufgaben hilfreich. 6.6 ICC (Item Characteristic Curve) 6.6.1 Input Item Characteristic Curves (kurz: ICC) geben die Lösungswahrscheinlichkeit als Funktion der Fähigkeit der Testpersonen an. Dabei stellt die y-Achse die Lösungswahrscheinlichkeit und x-Achse die Personenfähigkeit dar. Um Verwirrungen zur Begrifflichkeit Item und Aufgabe zu vermeiden, wird an dieser Stelle darauf hingewiesen, dass Items hier im vereinfachten Sinne einer Aufgabe entsprechen. 6.6.2 R-Befehl für Generierung der ICC in TAM Das Ausführen der folgenden Funktion führt zur Erstellung der ICC für Item 20 sowie der Anteile richtiger Lösungen von sechs Personenfähigkeitsgruppen zu dieser Aufgabe. Wie im folgenden erläutert, lässt sich damit untersuchen, inwiefern die Modellvorhersage (Rasch-Modell mit den auf Basis des Antwortverhaltens geschätzten Schwierigkeits- und Fähigkeitsparametern) und die Daten (tatsächliches Antwortverhalten) voneinander abweichen. plot(mod1, items = 20, ngroups = 6, export = FALSE) # export = FALSE verhindert separates Abspeichern der .png-Datei ## Iteration in WLE/MLE estimation 1 | Maximal change 0.8281 ## Iteration in WLE/MLE estimation 2 | Maximal change 0.4335 ## Iteration in WLE/MLE estimation 3 | Maximal change 0.0883 ## Iteration in WLE/MLE estimation 4 | Maximal change 7e-04 ## Iteration in WLE/MLE estimation 5 | Maximal change 0 ## ---- ## WLE Reliability= 0.894 Eine ICC kann alleine unter Kenntnis des Schwierigkeitsparameters der Aufgabe erstellt werden. Sie lässt sich nutzen, um grafisch zu beurteilen, inwiefern das tatsächliche Lösungsverhalten der Testpersonen von dem Verhalten, welches mithilfe des Rasch-Modells vorhergesagt wird, abweicht. Naiv würde man davon ausgehen, dass man nur die tatsächlichen Lösungswahrscheinlichkeiten in Abhängigkeit von den Fähigkeiten der Testpersonen eintragen müsste (so macht man es auch bspw. im Physikunterricht, bei der ersten grafischen linearen Regression). Diese Lösungswahrscheinlichkeiten sind aber nicht für einzelne Testpersonen, sondern nur für Gruppen, direkt beobachtbar. Aufgrunddessen werden die Testpersonen in Fähigkeitsgruppen eingeteilt (Binning). Der Prozentsatz einer bestimmten Gruppe, der die Aufgabe richtig gelöst hat, entspricht dann der Lösungswahrscheinlichkeit. Die Angaben MLE (Maximum Likelihood Estimates) und WLE (Weighted Likelihood Estimates) in der Ausgabe stehen für das angewandte Verfahren zur Schätzung der Personenfähigkeiten. Bei den durch die MLE geschätzten Personenfähigkeiten kommt es jedoch zu Verzerrungen, sodass auf den WLE zurückgegriffen wird (Trendtel, Pham &amp; Yanagida, 2016, S. 202). 6.6.3 Interpretation ICC im Rasch-Modell (Wu et al., 2016) Ausführungen beziehen sich auf Item 20, s. obigen Plot blaue Kurve: Modellkurve (vom Modell vorhergesagter Zusammenhang zw. Fähigkeit und Lösungswahrscheinlichkeit) –&gt; Testpersonen mit einer Fähigkeitsausprägung von 1, lösen das Item mit 75%iger Wahrscheinlichkeit richtig schwarze Kurve: empirische Kurve (beobachtete Lösungswahrscheinlichkeit bzw. Lösungsquote) –&gt; etwa 76 % der Testpersonen mit der Fähigkeitsausprägung (ability) = 1 lösten das Item richtig –&gt; Bei diesem Beispiel (Item 20) liegt eine gute Passung beider Kurven vor, d.h. das Modell beschreibt das Lösungsverhalten bei diesem Item sehr gut. 6.6.3.1 Misfits Für die Beurteilung von Items, kann die Passung der empirischen Kurve zur Modellkurve herangezogen werden. Dabei muss nicht immer die empirische Kurve gut zur Modellkurve passen –&gt; Misfits Es wird dabei zw. Overfit und Underfit unterschieden, wobei der wMNSQ-Wert (= standardized weighted mean square), welcher auch als Infit-Wert geführt wird, bei der Beurteilung eine Orientierung bieten kann “Die Infit-Statistik ist der mit den Varianzen […] gewichtete Mittelwert, sodass Personen mit einer Fähigkeit näher bei der Itemschwierigkeit des betrachteten Items ein höheres Gewicht bekommen als Personen, deren Fähigkeit weiter weg liegt.” (Trendtel et al., 2016, S. 209). Dabei sei darauf hingewiesen, dass trotz guter wMNSQ-Werte (Werte um 1), die Passung zwischen empirischer Kurve und Modellkurve als schlecht beurteilt werden kann. “Fit mean-square statistic is testing whether the slope of the observed ICC is the same as the theoretical ICC” (Wu et al., 2016, S. 145). Die Outfit-Statistik, zu der der unweighted mean square (MNSQ-Outfit) gehört, wird in Large-Scale-Assessments kaum Bedeutung zugetragen, sodass sich auf die Infit-Statistik fokussiert wird (Trendtel et al., 2016, S. 209). Overfit: wMNSQ-Wert (Infit) &lt; 1; empirische Kurve steiler als Modellkurve; entspricht hoher discrimination –&gt; Item unterscheidet Testpersonen verschiedener Fähigkeitsausprägungen besser als andere Items im Test Underfit: wMNSQ-Wert (Infit) &gt; 1; empirische Kurve flacher als Modellkurve; entspricht geringerer discrimination Es wird empfohlen, Items mit einem Overfit zu behalten und Items mit einem Underfit aus dem Test zu entfernen (Wu et al., 2016, S. 153). 6.6.4 Gegenüberstellung leichtes - schweres Item mod1$xsi # Itemschwierigkeiten xsi aller Items ## xsi se.xsi ## I1 -1.95901708 0.06465854 ## I2 -1.85702665 0.06311470 ## I3 -1.74443543 0.06153710 ## I4 -1.64074652 0.06019663 ## I5 -1.54480023 0.05904820 ## I6 -1.53436162 0.05892844 ## I7 -1.34662700 0.05694173 ## I8 -1.34987110 0.05697344 ## I9 -1.26034689 0.05613124 ## I10 -1.04688427 0.05438831 ## I11 -1.02329441 0.05421784 ## I12 -0.85632426 0.05313222 ## I13 -0.80020663 0.05281391 ## I14 -0.74473359 0.05252176 ## I15 -0.52021484 0.05156219 ## I16 -0.39622299 0.05118169 ## I17 -0.37267520 0.05112121 ## I18 -0.22981140 0.05083401 ## I19 -0.10615527 0.05069507 ## I20 -0.05221976 0.05066614 ## I21 0.03760166 0.05066059 ## I22 0.10436417 0.05069098 ## I23 0.33428538 0.05102212 ## I24 0.42316815 0.05124518 ## I25 0.52089569 0.05155264 ## I26 0.64154518 0.05202355 ## I27 0.64425227 0.05203529 ## I28 0.84842093 0.05307253 ## I29 0.93949280 0.05363422 ## I30 1.01792547 0.05416852 ## I31 1.11604992 0.05490459 ## I32 1.13721663 0.05507343 ## I33 1.31166540 0.05660515 ## I34 1.31487135 0.05663570 ## I35 1.55588652 0.05919171 ## I36 1.61976438 0.05995850 ## I37 1.70391234 0.06102867 ## I38 1.74904684 0.06163154 ## I39 1.93524957 0.06434122 ## I40 2.05021754 0.06620206 Item 1 plot(mod1, items = 1, ngroups = 6, export = FALSE) # xsi = -1.96 ## Iteration in WLE/MLE estimation 1 | Maximal change 0.8281 ## Iteration in WLE/MLE estimation 2 | Maximal change 0.4335 ## Iteration in WLE/MLE estimation 3 | Maximal change 0.0883 ## Iteration in WLE/MLE estimation 4 | Maximal change 7e-04 ## Iteration in WLE/MLE estimation 5 | Maximal change 0 ## ---- ## WLE Reliability= 0.894 Item 40 plot(mod1, items = 40, ngroups = 6, export = FALSE) # xsi = 2.05 ## Iteration in WLE/MLE estimation 1 | Maximal change 0.8281 ## Iteration in WLE/MLE estimation 2 | Maximal change 0.4335 ## Iteration in WLE/MLE estimation 3 | Maximal change 0.0883 ## Iteration in WLE/MLE estimation 4 | Maximal change 7e-04 ## Iteration in WLE/MLE estimation 5 | Maximal change 0 ## ---- ## WLE Reliability= 0.894 6.6.4.1 Interpretation Testpersonen mit einer geschätzten Fähigkeit von 0 lösen diese beiden Items mit unterschiedlichen Lösungswahrscheinlichkeiten. Während Testpersonen mit einer Fähigkeitsausprägung von 0 das Item 1 mit etwa 90%iger Wahrscheinlichkeit richtig lösen, lösen diese das Item 40 mit einer etwa 15%igen Lösungswahrscheinlichkeit. 6.6.5 Gegenüberstellung ICC Rasch - 2PL Item 40 Rasch plot(mod1, items = 40, ngroups = 6, export = FALSE) # Steigungsparameter = 1 ## Iteration in WLE/MLE estimation 1 | Maximal change 0.8281 ## Iteration in WLE/MLE estimation 2 | Maximal change 0.4335 ## Iteration in WLE/MLE estimation 3 | Maximal change 0.0883 ## Iteration in WLE/MLE estimation 4 | Maximal change 7e-04 ## Iteration in WLE/MLE estimation 5 | Maximal change 0 ## ---- ## WLE Reliability= 0.894 Item 40 2PL plot(mod2, items = 40, ngroups = 6, export = FALSE) # Steigungsparameter frei geschätzt ## Iteration in WLE/MLE estimation 1 | Maximal change 0.3445 ## Iteration in WLE/MLE estimation 2 | Maximal change 0.0677 ## Iteration in WLE/MLE estimation 3 | Maximal change 5e-04 ## Iteration in WLE/MLE estimation 4 | Maximal change 0 ## ---- ## WLE Reliability= 0.894 6.6.5.1 Deutung Beim 2PL Modell wird der Steigungsparameter für jedes Item frei geschätzt. Das zeigt sich unter anderem in den ICCs. Durch das Vorliegen des simulierten Datensatzes zeigt sich kein merkbarer Unterschied in den ICCs des Rasch und 2PL Modells. Dennoch sei an dieser Stelle angemerkt, dass es beim 2PL Modell durchaus zu ICCs kommen kann, die negative Steigungsparameter aufweisen. Die Interpretation dahinter wäre: Bei steigender Personenfähigkeit, sinkt die Lösungswahrscheinlichkeit des Items. Dies kann ein Indiz für ein nicht funktionierendes Item sein, aber ebenso ein Hinweis auf eine Mehrdimensionalität des Konstruktes (Bühner, 2011, S. 506; Chalmers, 2015, S. 216). Jenes Item könnte demnach in eine andere Dimension fallen. Bibliographie Bühner, M. (2011). Einführung in die Test- und Fragebogenkonstruktion (3. Auflage). München: Pearson. Verfügbar unter: http://ebookcentral.proquest.com/lib/huberlin-ebooks/detail.action?docID=5133497 Chalmers, R. P. (2015). Extended Mixed-Effects Item Response Models With the MH-RM Algorithm. Journal of Educational Measurement, 52(2), 200–222. https://doi.org/10.1111/jedm.12072 Linacre, J. M. (1999). Understanding Rasch measurement: estimation methods for Rasch Measures. Journal of Outcome Measurement, 3, 381–405. Robitzsch, A., Kiefer, T. &amp; Wu, M. (2020). TAM: Test Analysis Modules. Verfügbar unter: https://CRAN.R-project.org/package=TAM Torres Irribarra, D. &amp; Freund, R. (2016). IRT Item-Person-Map with ’ConQuest’ Integration. Verfügbar unter: https://github.com/david-ti/wrightmap Trendtel, M., Pham, G. &amp; Yanagida, T. (2016). Skalierung und Linking. In S. Breit &amp; C. Schreiner (Hrsg.), Large-Scale Assessment mit R: Methodische Grundlagen der österreichischen Bildungsstandardüberprüfung (S. 185–224). Wien: Facultas Verlags- und Buchhandels AG. Wickham, H. (2014). Tidy data. The Journal of Statistical Software, 59. Wu, M., Tam, H. P. &amp; Jen, T.-H. (2016). Educational Measurement for Applied Researchers: Theory into Practice. Singapore: Springer. https://doi.org/10.1007/978-981-10-3302-5 "],["auswertungsmöglichkeiten-von-sortieraufgaben.html", "7 Auswertungsmöglichkeiten von Sortieraufgaben 7.1 Der Datensatz 7.2 Die Auswertungsmöglichkeiten 7.3 Vergleich und Diskussion", " 7 Auswertungsmöglichkeiten von Sortieraufgaben Autor: Arne Bewersdorff Zur Erhebung von Fachwissen von Lernenden existiert eine Reihe von Testinstrumenten für die unterschiedlichen Disziplinen. Im Wesentlichen werden dabei offene oder gebundene Antwortformate eingesetzt: Bei den offenen Antwortformaten ist der Proband aufgefordert mit eigenen Worten, einer Zeichnung o. ä. zu antworten. Gebundene Antwortformate geben Antwortoptionen vor. Bei der gebundenen Beantwortung werden festgelegte Antwortkategorien vorgegeben, es gibt keinen Freiraum für eigene Antworten. Sortieraufgaben (Umordnungsaufgaben) gehören zu den gebundenen Antwortformaten (vgl. Bühner (1998)). Beispiele für den Einsatz solcher Sortieraufgaben sind vielfältig: Im Rahmen eines Fachwissenstests können die Probanden etwa aufgefordert werden die einzelnen Prozesse der Mitose in die korrekte Reihenfolge zu bringen. Soll das Prozesswissen zum Experimentieren (didaktisch reduziert) erfasst werden, bietet es sich an, die einzelnen Teilprozesse des deduktiven Experiments in die richtige Reihenfolge bringen zu lassen (vgl. Backes (unveröffentlicht)). Ähnlich bei Friege (2001) wobei Schritte im Problemlöseprozess soriert werden müssen. Eine weitere Möglichkeit stellt das Sortieren von Strukturen ihrer Größe nach in einem Fachwissenstest dar. Doch wie können die mittels eines solchen Tests gewonnenen Daten ausgewertet werden? Eine dichotome Auswertung nach ,richtig sortiert’/,falsch sortiert’ liegt auf der Hand, jedoch ist diese Methode nicht besonders sensibel bezüglich des Fehlergrades: Bei einer Sortierung, bei welcher der Proband nur zwei benachbarte Positionen vertauscht hat, wird der Fehler als ebenso schwerwiegend gewertet als wenn der Start- und Endposition vertauscht werden. Eine Sortierung, bei welcher der Proband nur zwei Positionen vertauscht hat wird als ebenso schwerwiegend gewertet, als wenn er mehrere (alle) Positionen falsch zugeordnet hat. Es gelten bei Sortieraufgaben also im Wesentlichen zwei Faktoren, welche auf den Grad des Fehlers einwirken: Die Anzahl der falsch sortierten Positionen und der Abstand der falsch sortierten Positionen zueinander. Im Folgenden sollen verschiedene Algorithmen zur Bestimmung des Fehlergrades, sowie ihre Implementation in R, vorgestellt werden. Anstatt der hier im Folgenden vorgestellten strukturellen (mathematischen) Maße können Sortieraufgaben auch nach inhaltlichen Kriterien ausgewertet werden, siehe hierzu beispielhaft Binder (2019). 7.1 Der Datensatz Aufgrund der fehlenden Verfügbarkeit eines bereits existierenden Datensatzes soll im Folgenden auf einen künstlich erzeugten, geeigneten Datensatz zurückgegriffen werden. Die Aufgabe zu diesem Datensatz lautet: Ordnen Sie die folgenden Strukturen nach ihrer Größe (kleinste zuerst). 1) DNA-Doppelhelix 2) Ribosom 3) AIDS-Virus 4) Bakterien 5) rote Blutkörperchen 6) menschliche Leberzelle 7) menschliche Eizelle 8) große Amöbe (aus Bayrhuber (2010)) Der Datensatz ist im GitHub-Verzeichnis dieser Publikation verfügbar. Aus Gründen der Nachvollziehbarkeit lautet die korrekte Sortierung 1) 2) 3) 4) 5) 6) 7) 8). In R wird der Datensatz folgendermaßen eingelesen: #library(xlsx) sorting &lt;- read.csv(&quot;./data_for_chapters/sorting_tasks/Dataset_Sorting_Task.csv&quot;) # Daten einlesen head(sorting) #ersten Einträge des Datensatzes ausgeben Fehlergrad &lt;- sorting #Matrix erstellen um den Fehlergrad für jede ID zu erfassen Der ,Fehlergrad’ soll im Folgenden immer den Hinweis auf die Schwere des Fehlers geben und ist abhängig vom verwendeten Algorithmus. ID Sortierung ‚Fehlertyp‘ 1 12345678 korrekt 2 13245678 Nachbarpositionen vertauscht 3 32145678 Zwei Positionen vertauscht 4 42315678 Zwei Positionen vertauscht 5 82345671 Randpositionen vertauscht 6 23145567 Eine Position falsch eingeordnet 7 21345687 Zwei Nachbarn vertauscht 8 32145876 Zwei Positionen falsch eingeordnet 9 23456781 Jeweils um 1 verschoben 10 87654321 Rückwärts 7.2 Die Auswertungsmöglichkeiten In den folgenden Teilkapiteln werden die unterschiedlichen Auswertungsmöglichkeiten vorgestellt. 7.2.1 Dichotom: Sortierung ist richtig oder falsch Die einfachste Möglichkeit der Auswertung von Sortieraufgaben ist zu entscheiden, ob die Sortierung korrekt oder nicht korrekt vorgenommen wurde. Diese Art der Auswertung ist etwa sinnvoll, wenn allein die absolute Anzahl bzw. der Anteil der korrekten Antworten von Interesse ist. In R ist diese Auswertung schnell zu implementieren: #Schleife über alle ID for(i in 1:length(sorting$ID)){ if(sorting$Sorting[i] == 1234567){ Fehlergrad$Sorting[i] &lt;- 0 } else { Fehlergrad$Sorting[i] &lt;- 1 } } Die Ausgabe bei Verwendung des Beispieldatensatzes lautet (zur besseren Lesbarkeit wurde die Ausgabe stets in der Formatierung aufbereitet): ID Sortierung Fehlergrad 1 12345678 0 2 13245678 1 3 32145678 1 4 42315678 1 5 82345671 1 6 23145567 1 7 21345687 1 8 32145876 1 9 23456781 1 10 87654321 1 Wenn die Sortierung korrekt angegeben wird beträgt der Fehlergrad 0, alle anderen Sortierungen – unabhängig von der Schwere des Fehlers – werden undifferenziert als falsch (Fehlergrad 1) gewertet. Eine Differenzierung des Fehlergrades findet also nicht statt. 7.2.2 Summe der korrekten oder falschen Positionen Differenziertere Ergebnisse liefert die Wertung der Anzahl der korrekten oder - invertiert - der falschen Positionen in der Sortierung. Es werden die einzelnen Positionen der Sortierung durchlaufen und für jede Position wird geprüft, ob diese korrekt (bzw. falsch) ist. Eine Implementation in R kann folgendermaßen umgesetzt werden: for(i in 1:length(sorting$ID)){ #die Zahlenkette wird aufgetrennt und als Vekor einzelner Ziffern in hold abgespeichert hold &lt;- as.numeric(strsplit(as.character(sorting$Sorting[i]), &quot;&quot;)[[1]]) Fehlergrad$Sorting[i] &lt;- sum(hold != c(1,2,3,4,5,6,7)) } Die Ausgabe des Beispieldatensatzes ist: ID Sortierung Fehlergrad 1 12345678 0 2 13245678 2 3 32145678 2 4 42315678 2 5 82345671 2 6 23145567 3 7 21345687 4 8 32145876 4 9 23456781 8 10 87654321 8 Mit dieser Art der Auswertung können bereits unterschiedliche Fehlergrade bestimmt werden, jedoch sind die einzelnen Fehler nicht nach Distanz gewichtet, so wiegt das Vertauschen benachbarter Positionen bei dieser Auswertungsmethode ebenso schwer wie das intuitiv ungleich problematischere Vertauschen der Anfangs- und Endposition der Sortierung. Im Fehlergrad wird unterschieden, ob allein zwei Positionen (ID 2-5) oder mehrere (bzw. alle) Positionen vertauscht angegeben wurden (ID 6-10). 7.2.3 Spearman-Korrelation Die Rangkorrelationsanalyse nach Spearman ist ein Verfahren zur Berechnung eines Korrelationskoeffizienten welches auf den Rängen der Variablen basiert. Der errechnete Korrelationskoeffizient \\(\\rho\\), die sogenannte Spearman-Korrelation, ist ein Maß für die Ähnlichkeit zweier Variablen (vgl. Bortz (2011)). Die Berechnung des Korrelationskoeffizienten ist in R über die bereits angelegte Funtion cor(x,y) schnell möglich. Es ergibt sich folgender Code: for(i in 1:length(sorting$ID)){ #die Zahlenkette wird aufgetrennt und als Vekor einzelner Ziffern in hold abgespeichert hold &lt;- as.numeric(strsplit(as.character(sorting$Sorting[i]), &quot;&quot;)[[1]]) Fehlergrad$Sorting[i] &lt;- round(cor(c(1,2,3,4,5,6,7,8), hold, method = &quot;spearman&quot;),2) } Die Ausgabe bei Anwendung der Rangkorrelation nach Spearman auf die Beispielsortierungen lautet: ID Sortierung Fehlergrad 1 12345678 1.00 2 13245678 0.98 3 32145678 0.90 4 42315678 0.79 5 82345671 -0.17 6 23145567 0.93 7 21345687 0.95 8 32145876 0.81 9 23456781 0.33 10 87654321 -1.00 Der Korrelationskoeffizient \\(\\rho\\) nimmt Werte zwischen -1 (vollständige Verschiedenheit) und 1 (vollständige Übereinstimmung) an. Sowohl die Distanz einer fehlerhaften Sortierung als auch die Anzahl der Fehler fließt in den Korrelationskoeffizienten (hier als Fehlergrad bezeichnet) ein. 7.2.4 Levenshtein-Algorithmus Der Levenshtein-Algorithmus (Levenshtein, 1966) berechnet die minimale Anzahl an Einfügeoperationen, Löschoperationen und Ersetzungen, die benötigt werden um die Eingabe in die korrekte Sortierung zu transformieren. Der Levenshtein-Algorithmus kann über das Package stringdist als Funktion aufgerufen werden. Der Rückgabewert ist die minimale Anzahl der Operationen. In R ergibt sich folgender Code: library(stringdist) for(i in 1:length(sorting$ID)){ Fehlergrad$Sorting[i] &lt;- stringdist(&#39;1234567&#39;,sorting$Sorting[i],method=&#39;lv&#39;) } Die Ausgabe bei Anwendung des Levenstein-Algorithmus auf die Beispielsortierungen lautet: ID Sortierung Fehlergrad 1 12345678 0 2 13245678 2 3 32145678 2 4 42315678 2 5 82345671 2 6 23145567 2 7 21345687 4 8 32145876 4 9 23456781 2 10 87654321 8 Zu erkennen ist, dass die Distanz einer fehlerhaften Sortierung nicht in den Fehlergrad einfließt, die Anzahl der Fehler hingegen, analog zum vorherigen Algorithmus (‚Korrekte Position‘), schon. 7.2.5 Der Bubblesort-Algorithmus Der Bubblesort-Algorithmus (Knuth, 2010) ist ein Algorithmus zur Sortierung von Werten aus dem Fachbereich der Informatik. Der Name ‚Bubblesort‘ leitet sich von der den Algorithmus erklärenden Analogie aufsteigender Kohlensäurebläschen in einem Glas ab. Der Algorithmus vergleicht, beginnend mit dem ersten Wert \\(x_1\\) der zu sortierenden Folge, paarweise benachbarte Werte. Ist der rechte Wert \\(x_{i+1}\\) größer oder gleich dem linken Wert \\(x_i\\) findet keine Tauschoperation statt. Ist der rechte Wert \\(x_{i+1}\\) jedoch kleiner als der linke Wert \\(x_i\\), werden die Positionen der beiden Werte vertauscht. Anschließend bearbeitet der Algorithmus das nächste Datenpaar \\((x_{i+1}; x_{i+2})\\). Große Werte steigen somit wie große Kohlensäurebläschen an kleineren Werten vorbei. In R lässt sich der Bubblesort-Algorithmus folgendermaßen implementieren: sortiere.bubble &lt;- function(x) { if(!is.unsorted(x)){ return(summe_tauschoperationen = 0) stop(&quot;Vektor ist bereits sortiert&quot;) } n = length(x) v = x for(j in 1:(n-1)) { for(i in 1:(n-j)) { #wenn der Nachfolger kleiner als der aktuelle Wert, vertausche beide Werte if(v[i+1]&lt;v[i]) { t = v[i+1] v[i+1] = v[i] v[i] = t summe_tauschoperationen = summe_tauschoperationen + 1 print(summe_tauschoperationen) } } } print(v) x = v return(summe_tauschoperationen) } #Counter setzen summe_tauschoperationen = 0 #Schleife über alle Pre-Bögen for(i in 1:length(sorting$ID)){ hold &lt;- as.numeric(strsplit(as.character(sorting$Sorting[i]), &quot;&quot;)[[1]]) #Algorithmus anwenden Fehlergrad$Sorting[i] &lt;- sortiere.bubble(hold) } Der Beispieldatensatz führt zu folgender Ausgabe: ID Sortierung Fehlergrad 1 12345678 0 2 13245678 1 3 32145678 3 4 42315678 5 5 82345671 13 6 23145567 2 7 21345687 2 8 32145876 6 9 23456781 7 10 87654321 28 Durch Zählen der Tauschoperationen ist eine sensible Erfassung des Fehlergrades möglich. So ‚kostet‘ die Korrektur falsch sortierter benachbarter Werte nur eine Tauschoperation. Je weiter jedoch die falsch zugeordneten Werte auseinander liegen, desto mehr Tauschoperationen werden nötig – das Maß für den Fehlergrad steigt. 7.2.6 Der Jaro-Algorithmus (Jaro-Distanz) Die Jaro-Distanz ist definiert als \\(dj=\\frac{1}{3}\\Bigl(\\frac{m}{|s1|} +\\frac{m}{|s2|} +\\frac{(m-t)}{m}\\Bigr)\\) mit \\(m\\) = Anzahl der übereinstimmenden Buchstaben (Ziffern) \\(s\\) = Länge der Zeichenkette \\(t\\) = Hälfte der Anzahl der Umstellungen der Buchstaben die nötig sind, damit die Buchstabenketten identisch sind. Der Jaro-Algorithmus (Jaro, 1989) kann über das Package stringdist als Funktion aufgerufen werden. Der Rückgabewert ist die minimale Anzahl der Operationen. In R ergibt sich folgender Code: library(stringdist) for(i in 1:length(sorting$ID)){ Fehlergrad$Sorting[i] &lt;- round(stringdist(&#39;1234567&#39;,sorting$Sorting[i],method=&#39;jw&#39;, p = 0),2) } Als Erweiterung der Jaro-Distanz kann auch die Jaro-Winkler-Distanz verwendet werden. Hierzu sei auf die Dokumentation des Packages stringdist verwiesen. Der Beispieldatensatz führt zu folgender Ausgabe: ID Sortierung Fehlergrad 1 12345678 0.00 2 13245678 0.04 3 32145678 0.04 4 42315678 0.04 5 82345671 0.17 6 23145567 0.06 7 21345687 0.08 8 32145876 0.08 9 23456781 0.08 10 87654321 0.50 Der Fehlergrad wird als Wert zwischen 0 (exakte Übereinstimmung) und 1 (totale Verschiedenheit) angegeben. Der Wert 1 kann nur erreicht werden, wenn die einzelnen Ziffern nicht in der korrekten Sortierung vorkommen, etwa wenn Buchstaben statt wie gefordert Zahlen angegeben werden. Analog zum Bubblesort-Algorithmus nimmt auch hier der Fehlergrad mit zunehmendem Abstand der falsch zugeordneten Positionen zu. 7.3 Vergleich und Diskussion Zum Vergleich der einzelnen Algorithmen wurde jeweils eine Rangfolge vom niedrigsten zum höchsten Fehlergrad gebildet: ID Sorting dichotom kor.Position Spearman Levenshtein Bubblesort JaroDist 1 12345678 0 0 0 0 0 0 2 13245678 1 1 1 1 1 1 3 32145678 1 1 4 1 3 1 4 42315678 1 1 6 1 4 1 5 82345671 1 1 8 1 7 7 6 23145678 1 5 3 1 2 2 7 21345687 1 6 2 7 2 4 8 32145876 1 6 5 7 5 4 9 23456781 1 8 7 1 6 4 10 87654321 1 8 9 8 8 8 Deutlich werden zwei Eigenschaften: Es gibt große Unterschiede bezüglich der Sensibilität des Fehlergrades (vgl. dichotom mit dem Bubblesort- oder Jaro-Algorithmus sowie der Spearman-Korrelation). Auswertungen über die Dichotomie korrekt/falsch, die Methode korrekte Position sowie den Levenshtein-Algorithmus sind nicht sensibel gegenüber der Distanz einer falschen Sortierung (vgl. jeweils ID 1-5). Der Zweck der Auswertung sollte stets ihre Methode bestimmen und die Aussage nicht durch beschränkte Auswertungsmethoden limitiert oder sogar verzerrt werden. Wie gezeigt werden konnte, hat jede Methode spezifische Eigenheiten und gewichtet unterschiedliche Fehlertypen verschieden, wobei besonders die Spearman-Korrelation sowie Sortieralgorithmen aus der Informatik oft geeignet scheinen den Fehlergrad differenzierter zu bestimmen als klassische Methoden der statistischen Analyse. Möglicherweise erschließen sich auch, je nach Zweck der Auswertung, durch eine gewichtete Kombination zweier Algorithmen weitere geeignete Auswertungsmethoden. Bibliographie Backes, O. ;. S., A.; Tepner. (unveröffentlicht). Test zum experimentell-fachdidaktischen Wissen von Chemielehrkräften. Bayrhuber, H. ;. K., H.; Linder. (2010). Linder Biologie - Lehrbuch für die Oberstufe: Gesamtbund (LINDER Biologie SII / Auflage 2010). Schroedel. Binder, P. und T., T.; Schmiemann. (2019). Erfassung von fachspezifischen Problemlöseprozessen mit Sortieraufgaben in Biologie und Physik. ZfDN, 25, 25–42. Bortz, C., J. und Schuster. (2011). Statistik für Human- und Sozialwissenschaftler. Berlin, Heidelberg: Springer. Bühner, D. E. (1998). The Art of Computer Programming, Band 3: Sorting and Searching. USA: Addison Wesley. Friege, G. (2001). Wissen und Problemlösen: eine empirische Untersuchung des wissenszentrierten Problemlösens im Gebiet der Elektrizitätslehre auf der Grundlage des Experten-Novizen-Vergleichs. Logos. Jaro, M. A. (1989). Advances in Record-Linkage Methodology as Applied to Matching the 1985 Census of Tampa, Florida. Journal of the American Statistical Association, 84(406), 414–420. Taylor &amp; Francis. Knuth, D. E. (2010). Einführung in die Test- und Fragebogenkonstruktion. Hallbergmoos: Pearson Studium. Levenshtein, V. I. (1966). Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady, 10(8), 707–710. "],["typische-fehlvorstellungen.html", "8 Typische Fehlvorstellungen 8.1 „Das Rasch-Modell berücksichtigt im Gegensatz zur klassischen Testtheorie, welche Aufgaben richtig bearbeitet werden“ 8.2 Anwendung eines t-Tests für unabhängige Stichproben auf abhängige Stichproben", " 8 Typische Fehlvorstellungen Im Laufe der Zeit sind wir immer wieder einigen Fehlvorstellungen und Fehlern begegnet - vornehmlich bei uns selbst. Im Folgenden stellen wir diese Vorstellungen und Fehler kurz vor und stellen sie einer fachlich angemessenen Beschreibung gegenüber. 8.1 „Das Rasch-Modell berücksichtigt im Gegensatz zur klassischen Testtheorie, welche Aufgaben richtig bearbeitet werden“ Autor: David Buschhüter Im Folgenden soll, was sich hinter dieser Vorstellung verbirgt, genauer beschrieben werden: Person A und B haben beide 2 Punkte im Test erreicht: Person A gelöst: Aufgabe 1 (leicht) und 2 (mittelschwer) nicht gelöst: Aufgabe 3 (schwer) Person B gelöst: Aufgabe 1 (leicht) und 3 (schwer) nicht gelöst: Aufgabe 2 (mittelschwer) Eine typische Fehlvorstellung ist, anzunehmen, dass die richtige Bearbeitung von Aufgabe 3 im Rasch-Modell zu einer höheren Personenfähigkeit führt und somit für Person A ein niedrigerer Fähigkeitsparameter geschätzt würde als für Person B. Das stimmt aber so nicht: Es wird von der Punktesumme direkt auf den Fähigkeitsparameter geschlossen (wenn alle Teilnehmenden, die gleichen Aufgaben bearbeitet haben). Es ist dennoch so, dass sich Person B nicht Rasch-Modell konform verhält: Wenn nur eine Art von Fähigkeit (z.B. die Fähigkeit zwei normale Zahlen zu addieren) verantwortlich ist für das Lösungsverhalten auf den Aufgaben, warum kann eine Person dann Aufgabe 3 richtig bearbeiten aber nicht Aufgabe 2, wobei Aufgabe 2 doch leichter sein sollte? Dies ist im Rahmen des Modells (sinnvollerweise) kein erwartetes Verhalten. Hier kurz eine Illustration anhand der simulierten Daten. Wir nutzen dabei der übersichtlichkeit halber nur 10 Aufgaben. # laden von Paket und Daten library(TAM) data(data.sim.rasch) # Items in den Spalten 1(einfachstes)- 40(schwierigstes) class(data.sim.rasch) df &lt;- as.data.frame(data.sim.rasch) # Wir nutzen nur 10 der items und verteilen Sie über das Schwierigkeitsspektrum id_items &lt;- paste0(&quot;I&quot;,seq(from = 1, to = 40, by = 4)) df &lt;- df[,id_items] # Wir erzeugen zwei künstliche Personen PersonA &lt;- rep(1, 10) PersonB &lt;- rep(1, 10) # Folgendes ist nur möglich, weil die Items in aufsteigender # Schwierigkeit angeordnet sind # Person A hat Item 7 bis 10 nicht richtig gelöst PersonA[7:10] &lt;- 0 # Person B hat die Items 5 bis 8 nicht richtig gelöst PersonB[5:8] &lt;- 0 # Wir setzen die beiden Person unter die Daten df &lt;- rbind(df, PersonA, PersonB) Im Folgenden führen wir unsere Modellschätzung durch und bestimmen die WLEs. mod1 &lt;- tam.mml(df) pers_wle &lt;- tam.wle(mod1) Nun betrachten wir die Personen und können feststellen, dass beide Personen, die gleiche Punktsumme und damit den gleichen WLE haben. Obowohl sie unterschiedlich schwere Items falsch (bzw. richtig) bearbeitet haben #Person A paste0(&quot;Person A Punkte:&quot;, pers_wle$PersonScores[2001], &quot; -- WLE: &quot;, pers_wle$theta[2001]) #Person B paste0(&quot;Person B Punkte:&quot;, pers_wle$PersonScores[2002], &quot; -- WLE: &quot;, pers_wle$theta[2002]) ## [1] &quot;Person A Punkte:6 -- WLE: 0.368938295521808&quot; ## [1] &quot;Person B Punkte:6 -- WLE: 0.368938295521808&quot; Auch in einer Kreuztabelle zeigt sich, dass jeder Punktsumme ein Personenparameter zugeordnet ist. table(pers_wle$theta, pers_wle$PersonScores) ## ## 0 1 2 3 4 5 6 7 8 9 10 ## -3.84551294734028 29 0 0 0 0 0 0 0 0 0 0 ## -2.53353632886601 0 91 0 0 0 0 0 0 0 0 0 ## -1.78796492184946 0 0 174 0 0 0 0 0 0 0 0 ## -1.19340354202587 0 0 0 200 0 0 0 0 0 0 0 ## -0.658440981659903 0 0 0 0 267 0 0 0 0 0 0 ## -0.145360356703396 0 0 0 0 0 303 0 0 0 0 0 ## 0.368938295521808 0 0 0 0 0 0 292 0 0 0 0 ## 0.907248431737091 0 0 0 0 0 0 0 275 0 0 0 ## 1.50654116366946 0 0 0 0 0 0 0 0 200 0 0 ## 2.25735722362075 0 0 0 0 0 0 0 0 0 127 0 ## 3.57430983710992 0 0 0 0 0 0 0 0 0 0 44 8.2 Anwendung eines t-Tests für unabhängige Stichproben auf abhängige Stichproben Autor: David Buschhüter Eine typische Analyse in der Lehr-Lern-Forschung ist die Überprüfung des Lernzuwachses einer Lerngruppe. Häufig sieht man, dass hier ein t-Test für unabhängige Stichproben verwendet wird, wobei es doch um den individuellen Lernzuwachs geht. Besser wäre es deshalb, hier einen Test für abhängige Stichproben zu verwenden. Dabei werden Paare von einzelnen Fähigkeitsparametern nach Personen gebildet (was auch einem t-Test der Differenzen der Fähigkeitsparameter (nach der Lerneinheit minus vor der Lerneinheit) gegen den theoretischen Wert 0 entspricht. https://www.r-bloggers.com/2009/07/paired-students-t-test/ "],["glossar.html", "9 Glossar", " 9 Glossar Aufgabenschwierigkeit Die Aufgabenschwierigkeit ist ein latentes also nicht direkt sichtbares Maß für die Schwierigkeit einer Aufgabe. Sie wird bei Modellen aus der Item-Response-Theorie über den Aufgabenschwierigkeitsparameter \\(\\delta\\) (teilweise in der Literatur sowie Softwareanwendungen auch \\(\\xi\\) oder \\(\\beta\\)) geschätzt, welcher Werte zwischen \\(-\\infty\\) und \\(\\infty\\) annehmen kann. Achtung! Man unterscheidet diesen Parameter von der Aufgabenschwierigkeit \\(M\\) aus der klassischen Testtheorie (“KTT”), die als Verhältnis von erfolgreichen Lösungen \\(N_{gelöst}\\) zu erfolgten Bearbeitungen \\(N_{bearbeitet}\\) formuliert wird und in Prozent ausgedrückt werden kann: \\(M=\\frac{N_{gelöst}}{N_{bearbeitet}}*100\\). In der Folge gilt für die IRT: Je größer die Zahl, desto schwerer die Aufgabe. In der KTT ist eine Aufgabe umso schwerer, je kleiner der Zahlwert ist und dieser kann außerdem nur Werte zwischen \\(0\\) und \\(1\\), bzw. \\(0\\%\\) und \\(100\\%\\) annehmen. ICC Die Item CharacteristicCurve kann zur Überprüfung der Modellpassung verwendet werden. Sie zeigt die modellierten Lösungswahrscheinlichkeiten der jeweiligen Aufgaben in Abhängigkeit der modellierten Personenfähigkeiten. Im R-Paket “TAM” werden darüber hinaus die in (willkürlich bestimmbaren) Personenfähigkeitsgruppen gemittelten, empirischen Lösungsanteile über der Modellkurve abgetragen. So kann manuell geprüft werden, wie gut die Vorhersage des Modells über alle Aufgaben und Personen und die empirisch gefundenen Lösungsanteile für einzelne Aufgaben zusammenpassen. Im Idealfall liegen beide Kurven eng übereinander, in realen Studiendatensätzen sind entdeckte Diskrepanzen ein gut erkennbarer Indikator zur Überprüfung der Aufgaben. Die Abkürzung ICC ist auch in so genannten Mehrebenenmodellen zu finden, steht dort für Intraclass Correlation Coefficient und sollte nicht mit den Item Characteristic Curves aus der Item Response Theorie verwechselt werden. Lösungswahrscheinlichkeit Die Lösungswahrscheinlichkeit \\(P\\) also die Wahrscheinlichkeit, dass eine Person \\(n\\) mit einer Fähigkeit \\(\\theta_n\\) eine Aufgabe \\(i\\) der Schwierigkeit \\(\\delta_i\\) lösen. Da die Wahrscheinlichkeit von diesen zwei Parametern abhängt, schreibt man häufig \\(P(1|\\theta_n, \\delta_i)\\) für die Wahrscheinlichkeit einer richtigen Lösung (1). \\(P(0|\\theta_n, \\delta_i)\\) wäre analog die Wahrscheinlichkeit, die Aufgabe nicht zu lösen. Personenfähigkeit Die Personenfähigkeit ist ein latentes also nicht direkt sichtbares Maß für die Fähigkeit einer Person. Diese Fähigkeit kann natürlich von Bereich zu Bereich (teilweise auch Dimension genannt) unterschiedlich sein. Die Fähigkeit wird über den Fähigkeitsparameter \\(\\theta\\) geschätzt, welcher Werte zwischen \\(-\\infty\\) und \\(\\infty\\) annehmen kann. Wright-Map Die Wright-Map ist eine grafische Darstellung zur Gegenüberstellung von Aufgabenschrierigkeiten und Personenfähigkeiten. Anhand dieser kann überprüft werden, ob die Schwierigkeiten der Aufgaben den Bereich der Fähigkeiten der Personen gut abdecken, die Aufgaben also weder zu schwer noch zu leicht für die getesteten Personen sind. "],["verzeichnis-der-beiträge-und-autorinnen.html", "10 Verzeichnis der Beiträge und Autor*innen 10.1 Kapitel 10.2 Beiträge zu Fehlvorstellungen", " 10 Verzeichnis der Beiträge und Autor*innen 10.1 Kapitel Autor*innen Datum Titel Weitere Beitragende Tina Grottke, Philipp Möhrke, Marvin Rost 2020 Praxisorientierte Einführung in die Item-Response-Theorie mit dem Fokus auf das Rasch-Modell - Arne Bewersdorff 2021 Auswertungsmöglichkeiten für Sortieraufgaben - Daniel Rehfeldt, Martin Brämer 2022 Basiswissen für R: Praxisguide Marvin Rost Daniel Rehfeldt, Martin Brämer 2022 Datenaufbereitung in R: Praxisguide Marvin Rost 10.2 Beiträge zu Fehlvorstellungen Autor*innen Datum Titel Weitere Beitragende David Buschhüter 2020 Das Rasch-Modell berücksichtigt im Gegensatz zur klassischen Testtheorie, welche Aufgaben richtig bearbeitet werden - David Buschhüter 2020 Anwendung eines t-Tests für unabhängige Stichproben auf abhängige Stichproben - "],["bibliographie.html", "11 Bibliographie", " 11 Bibliographie Backes, O. ;. S., A.; Tepner. (unveröffentlicht). Test zum experimentell-fachdidaktischen Wissen von Chemielehrkräften. Bayrhuber, H. ;. K., H.; Linder. (2010). Linder Biologie - Lehrbuch für die Oberstufe: Gesamtbund (LINDER Biologie SII / Auflage 2010). Schroedel. Binder, P. und T., T.; Schmiemann. (2019). Erfassung von fachspezifischen Problemlöseprozessen mit Sortieraufgaben in Biologie und Physik. ZfDN, 25, 25–42. Bortz, C., J. und Schuster. (2011). Statistik für Human- und Sozialwissenschaftler. Berlin, Heidelberg: Springer. Bühner, D. E. (1998). The Art of Computer Programming, Band 3: Sorting and Searching. USA: Addison Wesley. Bühner, M. (2011). Einführung in die Test- und Fragebogenkonstruktion (3. Auflage). München: Pearson. Verfügbar unter: http://ebookcentral.proquest.com/lib/huberlin-ebooks/detail.action?docID=5133497 Chalmers, R. P. (2015). Extended Mixed-Effects Item Response Models With the MH-RM Algorithm. Journal of Educational Measurement, 52(2), 200–222. https://doi.org/10.1111/jedm.12072 Field, A. P., Miles, J. &amp; Field, Z. (2012). Discovering statistics using R. London ; Thousand Oaks, Calif: Sage. Friege, G. (2001). Wissen und Problemlösen: eine empirische Untersuchung des wissenszentrierten Problemlösens im Gebiet der Elektrizitätslehre auf der Grundlage des Experten-Novizen-Vergleichs. Logos. Jaro, M. A. (1989). Advances in Record-Linkage Methodology as Applied to Matching the 1985 Census of Tampa, Florida. Journal of the American Statistical Association, 84(406), 414–420. Taylor &amp; Francis. Knuth, D. E. (2010). Einführung in die Test- und Fragebogenkonstruktion. Hallbergmoos: Pearson Studium. Levenshtein, V. I. (1966). Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady, 10(8), 707–710. Linacre, J. M. (1999). Understanding Rasch measurement: estimation methods for Rasch Measures. Journal of Outcome Measurement, 3, 381–405. Lüdtke, O., Robitzsch, A., Trautwein, U. &amp; Köller, O. (2007). Umgang mit fehlenden Werten in der psychologischen Forschung. Psychologische Rundschau, 58(2), 103–117. Porst, R. (2014). Fragebogen: ein Arbeitsbuch (Studienskripten zur Soziologie) (4., erw. Aufl.). Wiesbaden: Springer VS. Robitzsch, A., Kiefer, T. &amp; Wu, M. (2020). TAM: Test Analysis Modules. Verfügbar unter: https://CRAN.R-project.org/package=TAM Torres Irribarra, D. &amp; Freund, R. (2016). IRT Item-Person-Map with ’ConQuest’ Integration. Verfügbar unter: https://github.com/david-ti/wrightmap Trendtel, M., Pham, G. &amp; Yanagida, T. (2016). Skalierung und Linking. In S. Breit &amp; C. Schreiner (Hrsg.), Large-Scale Assessment mit R: Methodische Grundlagen der österreichischen Bildungsstandardüberprüfung (S. 185–224). Wien: Facultas Verlags- und Buchhandels AG. Wickham, H. (2014). Tidy data. The Journal of Statistical Software, 59. Wu, M., Tam, H. P. &amp; Jen, T.-H. (2016). Educational Measurement for Applied Researchers: Theory into Practice. Singapore: Springer. https://doi.org/10.1007/978-981-10-3302-5 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
